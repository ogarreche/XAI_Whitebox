{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Initializing DNN program\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Importing Libraries\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining Metric Equations\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining features of interest\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Loading Database\n",
      "--------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating features and labels\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'Denial of Service': 642515, 'Port Scanning': 417040, 'None': 195521})\n",
      "---------------------------------------------------------------------------------\n",
      "number of Labels   [195521, 642515, 417040]\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating Training and Testing db\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Number of the training data: 878476\n",
      "Number of the testing data: 376600\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "### DNN Regression\n",
    "# It is now time to implements single-input and multiple-inputs DNN models\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Initializing DNN program')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "#---------------------------------------------------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Importing Libraries')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import LSTM\n",
    "#from keras.layers import Dropout\n",
    "#from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.preprocessing import sequence\n",
    "#from keras.utils import pad_sequences\n",
    "#from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "import shap\n",
    "np.random.seed(0)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining metric equations\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Metric Equations')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "def ACC(TP,TN,FP,FN):\n",
    "    Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return Acc\n",
    "def ACC_2 (TP, FN):\n",
    "    ac = (TP/(TP+FN))\n",
    "    return ac\n",
    "def PRECISION(TP,FP):\n",
    "    Precision = TP/(TP+FP)\n",
    "    return Precision\n",
    "def RECALL(TP,FN):\n",
    "    Recall = TP/(TP+FN)\n",
    "    return Recall\n",
    "def F1(Recall, Precision):\n",
    "    F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "    return F1\n",
    "def BACC(TP,TN,FP,FN):\n",
    "    BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "    return BACC\n",
    "def MCC(TP,TN,FP,FN):\n",
    "    MCC = (TN*TP-FN*FP)/(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**.5)\n",
    "    return MCC\n",
    "def AUC_ROC(y_test_bin,y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    auc_avg = 0\n",
    "    counting = 0\n",
    "    for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "     # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "      #print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "      auc_avg += auc(fpr[i], tpr[i])\n",
    "      counting = i+1\n",
    "    return auc_avg/counting\n",
    "#---------------------------------------------------------------------\n",
    "# Defining features of interest\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining features of interest')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "req_cols = ['FLOW_DURATION_MILLISECONDS','FIRST_SWITCHED',\n",
    "            'TOTAL_FLOWS_EXP','TCP_WIN_MSS_IN','LAST_SWITCHED',\n",
    "            'TCP_WIN_MAX_IN','TCP_WIN_MIN_IN','TCP_WIN_MIN_OUT',\n",
    "           'PROTOCOL','TCP_WIN_MAX_OUT','TCP_FLAGS',\n",
    "            'TCP_WIN_SCALE_OUT','TCP_WIN_SCALE_IN','SRC_TOS',\n",
    "            'DST_TOS','FLOW_ID','L4_SRC_PORT','L4_DST_PORT',\n",
    "           'MIN_IP_PKT_LEN','MAX_IP_PKT_LEN','TOTAL_PKTS_EXP',\n",
    "           'TOTAL_BYTES_EXP','IN_BYTES','IN_PKTS','OUT_BYTES','OUT_PKTS',\n",
    "            'ALERT']\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "address = '/home/oarreche@ads.iu.edu/HITL/sensor/sensor_db'\n",
    "print('Loading Database')\n",
    "print('--------------------------------------------------')\n",
    "\n",
    "fraction = 0.1\n",
    "fraction2 = 0.01\n",
    "\n",
    "#Denial of Service\n",
    "df0 = pd.read_csv (address + '/dos-03-15-2022-15-44-32.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df1 = pd.read_csv (address + '/dos-03-16-2022-13-45-18.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df2 = pd.read_csv (address + '/dos-03-17-2022-16-22-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df3 = pd.read_csv (address + '/dos-03-18-2022-19-27-05.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df4 = pd.read_csv (address + '/dos-03-19-2022-20-01-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df5 = pd.read_csv (address + '/dos-03-20-2022-14-27-54.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "#Malware\n",
    "#df6 = pd.read_csv ('sensor_db/malware-03-25-2022-17-57-07.csv', usecols=req_cols)\n",
    "\n",
    "#Normal\n",
    "df7 = pd.read_csv  (address + '/normal-03-15-2022-15-43-44.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df8 = pd.read_csv  (address + '/normal-03-16-2022-13-44-27.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df9 = pd.read_csv  (address + '/normal-03-17-2022-16-21-30.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df10 = pd.read_csv (address + '/normal-03-18-2022-19-17-31.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df11 = pd.read_csv (address + '/normal-03-18-2022-19-25-48.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df12 = pd.read_csv (address + '/normal-03-19-2022-20-01-16.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df13 = pd.read_csv (address + '/normal-03-20-2022-14-27-30.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "\n",
    "\n",
    "#PortScanning\n",
    "df14 = pd.read_csv  (address + '/portscanning-03-15-2022-15-44-06.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df15 = pd.read_csv  (address + '/portscanning-03-16-2022-13-44-50.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df16 = pd.read_csv  (address + '/portscanning-03-17-2022-16-22-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df17 = pd.read_csv  (address + '/portscanning-03-18-2022-19-27-05.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df18 = pd.read_csv  (address + '/portscanning-03-19-2022-20-01-45.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df19 = pd.read_csv  (address + '/portscanning-03-20-2022-14-27-49.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "frames = [df0, df1, df2, df3, df4, df5, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19]\n",
    "\n",
    "# fraction = 0.1\n",
    "\n",
    "#concat data frames\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "# shuffle the DataFrame rows\n",
    "# df = df.sample(frac = fraction)\n",
    "\n",
    "y = df.pop('ALERT')\n",
    "X = df\n",
    "\n",
    "df_max_scaled = X\n",
    "for col in df_max_scaled.columns:\n",
    "    t = abs(df_max_scaled[col].max())\n",
    "    df_max_scaled[col] = df_max_scaled[col]/t\n",
    "df_max_scaled\n",
    "df = df_max_scaled.assign( Label = y)\n",
    "#df\n",
    "df = df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# IG\n",
    "\n",
    "# df.pop('TOTAL_FLOWS_EXP')\n",
    "# df.pop('LAST_SWITCHED')\n",
    "# df.pop('TCP_WIN_MAX_IN')\n",
    "# df.pop('TCP_WIN_MIN_IN')\n",
    "# df.pop('FIRST_SWITCHED')\n",
    "\n",
    "# df.pop('L4_SRC_PORT')\n",
    "# df.pop('TCP_WIN_SCALE_IN')\n",
    "# df.pop('FLOW_ID')\n",
    "# df.pop('L4_DST_PORT')\n",
    "# df.pop('PROTOCOL')\n",
    "\n",
    "# df.pop('TCP_WIN_MSS_IN')\n",
    "# df.pop('TCP_WIN_MIN_OUT')\n",
    "# df.pop('DST_TOS')\n",
    "# df.pop('FLOW_DURATION_MILLISECONDS')\n",
    "# df.pop('TCP_FLAGS')\n",
    "# df.pop('TCP_WIN_MAX_OUT')\n",
    "# df.pop('TCP_WIN_SCALE_OUT')\n",
    "# df.pop('SRC_TOS')\n",
    "# df.pop('IN_PKTS')\n",
    "# df.pop('OUT_PKTS')\n",
    "# df.pop('OUT_BYTES')\n",
    "\n",
    "# df.pop('IN_BYTES')\n",
    "# df.pop('MIN_IP_PKT_LEN')\n",
    "# df.pop('MAX_IP_PKT_LEN')\n",
    "# df.pop('TOTAL_PKTS_EXP')\n",
    "# df.pop('TOTAL_BYTES_EXP')\n",
    "\n",
    "\n",
    "# LRP \n",
    "# df.pop('TOTAL_FLOWS_EXP')\n",
    "# df.pop('LAST_SWITCHED')\n",
    "# df.pop('TCP_WIN_MIN_IN')\n",
    "# df.pop('TCP_WIN_MAX_IN')\n",
    "# df.pop('FIRST_SWITCHED')\n",
    "\n",
    "# df.pop('TCP_WIN_SCALE_IN')\n",
    "# df.pop('L4_SRC_PORT')\n",
    "# df.pop('L4_DST_PORT')\n",
    "# df.pop('FLOW_ID')\n",
    "# df.pop('PROTOCOL')\n",
    "\n",
    "# df.pop('TCP_WIN_MSS_IN')\n",
    "# df.pop('TCP_FLAGS')\n",
    "# df.pop('FLOW_DURATION_MILLISECONDS')\n",
    "# df.pop('DST_TOS')\n",
    "# df.pop('TCP_WIN_MIN_OUT')\n",
    "# df.pop('TCP_WIN_MAX_OUT')\n",
    "# df.pop('TCP_WIN_SCALE_OUT')\n",
    "# df.pop('SRC_TOS')\n",
    "# df.pop('IN_PKTS')\n",
    "# df.pop('OUT_PKTS')\n",
    "# df.pop('OUT_BYTES')\n",
    "\n",
    "# df.pop('IN_BYTES')\n",
    "# df.pop('MIN_IP_PKT_LEN')\n",
    "# df.pop('MAX_IP_PKT_LEN')\n",
    "# df.pop('TOTAL_PKTS_EXP')\n",
    "# df.pop('TOTAL_BYTES_EXP')\n",
    "\n",
    "\n",
    "# SHAP\n",
    "# df.pop('TCP_WIN_MIN_IN')\n",
    "# df.pop('TCP_WIN_MIN_OUT')\n",
    "# df.pop('TCP_WIN_SCALE_OUT')\n",
    "# df.pop('FLOW_DURATION_MILLISECONDS')\n",
    "# df.pop('TCP_WIN_MAX_IN')\n",
    "\n",
    "# df.pop('TCP_WIN_SCALE_IN')\n",
    "# df.pop('TCP_WIN_MAX_OUT')\n",
    "# df.pop('FLOW_ID')\n",
    "# df.pop('L4_SRC_PORT')\n",
    "# df.pop('TOTAL_FLOWS_EXP')\n",
    "\n",
    "# df.pop('TCP_FLAGS')\n",
    "# df.pop('TCP_WIN_MSS_IN')\n",
    "# df.pop('LAST_SWITCHED')\n",
    "# df.pop('FIRST_SWITCHED')\n",
    "# df.pop('IN_PKTS')\n",
    "# df.pop('OUT_PKTS')\n",
    "# df.pop('IN_BYTES')\n",
    "# df.pop('OUT_BYTES')\n",
    "# df.pop('L4_DST_PORT')\n",
    "# df.pop('SRC_TOS')\n",
    "# df.pop('DST_TOS')\n",
    "\n",
    "# df.pop('PROTOCOL')\n",
    "# df.pop('MIN_IP_PKT_LEN')\n",
    "# df.pop('MAX_IP_PKT_LEN')\n",
    "# df.pop('TOTAL_PKTS_EXP')\n",
    "# df.pop('TOTAL_BYTES_EXP')\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating features and labels')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "print('---------------------------------------------------------------------------------')\n",
    "result_list = [counter['None'],counter['Denial of Service'], counter['Port Scanning']]\n",
    "print('number of Labels  ',result_list)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate Training and Testing db\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating Training and Testing db')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# y = df_train.pop('ALERT')\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .70\n",
    "#print(df.head())\n",
    "\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "print('Number of the training data:', len(train))\n",
    "print('Number of the testing data:', len(test))\n",
    "\n",
    "features = df.columns[:len(req_cols)-1]\n",
    "\n",
    "y_train, label = pd.factorize(train['Label'])\n",
    "y_test, label = pd.factorize(test['Label'])\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining the DNN model\n",
    "\n",
    "df_y, df_label = pd.factorize(df['Label'])\n",
    "\n",
    "\n",
    "df2 = df.drop(columns=['Label','is_train'])\n",
    "df2['Label'] = df_y\n",
    "df2\n",
    "train = train.drop(columns=['Label','is_train'])\n",
    "# # uncomented for Information Gain\n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "# %matplotlib inline\n",
    "\n",
    "# # Compute information gain using mutual information\n",
    "# importances0 = mutual_info_classif(train, y_train)\n",
    "\n",
    "\n",
    "# feat_importances0 = pd.Series(importances0, df2.columns[0:len(df2.columns)-1])\n",
    "\n",
    "    \n",
    "# feat_importances_sorted0 = feat_importances0.sort_values( ascending=False)\n",
    "\n",
    "\n",
    "# # Print or use the sorted DataFrame\n",
    "# print(feat_importances_sorted0)\n",
    "\n",
    "# # feat_importances_sorted.plot(kind='barh', color = 'teal')\n",
    "# # feat_importances_sorted\n",
    "# top_features0 = feat_importances_sorted0.nlargest(10)\n",
    "\n",
    "# top_feature_names0 = top_features0.index.tolist()\n",
    "\n",
    "\n",
    "# print(\"Top 5 feature names:\")\n",
    "# print(top_features0)\n",
    "\n",
    "# column_features = top_feature_names0\n",
    "\n",
    "# top10 = ['IN_BYTES'\n",
    "#          ,'FLOW_ID'\n",
    "#          ,'TOTAL_FLOWS_EXP'\n",
    "#          ,'LAST_SWITCHED'\n",
    "#          ,'FIRST_SWITCHED'\n",
    "#          ,'TCP_WIN_MAX_IN'\n",
    "#          ,'TCP_WIN_MIN_IN'\n",
    "#          ,'TCP_FLAGS'\n",
    "#          ,'FLOW_DURATION_MILLISECONDS'\n",
    "#          ,'L4_SRC_PORT']\n",
    "\n",
    "\n",
    "test.pop('is_train')\n",
    "test.pop('Label')\n",
    "y_test\n",
    "y_train\n",
    "\n",
    "X_train = train\n",
    "X_test = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Now you can use Keras modules directly from tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Flatten\n",
    "import innvestigate\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining the DNN model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining the DNN model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# Define the number of nodes per layer\n",
    "nodes_first_layer = 128\n",
    "nodes_second_layer = 64\n",
    "nodes_third_layer = 32\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(len(X_train.columns,))))\n",
    "\n",
    "# First dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_first_layer, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))  # Dropout layer follows the first dense layer\n",
    "\n",
    "# Second dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_second_layer, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))  # Dropout layer follows the second dense layer\n",
    "\n",
    "# Third dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_third_layer, activation='relu'))\n",
    "\n",
    "# Output layer, assuming the task involves classification into 3 classes\n",
    "model.add(tf.keras.layers.Dense(3))  # No activation here, add 'softmax' when using the model for prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(3))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# # Define your DNN model\n",
    "# model = Sequential([\n",
    "#     Flatten(input_shape=(28, 28)),  # Input layer\n",
    "#     Dense(128, activation='relu'),  # Hidden layer\n",
    "#     Dense(10, activation='relu')  # Output layer\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save('your_model.h5')\n",
    "\n",
    "# # Load your trained model\n",
    "# model = tf.keras.models.load_model('your_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training the model\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 878476 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 1/1000\n",
      "878476/878476 [==============================] - 1s 2us/sample - loss: 4.2339 - accuracy: 0.5103 - lr: 0.0010\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 2/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 3.3813 - accuracy: 0.5094 - lr: 0.0010\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 3/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 2.9274 - accuracy: 0.5093 - lr: 0.0010\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 4/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 2.7118 - accuracy: 0.5102 - lr: 0.0010\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 5/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 2.5569 - accuracy: 0.5139 - lr: 0.0010\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 6/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 2.3612 - accuracy: 0.5211 - lr: 0.0010\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 7/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 2.1416 - accuracy: 0.5312 - lr: 0.0010\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 8/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.8709 - accuracy: 0.5417 - lr: 0.0010\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 9/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.6150 - accuracy: 0.5446 - lr: 0.0010\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 10/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.4196 - accuracy: 0.5418 - lr: 0.0010\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 11/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.3087 - accuracy: 0.5302 - lr: 0.0010\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.2252 - accuracy: 0.5212 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.2163 - accuracy: 0.5210 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 14/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.2151 - accuracy: 0.5205 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 15/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.2171 - accuracy: 0.5209 - lr: 1.0000e-07\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 16/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.2185 - accuracy: 0.5211 - lr: 1.0000e-08\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 17/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.2177 - accuracy: 0.5211 - lr: 1.0000e-09\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 18/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.2162 - accuracy: 0.5213 - lr: 1.0000e-10\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 19/1000\n",
      "878476/878476 [==============================] - 2s 2us/sample - loss: 1.2177 - accuracy: 0.5214 - lr: 1.0000e-11\n",
      "---------------------------------------------------------------------------------\n",
      "ELAPSE TIME TRAINING MODEL:  0.3833666443824768 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Define your learning rate schedule function\n",
    "# def lr_schedule(epoch):\n",
    "#     # Your learning rate schedule logic here\n",
    "#     learning_rate = 0.1\n",
    "#     if epoch > 10:\n",
    "#         learning_rate = 0.01\n",
    "#     if epoch > 20:\n",
    "#         learning_rate = 0.001\n",
    "#     return learning_rate\n",
    "# lr_sched = LearningRateScheduler(lambda epoch: 1e-3 * (0.75 ** np.floor(epoch / 2)))\n",
    "\n",
    "# # Create a LearningRateScheduler callback\n",
    "# lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training the model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Modify model.fit to include the EarlyStopping callback\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train), callbacks=[early_stopping,lr_scheduler])\n",
    "\n",
    "# Pass the lr_scheduler callback to your model.fit() function\n",
    "# model.fit(X_train, y_train, callbacks=[lr_scheduler], ...)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('ELAPSE TIME TRAINING MODEL: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Model Prediction\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSE TIME MODEL PREDICTION:  0.16981900533040364 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "92.39697291556027\n",
      "Counter({0: 192329, 2: 125302, 1: 58969})\n",
      "Counter({0: 200863, 2: 130289, 1: 45448})\n",
      "92.39697291556027\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Model Prediction')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time()\n",
    "print('ELAPSE TIME MODEL PREDICTION: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "#print(y_pred)\n",
    "ynew = np.argmax(y_pred,axis = 1)\n",
    "#print(ynew)\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "#print(score)\n",
    "pred_label = label[ynew]\n",
    "#print(score)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# pd.crosstab(test['ALERT'], preds, rownames=['Actual ALERT'], colnames = ['Predicted ALERT'])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_test)\n",
    "print(label_counts)\n",
    "\n",
    "label_counts = Counter(ynew)\n",
    "print(label_counts)\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample_df = X_test.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.129  0.028  0.    ...  0.     0.     0.   ]\n",
      " [-0.014  0.076  0.016 ...  0.    -0.    -0.   ]\n",
      " [-0.01   0.073  0.    ...  0.     0.     0.   ]\n",
      " ...\n",
      " [ 0.107  0.032  0.    ...  0.     0.     0.   ]\n",
      " [ 0.129  0.02   0.    ...  0.     0.     0.   ]\n",
      " [ 0.124  0.026  0.    ...  0.     0.     0.   ]]\n",
      "376600\n",
      "26\n",
      "(100, 26)\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 0.129  0.028  0.    ...  0.     0.     0.   ]\n",
      " [-0.014  0.076  0.016 ...  0.    -0.    -0.   ]\n",
      " [-0.01   0.073  0.    ...  0.     0.     0.   ]\n",
      " ...\n",
      " [ 0.107  0.032  0.    ...  0.     0.     0.   ]\n",
      " [ 0.129  0.02   0.    ...  0.     0.     0.   ]\n",
      " [ 0.124  0.026  0.    ...  0.     0.     0.   ]]\n",
      "('FLOW_ID', 'TCP_WIN_MAX_IN', 'TCP_WIN_MIN_IN', 'TOTAL_FLOWS_EXP', 'L4_SRC_PORT', 'FIRST_SWITCHED', 'LAST_SWITCHED', 'TCP_WIN_SCALE_IN', 'L4_DST_PORT', 'FLOW_DURATION_MILLISECONDS', 'PROTOCOL', 'TCP_FLAGS', 'TCP_WIN_MSS_IN', 'DST_TOS', 'SRC_TOS', 'TCP_WIN_MAX_OUT', 'TCP_WIN_MIN_OUT', 'IN_PKTS', 'OUT_PKTS', 'IN_BYTES', 'OUT_BYTES', 'TCP_WIN_SCALE_OUT', 'MIN_IP_PKT_LEN', 'MAX_IP_PKT_LEN', 'TOTAL_PKTS_EXP', 'TOTAL_BYTES_EXP')\n",
      "(7.169097423553467, 6.441859245300293, 4.725471496582031, 4.677468299865723, 4.3540358543396, 3.830751895904541, 2.543985366821289, 1.6605029106140137, 1.3015836477279663, 0.3280622661113739, 0.2021767944097519, 0.15652453899383545, 0.1550353467464447, 0.1208634227514267, 0.09177619963884354, 0.0050889309495687485, 0.002213591244071722, 7.85396114224568e-05, 4.6544006181648e-05, 5.194000550545752e-06, 2.5668407488410594e-06, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "CPU times: user 7.77 s, sys: 9.57 ms, total: 7.78 s\n",
      "Wall time: 7.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('FLOW_ID',\n",
       " 'TCP_WIN_MAX_IN',\n",
       " 'TCP_WIN_MIN_IN',\n",
       " 'TOTAL_FLOWS_EXP',\n",
       " 'L4_SRC_PORT',\n",
       " 'FIRST_SWITCHED',\n",
       " 'LAST_SWITCHED',\n",
       " 'TCP_WIN_SCALE_IN',\n",
       " 'L4_DST_PORT',\n",
       " 'FLOW_DURATION_MILLISECONDS',\n",
       " 'PROTOCOL',\n",
       " 'TCP_FLAGS',\n",
       " 'TCP_WIN_MSS_IN',\n",
       " 'DST_TOS',\n",
       " 'SRC_TOS',\n",
       " 'TCP_WIN_MAX_OUT',\n",
       " 'TCP_WIN_MIN_OUT',\n",
       " 'IN_PKTS',\n",
       " 'OUT_PKTS',\n",
       " 'IN_BYTES',\n",
       " 'OUT_BYTES',\n",
       " 'TCP_WIN_SCALE_OUT',\n",
       " 'MIN_IP_PKT_LEN',\n",
       " 'MAX_IP_PKT_LEN',\n",
       " 'TOTAL_PKTS_EXP',\n",
       " 'TOTAL_BYTES_EXP')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Create an analyzer for the model\n",
    "analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model)\n",
    "\n",
    "# Replace X_test with your input data\n",
    "\n",
    "# X_test = np.random.rand(100, 28, 28)  # Example input data\n",
    "\n",
    "# X_test2 = X_test.frac(0.001, random_state=42)\n",
    "\n",
    "# Perform LRP analysis on the input data\n",
    "analysis = analyzer.analyze(X_test)\n",
    "#uncomment for single sample\n",
    "# analysis = analyzer.analyze(single_sample_df)\n",
    "\n",
    "# Perform LRP analysis on a certain number of samples\n",
    "analysis = analyzer.analyze(X_test.sample(100))\n",
    "\n",
    "# Print or use the analysis results as needed\n",
    "print(analysis)\n",
    "\n",
    "print(len(X_test))\n",
    "print(len(X_test.columns))\n",
    "names = X_test.columns\n",
    "print(analysis.shape)\n",
    "print(type(analysis))\n",
    "scores = pd.DataFrame(analysis)\n",
    "print(analysis)\n",
    "scores_abs = scores.abs()\n",
    "\n",
    "# Calculate the sum of each column\n",
    "sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "names = list(names)\n",
    "\n",
    "names\n",
    "\n",
    "sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "sum_of_columns\n",
    "# type(sum_of_columns)\n",
    "# # sorted_series = sum_of_columns.sort_values(ascending=False)\n",
    "\n",
    "# print(\"Sorted Series in descending order:\")\n",
    "# print(sorted_series)\n",
    "### Results\n",
    "# names = ['John', 'Alice', 'Bob', 'Emily']\n",
    "# sum_of_columns = [10, 5, 15, 8]\n",
    "\n",
    "# Zip the two lists together\n",
    "combined = list(zip(names, sum_of_columns))\n",
    "\n",
    "# Sort the combined list in descending order based on the values from sum_of_columns\n",
    "sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "\n",
    "print(sorted_names)\n",
    "print(sorted_sum_of_columns)\n",
    "\n",
    "\n",
    "sorted_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.001  0.056  0.036  0.028 -0.     0.05   0.002 -0.001  0.001 -0.\n",
      "   0.     0.     0.004 -0.     0.    -0.    -0.    -0.026 -0.    -0.\n",
      "   0.    -0.    -0.     0.    -0.    -0.   ]]\n",
      "376600\n",
      "26\n",
      "(1, 26)\n",
      "<class 'numpy.ndarray'>\n",
      "[[-0.001  0.056  0.036  0.028 -0.     0.05   0.002 -0.001  0.001 -0.\n",
      "   0.     0.     0.004 -0.     0.    -0.    -0.    -0.026 -0.    -0.\n",
      "   0.    -0.    -0.     0.    -0.    -0.   ]]\n",
      "('L4_SRC_PORT', 'LAST_SWITCHED', 'L4_DST_PORT', 'FIRST_SWITCHED', 'TOTAL_FLOWS_EXP', 'TCP_WIN_MSS_IN', 'PROTOCOL', 'TCP_FLAGS', 'FLOW_ID', 'TCP_WIN_MAX_IN', 'TCP_WIN_MIN_IN', 'IN_PKTS', 'OUT_PKTS', 'IN_BYTES', 'OUT_BYTES', 'FLOW_DURATION_MILLISECONDS', 'TCP_WIN_MAX_OUT', 'TCP_WIN_MIN_OUT', 'TCP_WIN_SCALE_IN', 'TCP_WIN_SCALE_OUT', 'SRC_TOS', 'DST_TOS', 'MIN_IP_PKT_LEN', 'MAX_IP_PKT_LEN', 'TOTAL_PKTS_EXP', 'TOTAL_BYTES_EXP')\n",
      "(0.05625816807150841, 0.049711477011442184, 0.03615616261959076, 0.028453459963202477, 0.026484090834856033, 0.003746533300727606, 0.001537513337098062, 0.0014093829086050391, 0.0009540505125187337, 0.0007362438482232392, 0.00026905021513812244, 1.6071211916823813e-07, 8.444041554866999e-08, 4.4651131680950584e-09, 1.0301374198107283e-09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
      "CPU times: user 14.1 s, sys: 3.14 ms, total: 14.1 s\n",
      "Wall time: 14.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('L4_SRC_PORT',\n",
       " 'LAST_SWITCHED',\n",
       " 'L4_DST_PORT',\n",
       " 'FIRST_SWITCHED',\n",
       " 'TOTAL_FLOWS_EXP',\n",
       " 'TCP_WIN_MSS_IN',\n",
       " 'PROTOCOL',\n",
       " 'TCP_FLAGS',\n",
       " 'FLOW_ID',\n",
       " 'TCP_WIN_MAX_IN',\n",
       " 'TCP_WIN_MIN_IN',\n",
       " 'IN_PKTS',\n",
       " 'OUT_PKTS',\n",
       " 'IN_BYTES',\n",
       " 'OUT_BYTES',\n",
       " 'FLOW_DURATION_MILLISECONDS',\n",
       " 'TCP_WIN_MAX_OUT',\n",
       " 'TCP_WIN_MIN_OUT',\n",
       " 'TCP_WIN_SCALE_IN',\n",
       " 'TCP_WIN_SCALE_OUT',\n",
       " 'SRC_TOS',\n",
       " 'DST_TOS',\n",
       " 'MIN_IP_PKT_LEN',\n",
       " 'MAX_IP_PKT_LEN',\n",
       " 'TOTAL_PKTS_EXP',\n",
       " 'TOTAL_BYTES_EXP')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create an analyzer for the model\n",
    "analyzer = innvestigate.create_analyzer(\"lrp.z\", model)\n",
    "\n",
    "# Replace X_test with your input data\n",
    "\n",
    "# X_test = np.random.rand(100, 28, 28)  # Example input data\n",
    "\n",
    "# X_test2 = X_test.frac(0.001, random_state=42)\n",
    "\n",
    "# Perform LRP analysis on the input data\n",
    "analysis = analyzer.analyze(X_test)\n",
    "\n",
    "#uncomment for single sample\n",
    "# analysis = analyzer.analyze(single_sample_df)\n",
    "\n",
    "# Perform LRP analysis on a certain number of samples\n",
    "# analysis = analyzer.analyze(X_test.sample(1))\n",
    "\n",
    "\n",
    "# Print or use the analysis results as needed\n",
    "print(analysis)\n",
    "print(len(X_test))\n",
    "print(len(X_test.columns))\n",
    "names = X_test.columns\n",
    "print(analysis.shape)\n",
    "print(type(analysis))\n",
    "scores = pd.DataFrame(analysis)\n",
    "print(analysis)\n",
    "scores_abs = scores.abs()\n",
    "\n",
    "# Calculate the sum of each column\n",
    "sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "names = list(names)\n",
    "\n",
    "names\n",
    "\n",
    "sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "sum_of_columns\n",
    "# type(sum_of_columns)\n",
    "# # sorted_series = sum_of_columns.sort_values(ascending=False)\n",
    "\n",
    "# print(\"Sorted Series in descending order:\")\n",
    "# print(sorted_series)\n",
    "# names = ['John', 'Alice', 'Bob', 'Emily']\n",
    "# sum_of_columns = [10, 5, 15, 8]\n",
    "\n",
    "# Zip the two lists together\n",
    "combined = list(zip(names, sum_of_columns))\n",
    "\n",
    "# Sort the combined list in descending order based on the values from sum_of_columns\n",
    "sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "\n",
    "print(sorted_names)\n",
    "print(sorted_sum_of_columns)\n",
    "\n",
    "sorted_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Generating Explainer\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      col_name  feature_importance_vals\n",
      "1                  L4_SRC_PORT             4.263129e-02\n",
      "0                      FLOW_ID             4.591741e-07\n",
      "4   FLOW_DURATION_MILLISECONDS             4.117334e-07\n",
      "17             TOTAL_FLOWS_EXP             3.589247e-07\n",
      "5                LAST_SWITCHED             4.495109e-10\n",
      "3               FIRST_SWITCHED             3.042478e-10\n",
      "15                     SRC_TOS             0.000000e+00\n",
      "24                   OUT_BYTES             0.000000e+00\n",
      "23                     IN_PKTS             0.000000e+00\n",
      "22                    IN_BYTES             0.000000e+00\n",
      "21             TOTAL_BYTES_EXP             0.000000e+00\n",
      "20              TOTAL_PKTS_EXP             0.000000e+00\n",
      "19              MAX_IP_PKT_LEN             0.000000e+00\n",
      "18              MIN_IP_PKT_LEN             0.000000e+00\n",
      "16                     DST_TOS             0.000000e+00\n",
      "13            TCP_WIN_SCALE_IN             0.000000e+00\n",
      "14           TCP_WIN_SCALE_OUT             0.000000e+00\n",
      "12              TCP_WIN_MSS_IN             0.000000e+00\n",
      "11             TCP_WIN_MIN_OUT             0.000000e+00\n",
      "10              TCP_WIN_MIN_IN             0.000000e+00\n",
      "9              TCP_WIN_MAX_OUT             0.000000e+00\n",
      "8               TCP_WIN_MAX_IN             0.000000e+00\n",
      "7                    TCP_FLAGS             0.000000e+00\n",
      "6                     PROTOCOL             0.000000e+00\n",
      "2                  L4_DST_PORT             0.000000e+00\n",
      "25                    OUT_PKTS             0.000000e+00\n",
      "---------------------------------------------------------------------------------\n",
      "'L4_SRC_PORT',\n",
      "'FLOW_ID',\n",
      "'FLOW_DURATION_MILLISECONDS',\n",
      "'TOTAL_FLOWS_EXP',\n",
      "'LAST_SWITCHED',\n",
      "'FIRST_SWITCHED',\n",
      "'SRC_TOS',\n",
      "'OUT_BYTES',\n",
      "'IN_PKTS',\n",
      "'IN_BYTES',\n",
      "'TOTAL_BYTES_EXP',\n",
      "'TOTAL_PKTS_EXP',\n",
      "'MAX_IP_PKT_LEN',\n",
      "'MIN_IP_PKT_LEN',\n",
      "'DST_TOS',\n",
      "'TCP_WIN_SCALE_IN',\n",
      "'TCP_WIN_SCALE_OUT',\n",
      "'TCP_WIN_MSS_IN',\n",
      "'TCP_WIN_MIN_OUT',\n",
      "'TCP_WIN_MIN_IN',\n",
      "'TCP_WIN_MAX_OUT',\n",
      "'TCP_WIN_MAX_IN',\n",
      "'TCP_FLAGS',\n",
      "'PROTOCOL',\n",
      "'L4_DST_PORT',\n",
      "'OUT_PKTS',\n",
      "---------------------------------------------------------------------------------\n",
      "CPU times: user 2.15 s, sys: 311 µs, total: 2.15 s\n",
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_file_name = 'SHAPCIC.txt'\n",
    "with open(output_file_name, \"w\") as f:print('',file = f)\n",
    "\n",
    "###here\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Generating Explainer')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "samples = 1\n",
    "\n",
    "#uncomment for single sample\n",
    "# samples = 1\n",
    "\n",
    "Label = label\n",
    "# df.pop('Label')\n",
    "#train.pop('Label')\n",
    "test = X_test\n",
    "train = X_train\n",
    "#df.pop('is_train')\n",
    "start_index = 0\n",
    "\n",
    "#end_index = len(test)\n",
    "end_index = samples\n",
    "#test = test[features]\n",
    "explainer = shap.DeepExplainer(model,train[start_index:end_index].values.astype('float'))\n",
    "shap_values = explainer.shap_values(test[start_index:end_index].values.astype('float'))\n",
    "# shap_values = explainer.shap_values(test[start_index:len(test)].values.astype('float'))\n",
    "\n",
    "# shap.summary_plot(shap_values = shap_values,\n",
    "#                  features = test[start_index:end_index],\n",
    "#                   class_names=[label[0],label[1],label[2],label[3],label[4],label[5],label[6]],show=False)\n",
    "\n",
    "# plt.savefig('DNN_Shap_Summary_Cicids.png')\n",
    "# plt.clf()\n",
    "\n",
    "vals= np.abs(shap_values).mean(1)\n",
    "feature_importance = pd.DataFrame(list(zip(train.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "feature_importance.head()\n",
    "print(feature_importance.to_string())\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "# feature_importance_vals = 'feature_importance_vals'  # Replace with the name of the column you want to extract\n",
    "feature_val = feature_importance['feature_importance_vals'].tolist()\n",
    "\n",
    "# col_name = 'col_name'  # Replace with the name of the column you want to extract\n",
    "feature_name = feature_importance['col_name'].tolist()\n",
    "\n",
    "\n",
    "# for item1, item2 in zip(feature_name, feature_val):\n",
    "#     print(item1, item2)\n",
    "\n",
    "\n",
    "# Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "zipped_lists = list(zip(feature_name, feature_val))\n",
    "zipped_lists.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Convert the sorted result back into separate lists\n",
    "sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "for k in sorted_list1:\n",
    "  with open(output_file_name, \"a\") as f:print(\"df.pop('\",k,\"')\", sep='',file = f)\n",
    "with open(output_file_name, \"a\") as f:print(\"Trial_ =[\", file = f)\n",
    "for k in sorted_list1:\n",
    "  with open(output_file_name, \"a\") as f:print(\"'\",k,\"',\", sep='', file = f)\n",
    "  print(\"'\",k,\"',\", sep='')\n",
    "with open(output_file_name, \"a\") as f:print(\"]\", file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # shap.summary_plot(shap_values = shap_values[0],\n",
    "# #                  features = test[start_index:end_index],\n",
    "# #                   class_names=[label[0],label[1],label[2],label[3],label[4],label[5],label[6]],show=False)\n",
    "\n",
    "\n",
    "# # plt.savefig('DNN_Shap_Summary_Beeswarms.png')\n",
    "# # plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('Generating Sparsity Graph')\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('')\n",
    "\n",
    "# # Find the minimum and maximum values in the list\n",
    "# min_value = min(feature_val)\n",
    "# max_value = max(feature_val)\n",
    "\n",
    "# # Normalize the list to the range [0, 1]\n",
    "# normalized_list = []\n",
    "# for x in feature_val:\n",
    "#     if max_value - min_value == 0:\n",
    "#         normalized_list.append(0)\n",
    "#     else:\n",
    "#         normalized_list.append((x - min_value) / (max_value - min_value))\n",
    "   \n",
    "# # print(feature_name,normalized_list,'\\n')\n",
    "# # for item1, item2 in zip(feature_name, normalized_list):\n",
    "# #     print(item1, item2)\n",
    "\n",
    "# #calculating Sparsity\n",
    "\n",
    "# # Define the threshold\n",
    "# threshold = 1e-10\n",
    "\n",
    "# # Initialize a count variable to keep track of values below the threshold\n",
    "# count_below_threshold = 0\n",
    "\n",
    "# # Iterate through the list and count values below the threshold\n",
    "# for value in normalized_list:\n",
    "#     if value < threshold:\n",
    "#         count_below_threshold += 1\n",
    "\n",
    "# Sparsity = count_below_threshold/len(normalized_list)\n",
    "# Spar = []\n",
    "# print('Sparsity = ',Sparsity)\n",
    "# X_axis = []\n",
    "# #----------------------------------------------------------------------------\n",
    "# for i in range(0, 11):\n",
    "#     i/10\n",
    "#     threshold = i/10\n",
    "#     for value in normalized_list:\n",
    "#         if value < threshold:\n",
    "#             count_below_threshold += 1\n",
    "\n",
    "#     Sparsity = count_below_threshold/len(normalized_list)\n",
    "#     Spar.append(Sparsity)\n",
    "#     X_axis.append(i/10)\n",
    "#     count_below_threshold = 0\n",
    "\n",
    "\n",
    "# #---------------------------------------------------------------------------\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('y_axis_RF = ', Spar ,'', file = f)\n",
    "# with open(output_file_name, \"a\") as f:print('x_axis_RF = ', X_axis ,'', file = f)\n",
    "\n",
    "# plt.clf()\n",
    "\n",
    "# # Create a plot\n",
    "# plt.plot(X_axis, Spar, marker='o', linestyle='-')\n",
    "\n",
    "# # Set labels for the axes\n",
    "# plt.xlabel('X-Axis')\n",
    "# plt.ylabel('Y-Axis')\n",
    "\n",
    "# # Set the title of the plot\n",
    "# plt.title('Values vs. X-Axis')\n",
    "\n",
    "# # Show the plot\n",
    "# # plt.show()\n",
    "# plt.savefig('sparsity.png')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HITL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
