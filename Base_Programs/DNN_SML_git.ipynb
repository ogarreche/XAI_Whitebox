{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 17:54:10.580910: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "/home/oarreche@ads.iu.edu/anaconda3/envs/tf23/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score, precision_score, \n",
    "                             recall_score, f1_score, balanced_accuracy_score, \n",
    "                             matthews_corrcoef)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "import shap\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import innvestigate\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(0)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining features of interest\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Loading Database\n",
      "--------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating features and labels\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'Denial of Service': 642515, 'Port Scanning': 417040, 'None': 195521})\n",
      "---------------------------------------------------------------------------------\n",
      "number of Labels   [195521, 642515, 417040]\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating Training and Testing db\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Number of the training data: 878476\n",
      "Number of the testing data: 376600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining features of interest\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining features of interest')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "req_cols = ['FLOW_DURATION_MILLISECONDS','FIRST_SWITCHED',\n",
    "            'TOTAL_FLOWS_EXP','TCP_WIN_MSS_IN','LAST_SWITCHED',\n",
    "            'TCP_WIN_MAX_IN','TCP_WIN_MIN_IN','TCP_WIN_MIN_OUT',\n",
    "           'PROTOCOL','TCP_WIN_MAX_OUT','TCP_FLAGS',\n",
    "            'TCP_WIN_SCALE_OUT','TCP_WIN_SCALE_IN','SRC_TOS',\n",
    "            'DST_TOS','FLOW_ID','L4_SRC_PORT','L4_DST_PORT',\n",
    "           'MIN_IP_PKT_LEN','MAX_IP_PKT_LEN','TOTAL_PKTS_EXP',\n",
    "           'TOTAL_BYTES_EXP','IN_BYTES','IN_PKTS','OUT_BYTES','OUT_PKTS',\n",
    "            'ALERT']\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "address = '/home/oarreche@ads.iu.edu/HITL/sensor/sensor_db'\n",
    "print('Loading Database')\n",
    "print('--------------------------------------------------')\n",
    "\n",
    "fraction = 0.1\n",
    "fraction2 = 0.01\n",
    "\n",
    "#Denial of Service\n",
    "df0 = pd.read_csv (address + '/dos-03-15-2022-15-44-32.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df1 = pd.read_csv (address + '/dos-03-16-2022-13-45-18.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df2 = pd.read_csv (address + '/dos-03-17-2022-16-22-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df3 = pd.read_csv (address + '/dos-03-18-2022-19-27-05.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df4 = pd.read_csv (address + '/dos-03-19-2022-20-01-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df5 = pd.read_csv (address + '/dos-03-20-2022-14-27-54.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "#Malware\n",
    "#df6 = pd.read_csv ('sensor_db/malware-03-25-2022-17-57-07.csv', usecols=req_cols)\n",
    "\n",
    "#Normal\n",
    "df7 = pd.read_csv  (address + '/normal-03-15-2022-15-43-44.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df8 = pd.read_csv  (address + '/normal-03-16-2022-13-44-27.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df9 = pd.read_csv  (address + '/normal-03-17-2022-16-21-30.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df10 = pd.read_csv (address + '/normal-03-18-2022-19-17-31.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df11 = pd.read_csv (address + '/normal-03-18-2022-19-25-48.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df12 = pd.read_csv (address + '/normal-03-19-2022-20-01-16.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df13 = pd.read_csv (address + '/normal-03-20-2022-14-27-30.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "\n",
    "\n",
    "#PortScanning\n",
    "df14 = pd.read_csv  (address + '/portscanning-03-15-2022-15-44-06.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df15 = pd.read_csv  (address + '/portscanning-03-16-2022-13-44-50.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df16 = pd.read_csv  (address + '/portscanning-03-17-2022-16-22-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df17 = pd.read_csv  (address + '/portscanning-03-18-2022-19-27-05.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df18 = pd.read_csv  (address + '/portscanning-03-19-2022-20-01-45.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df19 = pd.read_csv  (address + '/portscanning-03-20-2022-14-27-49.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "frames = [df0, df1, df2, df3, df4, df5, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19]\n",
    "\n",
    "\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "y = df.pop('ALERT')\n",
    "X = df\n",
    "\n",
    "df_max_scaled = X\n",
    "for col in df_max_scaled.columns:\n",
    "    t = abs(df_max_scaled[col].max())\n",
    "    df_max_scaled[col] = df_max_scaled[col]/t\n",
    "df_max_scaled\n",
    "df = df_max_scaled.assign( Label = y)\n",
    "#df\n",
    "df = df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# IG\n",
    "\n",
    "# df.pop('TOTAL_FLOWS_EXP')\n",
    "# df.pop('LAST_SWITCHED')\n",
    "# df.pop('TCP_WIN_MAX_IN')\n",
    "# df.pop('TCP_WIN_MIN_IN')\n",
    "# df.pop('FIRST_SWITCHED')\n",
    "\n",
    "# df.pop('L4_SRC_PORT')\n",
    "# df.pop('TCP_WIN_SCALE_IN')\n",
    "# df.pop('FLOW_ID')\n",
    "# df.pop('L4_DST_PORT')\n",
    "# df.pop('PROTOCOL')\n",
    "\n",
    "# df.pop('TCP_WIN_MSS_IN')\n",
    "# df.pop('TCP_WIN_MIN_OUT')\n",
    "# df.pop('DST_TOS')\n",
    "# df.pop('FLOW_DURATION_MILLISECONDS')\n",
    "# df.pop('TCP_FLAGS')\n",
    "# df.pop('TCP_WIN_MAX_OUT')\n",
    "# df.pop('TCP_WIN_SCALE_OUT')\n",
    "# df.pop('SRC_TOS')\n",
    "# df.pop('IN_PKTS')\n",
    "# df.pop('OUT_PKTS')\n",
    "# df.pop('OUT_BYTES')\n",
    "\n",
    "# df.pop('IN_BYTES')\n",
    "# df.pop('MIN_IP_PKT_LEN')\n",
    "# df.pop('MAX_IP_PKT_LEN')\n",
    "# df.pop('TOTAL_PKTS_EXP')\n",
    "# df.pop('TOTAL_BYTES_EXP')\n",
    "\n",
    "\n",
    "# LRP \n",
    "# df.pop('TOTAL_FLOWS_EXP')\n",
    "# df.pop('LAST_SWITCHED')\n",
    "# df.pop('TCP_WIN_MIN_IN')\n",
    "# df.pop('TCP_WIN_MAX_IN')\n",
    "# df.pop('FIRST_SWITCHED')\n",
    "\n",
    "# df.pop('TCP_WIN_SCALE_IN')\n",
    "# df.pop('L4_SRC_PORT')\n",
    "# df.pop('L4_DST_PORT')\n",
    "# df.pop('FLOW_ID')\n",
    "# df.pop('PROTOCOL')\n",
    "\n",
    "# df.pop('TCP_WIN_MSS_IN')\n",
    "# df.pop('TCP_FLAGS')\n",
    "# df.pop('FLOW_DURATION_MILLISECONDS')\n",
    "# df.pop('DST_TOS')\n",
    "# df.pop('TCP_WIN_MIN_OUT')\n",
    "# df.pop('TCP_WIN_MAX_OUT')\n",
    "# df.pop('TCP_WIN_SCALE_OUT')\n",
    "# df.pop('SRC_TOS')\n",
    "# df.pop('IN_PKTS')\n",
    "# df.pop('OUT_PKTS')\n",
    "# df.pop('OUT_BYTES')\n",
    "\n",
    "# df.pop('IN_BYTES')\n",
    "# df.pop('MIN_IP_PKT_LEN')\n",
    "# df.pop('MAX_IP_PKT_LEN')\n",
    "# df.pop('TOTAL_PKTS_EXP')\n",
    "# df.pop('TOTAL_BYTES_EXP')\n",
    "\n",
    "\n",
    "# SHAP\n",
    "# df.pop('TCP_WIN_MIN_IN')\n",
    "# df.pop('TCP_WIN_MIN_OUT')\n",
    "# df.pop('TCP_WIN_SCALE_OUT')\n",
    "# df.pop('FLOW_DURATION_MILLISECONDS')\n",
    "# df.pop('TCP_WIN_MAX_IN')\n",
    "\n",
    "# df.pop('TCP_WIN_SCALE_IN')\n",
    "# df.pop('TCP_WIN_MAX_OUT')\n",
    "# df.pop('FLOW_ID')\n",
    "# df.pop('L4_SRC_PORT')\n",
    "# df.pop('TOTAL_FLOWS_EXP')\n",
    "\n",
    "# df.pop('TCP_FLAGS')\n",
    "# df.pop('TCP_WIN_MSS_IN')\n",
    "# df.pop('LAST_SWITCHED')\n",
    "# df.pop('FIRST_SWITCHED')\n",
    "# df.pop('IN_PKTS')\n",
    "# df.pop('OUT_PKTS')\n",
    "# df.pop('IN_BYTES')\n",
    "# df.pop('OUT_BYTES')\n",
    "# df.pop('L4_DST_PORT')\n",
    "# df.pop('SRC_TOS')\n",
    "# df.pop('DST_TOS')\n",
    "\n",
    "# df.pop('PROTOCOL')\n",
    "# df.pop('MIN_IP_PKT_LEN')\n",
    "# df.pop('MAX_IP_PKT_LEN')\n",
    "# df.pop('TOTAL_PKTS_EXP')\n",
    "# df.pop('TOTAL_BYTES_EXP')\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating features and labels')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "print('---------------------------------------------------------------------------------')\n",
    "result_list = [counter['None'],counter['Denial of Service'], counter['Port Scanning']]\n",
    "print('number of Labels  ',result_list)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate Training and Testing db\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating Training and Testing db')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .70\n",
    "#print(df.head())\n",
    "\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "print('Number of the training data:', len(train))\n",
    "print('Number of the testing data:', len(test))\n",
    "\n",
    "features = df.columns[:len(req_cols)-1]\n",
    "\n",
    "y_train, label = pd.factorize(train['Label'])\n",
    "y_test, label = pd.factorize(test['Label'])\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining the DNN model\n",
    "\n",
    "df_y, df_label = pd.factorize(df['Label'])\n",
    "\n",
    "\n",
    "df2 = df.drop(columns=['Label','is_train'])\n",
    "df2['Label'] = df_y\n",
    "df2\n",
    "train = train.drop(columns=['Label','is_train'])\n",
    "\n",
    "test.pop('is_train')\n",
    "test.pop('Label')\n",
    "y_test\n",
    "y_train\n",
    "\n",
    "X_train = train\n",
    "X_test = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining the DNN model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 17:54:55.003654: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-06-11 17:54:55.030383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:67:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.54GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-06-11 17:54:55.030542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:68:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.53GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-06-11 17:54:55.030565: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-06-11 17:54:55.046975: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-06-11 17:54:55.047099: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-06-11 17:54:55.049480: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-06-11 17:54:55.049820: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-06-11 17:54:55.050314: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-06-11 17:54:55.050907: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-06-11 17:54:55.051006: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-06-11 17:54:55.051017: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining the DNN model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# Define the number of nodes per layer\n",
    "nodes_first_layer = 128\n",
    "nodes_second_layer = 64\n",
    "nodes_third_layer = 32\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(len(X_train.columns,))))\n",
    "\n",
    "# First dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_first_layer, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))  # Dropout layer follows the first dense layer\n",
    "\n",
    "# Second dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_second_layer, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))  # Dropout layer follows the second dense layer\n",
    "\n",
    "# Third dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_third_layer, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(tf.keras.layers.Dense(3))  # Cannot use softmax, it is imconpatible with innvestigate\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training the model\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 17:54:55.251326: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 17:54:55.252606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-06-11 17:54:55.252622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 878476 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 17:54:55.269547: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3699850000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878476/878476 [==============================] - 1s 1us/sample - loss: 6.4192 - accuracy: 0.3261 - lr: 0.0010\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 2/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 4.2241 - accuracy: 0.3202 - lr: 0.0010\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 3/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 2.5286 - accuracy: 0.3153 - lr: 0.0010\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 4/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.6604 - accuracy: 0.3158 - lr: 0.0010\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 5/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.3306 - accuracy: 0.3152 - lr: 0.0010\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 6/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.2128 - accuracy: 0.3115 - lr: 0.0010\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 7/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.1581 - accuracy: 0.3050 - lr: 0.0010\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 8/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.1341 - accuracy: 0.2964 - lr: 0.0010\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 9/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.1196 - accuracy: 0.2888 - lr: 0.0010\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 10/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.1127 - accuracy: 0.2834 - lr: 0.0010\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 11/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.1070 - accuracy: 0.2792 - lr: 0.0010\n",
      "---------------------------------------------------------------------------------\n",
      "ELAPSE TIME TRAINING MODEL:  0.1951635241508484 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training the model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Modify model.fit to include the EarlyStopping callback\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train), callbacks=[early_stopping,lr_scheduler])\n",
    "\n",
    "end = time.time()\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('ELAPSE TIME TRAINING MODEL: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Model Prediction\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSE TIME MODEL PREDICTION:  0.17844028075536092 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "33.53133297928837\n",
      "Counter({0: 192329, 2: 125302, 1: 58969})\n",
      "Counter({2: 374063, 0: 2074, 1: 463})\n",
      "33.53133297928837\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Model Prediction')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time()\n",
    "print('ELAPSE TIME MODEL PREDICTION: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "#print(y_pred)\n",
    "ynew = np.argmax(y_pred,axis = 1)\n",
    "#print(ynew)\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "#print(score)\n",
    "pred_label = label[ynew]\n",
    "#print(score)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)\n",
    "\n",
    "label_counts = Counter(y_test)\n",
    "print(label_counts)\n",
    "\n",
    "label_counts = Counter(ynew)\n",
    "print(label_counts)\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample_df = X_test.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.022 -0.066 -0.    ... -0.    -0.     0.   ]\n",
      " [-0.026 -0.069 -0.    ... -0.    -0.     0.   ]\n",
      " [-0.057 -0.056  0.    ... -0.    -0.     0.   ]\n",
      " ...\n",
      " [-0.011  0.007  0.005 ... -0.    -0.     0.   ]\n",
      " [-0.016 -0.     0.002 ... -0.    -0.     0.   ]\n",
      " [-0.014  0.016  0.003 ... -0.    -0.     0.   ]]\n",
      "376600\n",
      "26\n",
      "(376600, 26)\n",
      "<class 'numpy.ndarray'>\n",
      "[[-0.022 -0.066 -0.    ... -0.    -0.     0.   ]\n",
      " [-0.026 -0.069 -0.    ... -0.    -0.     0.   ]\n",
      " [-0.057 -0.056  0.    ... -0.    -0.     0.   ]\n",
      " ...\n",
      " [-0.011  0.007  0.005 ... -0.    -0.     0.   ]\n",
      " [-0.016 -0.     0.002 ... -0.    -0.     0.   ]\n",
      " [-0.014  0.016  0.003 ... -0.    -0.     0.   ]]\n",
      "('LAST_SWITCHED', 'FIRST_SWITCHED', 'FLOW_ID', 'L4_SRC_PORT', 'TCP_WIN_SCALE_IN', 'TOTAL_FLOWS_EXP', 'TCP_WIN_MIN_IN', 'TCP_WIN_MAX_IN', 'PROTOCOL', 'L4_DST_PORT', 'TCP_WIN_MAX_OUT', 'TCP_FLAGS', 'TCP_WIN_MSS_IN', 'FLOW_DURATION_MILLISECONDS', 'TCP_WIN_SCALE_OUT', 'TCP_WIN_MIN_OUT', 'SRC_TOS', 'DST_TOS', 'OUT_PKTS', 'OUT_BYTES', 'IN_PKTS', 'IN_BYTES', 'MIN_IP_PKT_LEN', 'MAX_IP_PKT_LEN', 'TOTAL_PKTS_EXP', 'TOTAL_BYTES_EXP')\n",
      "(20575.978515625, 18025.369140625, 14762.1259765625, 9514.7216796875, 8470.5615234375, 8266.009765625, 7335.80615234375, 6288.1552734375, 1506.6876220703125, 1114.649658203125, 877.1448974609375, 844.831298828125, 633.1632690429688, 529.9351196289062, 389.39068603515625, 197.8826904296875, 197.6292266845703, 110.20832061767578, 1.2419840097427368, 0.5694125890731812, 0.325784832239151, 0.2235884815454483, 0.0, 0.0, 0.0, 0.0)\n",
      "CPU times: user 1min 42s, sys: 36 s, total: 2min 18s\n",
      "Wall time: 17.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('LAST_SWITCHED',\n",
       " 'FIRST_SWITCHED',\n",
       " 'FLOW_ID',\n",
       " 'L4_SRC_PORT',\n",
       " 'TCP_WIN_SCALE_IN',\n",
       " 'TOTAL_FLOWS_EXP',\n",
       " 'TCP_WIN_MIN_IN',\n",
       " 'TCP_WIN_MAX_IN',\n",
       " 'PROTOCOL',\n",
       " 'L4_DST_PORT',\n",
       " 'TCP_WIN_MAX_OUT',\n",
       " 'TCP_FLAGS',\n",
       " 'TCP_WIN_MSS_IN',\n",
       " 'FLOW_DURATION_MILLISECONDS',\n",
       " 'TCP_WIN_SCALE_OUT',\n",
       " 'TCP_WIN_MIN_OUT',\n",
       " 'SRC_TOS',\n",
       " 'DST_TOS',\n",
       " 'OUT_PKTS',\n",
       " 'OUT_BYTES',\n",
       " 'IN_PKTS',\n",
       " 'IN_BYTES',\n",
       " 'MIN_IP_PKT_LEN',\n",
       " 'MAX_IP_PKT_LEN',\n",
       " 'TOTAL_PKTS_EXP',\n",
       " 'TOTAL_BYTES_EXP')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Create an analyzer for the model\n",
    "analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model)\n",
    "\n",
    "# Perform LRP analysis on the input data\n",
    "analysis = analyzer.analyze(X_test)\n",
    "#uncomment for single sample\n",
    "# analysis = analyzer.analyze(single_sample_df)\n",
    "\n",
    "# Perform LRP analysis on a certain number of samples\n",
    "# analysis = analyzer.analyze(X_test.sample(100))\n",
    "\n",
    "# Print or use the analysis results as needed\n",
    "print(analysis)\n",
    "\n",
    "print(len(X_test))\n",
    "print(len(X_test.columns))\n",
    "names = X_test.columns\n",
    "print(analysis.shape)\n",
    "print(type(analysis))\n",
    "scores = pd.DataFrame(analysis)\n",
    "print(analysis)\n",
    "scores_abs = scores.abs()\n",
    "\n",
    "# Calculate the sum of each column\n",
    "sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "names = list(names)\n",
    "\n",
    "sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "# Zip the two lists together\n",
    "combined = list(zip(names, sum_of_columns))\n",
    "\n",
    "# Sort the combined list in descending order based on the values from sum_of_columns\n",
    "sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "\n",
    "print(sorted_names)\n",
    "print(sorted_sum_of_columns)\n",
    "\n",
    "\n",
    "sorted_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.019 -0.068 -0.    ... -0.    -0.     0.   ]\n",
      " [-0.026 -0.069 -0.    ... -0.    -0.     0.   ]\n",
      " [-0.059 -0.056  0.    ... -0.    -0.     0.   ]\n",
      " ...\n",
      " [-0.012  0.004  0.007 ... -0.    -0.     0.   ]\n",
      " [-0.015  0.001  0.003 ... -0.    -0.     0.   ]\n",
      " [-0.012  0.019  0.003 ... -0.    -0.     0.   ]]\n",
      "376600\n",
      "26\n",
      "(376600, 26)\n",
      "<class 'numpy.ndarray'>\n",
      "[[-0.019 -0.068 -0.    ... -0.    -0.     0.   ]\n",
      " [-0.026 -0.069 -0.    ... -0.    -0.     0.   ]\n",
      " [-0.059 -0.056  0.    ... -0.    -0.     0.   ]\n",
      " ...\n",
      " [-0.012  0.004  0.007 ... -0.    -0.     0.   ]\n",
      " [-0.015  0.001  0.003 ... -0.    -0.     0.   ]\n",
      " [-0.012  0.019  0.003 ... -0.    -0.     0.   ]]\n",
      "('LAST_SWITCHED', 'FIRST_SWITCHED', 'FLOW_ID', 'L4_SRC_PORT', 'TOTAL_FLOWS_EXP', 'TCP_WIN_SCALE_IN', 'TCP_WIN_MIN_IN', 'TCP_WIN_MAX_IN', 'PROTOCOL', 'L4_DST_PORT', 'TCP_WIN_MAX_OUT', 'TCP_FLAGS', 'TCP_WIN_MSS_IN', 'FLOW_DURATION_MILLISECONDS', 'TCP_WIN_SCALE_OUT', 'TCP_WIN_MIN_OUT', 'SRC_TOS', 'DST_TOS', 'OUT_PKTS', 'OUT_BYTES', 'IN_PKTS', 'IN_BYTES', 'MIN_IP_PKT_LEN', 'MAX_IP_PKT_LEN', 'TOTAL_PKTS_EXP', 'TOTAL_BYTES_EXP')\n",
      "(19890.193359375, 17304.6171875, 15287.94140625, 10160.48828125, 8434.8984375, 8429.580078125, 7819.689453125, 6004.88330078125, 1633.395751953125, 1320.6859130859375, 911.2140502929688, 861.8372192382812, 654.7435302734375, 519.5130004882812, 389.8062438964844, 220.86050415039062, 200.62255859375, 116.81381225585938, 1.2877233028411865, 0.5975202322006226, 0.34427836537361145, 0.2324230968952179, 0.0, 0.0, 0.0, 0.0)\n",
      "CPU times: user 4.42 s, sys: 1.17 s, total: 5.59 s\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('LAST_SWITCHED',\n",
       " 'FIRST_SWITCHED',\n",
       " 'FLOW_ID',\n",
       " 'L4_SRC_PORT',\n",
       " 'TOTAL_FLOWS_EXP',\n",
       " 'TCP_WIN_SCALE_IN',\n",
       " 'TCP_WIN_MIN_IN',\n",
       " 'TCP_WIN_MAX_IN',\n",
       " 'PROTOCOL',\n",
       " 'L4_DST_PORT',\n",
       " 'TCP_WIN_MAX_OUT',\n",
       " 'TCP_FLAGS',\n",
       " 'TCP_WIN_MSS_IN',\n",
       " 'FLOW_DURATION_MILLISECONDS',\n",
       " 'TCP_WIN_SCALE_OUT',\n",
       " 'TCP_WIN_MIN_OUT',\n",
       " 'SRC_TOS',\n",
       " 'DST_TOS',\n",
       " 'OUT_PKTS',\n",
       " 'OUT_BYTES',\n",
       " 'IN_PKTS',\n",
       " 'IN_BYTES',\n",
       " 'MIN_IP_PKT_LEN',\n",
       " 'MAX_IP_PKT_LEN',\n",
       " 'TOTAL_PKTS_EXP',\n",
       " 'TOTAL_BYTES_EXP')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create an analyzer for the model\n",
    "analyzer = innvestigate.create_analyzer(\"lrp.z\", model)\n",
    "\n",
    "# Perform LRP analysis on the input data\n",
    "analysis = analyzer.analyze(X_test)\n",
    "\n",
    "#uncomment for single sample\n",
    "# analysis = analyzer.analyze(single_sample_df)\n",
    "\n",
    "# Perform LRP analysis on a certain number of samples\n",
    "# analysis = analyzer.analyze(X_test.sample(2500))\n",
    "\n",
    "\n",
    "# Print or use the analysis results as needed\n",
    "print(analysis)\n",
    "print(len(X_test))\n",
    "print(len(X_test.columns))\n",
    "names = X_test.columns\n",
    "print(analysis.shape)\n",
    "print(type(analysis))\n",
    "scores = pd.DataFrame(analysis)\n",
    "print(analysis)\n",
    "scores_abs = scores.abs()\n",
    "\n",
    "# Calculate the sum of each column\n",
    "sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "names = list(names)\n",
    "\n",
    "sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "# Zip the two lists together\n",
    "combined = list(zip(names, sum_of_columns))\n",
    "\n",
    "# Sort the combined list in descending order based on the values from sum_of_columns\n",
    "sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "\n",
    "print(sorted_names)\n",
    "print(sorted_sum_of_columns)\n",
    "\n",
    "sorted_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Generating Explainer\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oarreche@ads.iu.edu/anaconda3/envs/tf23/lib/python3.8/site-packages/shap/explainers/tf_utils.py:28: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n",
      "                      col_name  feature_importance_vals\n",
      "4   FLOW_DURATION_MILLISECONDS             6.422987e-02\n",
      "10              TCP_WIN_MIN_IN             6.014560e-02\n",
      "13            TCP_WIN_SCALE_IN             3.745111e-02\n",
      "14           TCP_WIN_SCALE_OUT             3.461838e-02\n",
      "9              TCP_WIN_MAX_OUT             2.616915e-02\n",
      "8               TCP_WIN_MAX_IN             2.510519e-02\n",
      "11             TCP_WIN_MIN_OUT             1.434566e-02\n",
      "1                  L4_SRC_PORT             1.400353e-02\n",
      "0                      FLOW_ID             3.774872e-03\n",
      "12              TCP_WIN_MSS_IN             2.534154e-03\n",
      "7                    TCP_FLAGS             2.251392e-03\n",
      "17             TOTAL_FLOWS_EXP             1.861268e-03\n",
      "25                    OUT_PKTS             9.037743e-06\n",
      "5                LAST_SWITCHED             7.909931e-06\n",
      "3               FIRST_SWITCHED             5.697850e-06\n",
      "23                     IN_PKTS             6.323228e-07\n",
      "22                    IN_BYTES             4.389771e-07\n",
      "24                   OUT_BYTES             2.156205e-07\n",
      "2                  L4_DST_PORT             0.000000e+00\n",
      "15                     SRC_TOS             0.000000e+00\n",
      "16                     DST_TOS             0.000000e+00\n",
      "6                     PROTOCOL             0.000000e+00\n",
      "18              MIN_IP_PKT_LEN             0.000000e+00\n",
      "19              MAX_IP_PKT_LEN             0.000000e+00\n",
      "20              TOTAL_PKTS_EXP             0.000000e+00\n",
      "21             TOTAL_BYTES_EXP             0.000000e+00\n",
      "---------------------------------------------------------------------------------\n",
      "'FLOW_DURATION_MILLISECONDS',\n",
      "'TCP_WIN_MIN_IN',\n",
      "'TCP_WIN_SCALE_IN',\n",
      "'TCP_WIN_SCALE_OUT',\n",
      "'TCP_WIN_MAX_OUT',\n",
      "'TCP_WIN_MAX_IN',\n",
      "'TCP_WIN_MIN_OUT',\n",
      "'L4_SRC_PORT',\n",
      "'FLOW_ID',\n",
      "'TCP_WIN_MSS_IN',\n",
      "'TCP_FLAGS',\n",
      "'TOTAL_FLOWS_EXP',\n",
      "'OUT_PKTS',\n",
      "'LAST_SWITCHED',\n",
      "'FIRST_SWITCHED',\n",
      "'IN_PKTS',\n",
      "'IN_BYTES',\n",
      "'OUT_BYTES',\n",
      "'L4_DST_PORT',\n",
      "'SRC_TOS',\n",
      "'DST_TOS',\n",
      "'PROTOCOL',\n",
      "'MIN_IP_PKT_LEN',\n",
      "'MAX_IP_PKT_LEN',\n",
      "'TOTAL_PKTS_EXP',\n",
      "'TOTAL_BYTES_EXP',\n",
      "---------------------------------------------------------------------------------\n",
      "CPU times: user 7min 32s, sys: 25.7 s, total: 7min 57s\n",
      "Wall time: 57.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_file_name = 'SHAPCIC.txt'\n",
    "with open(output_file_name, \"w\") as f:print('',file = f)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Generating Explainer')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "samples = 2500\n",
    "\n",
    "#uncomment for single sample\n",
    "# samples = 1\n",
    "\n",
    "Label = label\n",
    "\n",
    "test = X_test\n",
    "train = X_train\n",
    "\n",
    "start_index = 0\n",
    "end_index = samples\n",
    "\n",
    "explainer = shap.DeepExplainer(model,train[start_index:end_index].values.astype('float'))\n",
    "shap_values = explainer.shap_values(test[start_index:end_index].values.astype('float'))\n",
    "# shap_values = explainer.shap_values(test[start_index:len(test)].values.astype('float'))\n",
    "\n",
    "vals= np.abs(shap_values).mean(1)\n",
    "feature_importance = pd.DataFrame(list(zip(train.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "feature_importance.head()\n",
    "print(feature_importance.to_string())\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "# feature_importance_vals = 'feature_importance_vals'  # Replace with the name of the column you want to extract\n",
    "feature_val = feature_importance['feature_importance_vals'].tolist()\n",
    "\n",
    "# col_name = 'col_name'  # Replace with the name of the column you want to extract\n",
    "feature_name = feature_importance['col_name'].tolist()\n",
    "\n",
    "# Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "zipped_lists = list(zip(feature_name, feature_val))\n",
    "zipped_lists.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Convert the sorted result back into separate lists\n",
    "sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "for k in sorted_list1:\n",
    "  with open(output_file_name, \"a\") as f:print(\"df.pop('\",k,\"')\", sep='',file = f)\n",
    "with open(output_file_name, \"a\") as f:print(\"Trial_ =[\", file = f)\n",
    "for k in sorted_list1:\n",
    "  with open(output_file_name, \"a\") as f:print(\"'\",k,\"',\", sep='', file = f)\n",
    "  print(\"'\",k,\"',\", sep='')\n",
    "with open(output_file_name, \"a\") as f:print(\"]\", file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HITL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
