{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score, precision_score, \n",
    "                             recall_score, f1_score, balanced_accuracy_score, \n",
    "                             matthews_corrcoef)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "import shap\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import innvestigate\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(0)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "output_file_name = 'SML_XAI.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_metrics (name_model,predictions,true_labels):\n",
    "\n",
    "    name = name_model\n",
    "    pred_label = predictions\n",
    "    y_test_01 = true_labels \n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(name, file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    # voting_acc_01 = Acc\n",
    "    # voting_pre_01 = Precision\n",
    "    # weighed_avg_rec_01 = Recall\n",
    "    # weighed_avg_f1_01 = F1\n",
    "    # weighed_avg_bacc_01 = BACC\n",
    "    # weighed_avg_mcc_01 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "    return Acc, Precision, Recall, F1, BACC, MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining features of interest\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Loading Database\n",
      "--------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating features and labels\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'Denial of Service': 642515, 'Port Scanning': 417040, 'None': 195521})\n",
      "---------------------------------------------------------------------------------\n",
      "number of Labels   [195521, 642515, 417040]\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating Training and Testing db\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Number of the training data: 878476\n",
      "Number of the testing data: 376600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining features of interest\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining features of interest')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "req_cols = ['FLOW_DURATION_MILLISECONDS','FIRST_SWITCHED',\n",
    "            'TOTAL_FLOWS_EXP','TCP_WIN_MSS_IN','LAST_SWITCHED',\n",
    "            'TCP_WIN_MAX_IN','TCP_WIN_MIN_IN','TCP_WIN_MIN_OUT',\n",
    "           'PROTOCOL','TCP_WIN_MAX_OUT','TCP_FLAGS',\n",
    "            'TCP_WIN_SCALE_OUT','TCP_WIN_SCALE_IN','SRC_TOS',\n",
    "            'DST_TOS','FLOW_ID','L4_SRC_PORT','L4_DST_PORT',\n",
    "           'MIN_IP_PKT_LEN','MAX_IP_PKT_LEN','TOTAL_PKTS_EXP',\n",
    "           'TOTAL_BYTES_EXP','IN_BYTES','IN_PKTS','OUT_BYTES','OUT_PKTS',\n",
    "            'ALERT']\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "address = '/home/oarreche@ads.iu.edu/HITL/sensor/sensor_db'\n",
    "print('Loading Database')\n",
    "print('--------------------------------------------------')\n",
    "\n",
    "fraction = 0.1\n",
    "fraction2 = 0.01\n",
    "\n",
    "#Denial of Service\n",
    "df0 = pd.read_csv (address + '/dos-03-15-2022-15-44-32.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df1 = pd.read_csv (address + '/dos-03-16-2022-13-45-18.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df2 = pd.read_csv (address + '/dos-03-17-2022-16-22-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df3 = pd.read_csv (address + '/dos-03-18-2022-19-27-05.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df4 = pd.read_csv (address + '/dos-03-19-2022-20-01-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df5 = pd.read_csv (address + '/dos-03-20-2022-14-27-54.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "#Malware\n",
    "#df6 = pd.read_csv ('sensor_db/malware-03-25-2022-17-57-07.csv', usecols=req_cols)\n",
    "\n",
    "#Normal\n",
    "df7 = pd.read_csv  (address + '/normal-03-15-2022-15-43-44.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df8 = pd.read_csv  (address + '/normal-03-16-2022-13-44-27.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df9 = pd.read_csv  (address + '/normal-03-17-2022-16-21-30.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df10 = pd.read_csv (address + '/normal-03-18-2022-19-17-31.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df11 = pd.read_csv (address + '/normal-03-18-2022-19-25-48.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df12 = pd.read_csv (address + '/normal-03-19-2022-20-01-16.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df13 = pd.read_csv (address + '/normal-03-20-2022-14-27-30.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "\n",
    "\n",
    "#PortScanning\n",
    "df14 = pd.read_csv  (address + '/portscanning-03-15-2022-15-44-06.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df15 = pd.read_csv  (address + '/portscanning-03-16-2022-13-44-50.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df16 = pd.read_csv  (address + '/portscanning-03-17-2022-16-22-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df17 = pd.read_csv  (address + '/portscanning-03-18-2022-19-27-05.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df18 = pd.read_csv  (address + '/portscanning-03-19-2022-20-01-45.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df19 = pd.read_csv  (address + '/portscanning-03-20-2022-14-27-49.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "frames = [df0, df1, df2, df3, df4, df5, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19]\n",
    "\n",
    "\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "y = df.pop('ALERT')\n",
    "X = df\n",
    "\n",
    "df_max_scaled = X\n",
    "for col in df_max_scaled.columns:\n",
    "    t = abs(df_max_scaled[col].max())\n",
    "    df_max_scaled[col] = df_max_scaled[col]/t\n",
    "df_max_scaled\n",
    "df = df_max_scaled.assign( Label = y)\n",
    "#df\n",
    "df = df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# IG\n",
    "\n",
    "# df.pop('TOTAL_FLOWS_EXP')\n",
    "# df.pop('LAST_SWITCHED')\n",
    "# df.pop('TCP_WIN_MAX_IN')\n",
    "# df.pop('TCP_WIN_MIN_IN')\n",
    "# df.pop('FIRST_SWITCHED')\n",
    "\n",
    "# df.pop('L4_SRC_PORT')\n",
    "# df.pop('TCP_WIN_SCALE_IN')\n",
    "# df.pop('FLOW_ID')\n",
    "# df.pop('L4_DST_PORT')\n",
    "# df.pop('PROTOCOL')\n",
    "\n",
    "# df.pop('TCP_WIN_MSS_IN')\n",
    "# df.pop('TCP_WIN_MIN_OUT')\n",
    "# df.pop('DST_TOS')\n",
    "# df.pop('FLOW_DURATION_MILLISECONDS')\n",
    "# df.pop('TCP_FLAGS')\n",
    "# df.pop('TCP_WIN_MAX_OUT')\n",
    "# df.pop('TCP_WIN_SCALE_OUT')\n",
    "# df.pop('SRC_TOS')\n",
    "# df.pop('IN_PKTS')\n",
    "# df.pop('OUT_PKTS')\n",
    "# df.pop('OUT_BYTES')\n",
    "\n",
    "# df.pop('IN_BYTES')\n",
    "# df.pop('MIN_IP_PKT_LEN')\n",
    "# df.pop('MAX_IP_PKT_LEN')\n",
    "# df.pop('TOTAL_PKTS_EXP')\n",
    "# df.pop('TOTAL_BYTES_EXP')\n",
    "\n",
    "\n",
    "# LRP \n",
    "# df.pop('TOTAL_FLOWS_EXP')\n",
    "# df.pop('LAST_SWITCHED')\n",
    "# df.pop('TCP_WIN_MIN_IN')\n",
    "# df.pop('TCP_WIN_MAX_IN')\n",
    "# df.pop('FIRST_SWITCHED')\n",
    "\n",
    "# df.pop('TCP_WIN_SCALE_IN')\n",
    "# df.pop('L4_SRC_PORT')\n",
    "# df.pop('L4_DST_PORT')\n",
    "# df.pop('FLOW_ID')\n",
    "# df.pop('PROTOCOL')\n",
    "\n",
    "# df.pop('TCP_WIN_MSS_IN')\n",
    "# df.pop('TCP_FLAGS')\n",
    "# df.pop('FLOW_DURATION_MILLISECONDS')\n",
    "# df.pop('DST_TOS')\n",
    "# df.pop('TCP_WIN_MIN_OUT')\n",
    "# df.pop('TCP_WIN_MAX_OUT')\n",
    "# df.pop('TCP_WIN_SCALE_OUT')\n",
    "# df.pop('SRC_TOS')\n",
    "# df.pop('IN_PKTS')\n",
    "# df.pop('OUT_PKTS')\n",
    "# df.pop('OUT_BYTES')\n",
    "\n",
    "# df.pop('IN_BYTES')\n",
    "# df.pop('MIN_IP_PKT_LEN')\n",
    "# df.pop('MAX_IP_PKT_LEN')\n",
    "# df.pop('TOTAL_PKTS_EXP')\n",
    "# df.pop('TOTAL_BYTES_EXP')\n",
    "\n",
    "\n",
    "# SHAP\n",
    "# df.pop('TCP_WIN_MIN_IN')\n",
    "# df.pop('TCP_WIN_MIN_OUT')\n",
    "# df.pop('TCP_WIN_SCALE_OUT')\n",
    "# df.pop('FLOW_DURATION_MILLISECONDS')\n",
    "# df.pop('TCP_WIN_MAX_IN')\n",
    "\n",
    "# df.pop('TCP_WIN_SCALE_IN')\n",
    "# df.pop('TCP_WIN_MAX_OUT')\n",
    "# df.pop('FLOW_ID')\n",
    "# df.pop('L4_SRC_PORT')\n",
    "# df.pop('TOTAL_FLOWS_EXP')\n",
    "\n",
    "# df.pop('TCP_FLAGS')\n",
    "# df.pop('TCP_WIN_MSS_IN')\n",
    "# df.pop('LAST_SWITCHED')\n",
    "# df.pop('FIRST_SWITCHED')\n",
    "# df.pop('IN_PKTS')\n",
    "# df.pop('OUT_PKTS')\n",
    "# df.pop('IN_BYTES')\n",
    "# df.pop('OUT_BYTES')\n",
    "# df.pop('L4_DST_PORT')\n",
    "# df.pop('SRC_TOS')\n",
    "# df.pop('DST_TOS')\n",
    "\n",
    "# df.pop('PROTOCOL')\n",
    "# df.pop('MIN_IP_PKT_LEN')\n",
    "# df.pop('MAX_IP_PKT_LEN')\n",
    "# df.pop('TOTAL_PKTS_EXP')\n",
    "# df.pop('TOTAL_BYTES_EXP')\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating features and labels')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "print('---------------------------------------------------------------------------------')\n",
    "result_list = [counter['None'],counter['Denial of Service'], counter['Port Scanning']]\n",
    "print('number of Labels  ',result_list)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate Training and Testing db\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating Training and Testing db')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .70\n",
    "#print(df.head())\n",
    "\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "print('Number of the training data:', len(train))\n",
    "print('Number of the testing data:', len(test))\n",
    "\n",
    "features = df.columns[:len(req_cols)-1]\n",
    "\n",
    "y_train, label = pd.factorize(train['Label'])\n",
    "y_test, label = pd.factorize(test['Label'])\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining the DNN model\n",
    "\n",
    "df_y, df_label = pd.factorize(df['Label'])\n",
    "\n",
    "\n",
    "df2 = df.drop(columns=['Label','is_train'])\n",
    "df2['Label'] = df_y\n",
    "df2\n",
    "train = train.drop(columns=['Label','is_train'])\n",
    "\n",
    "test.pop('is_train')\n",
    "test.pop('Label')\n",
    "y_test\n",
    "y_train\n",
    "\n",
    "X_train = train\n",
    "X_test = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining the DNN model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining the DNN model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# Define the number of nodes per layer\n",
    "nodes_first_layer = 128\n",
    "nodes_second_layer = 64\n",
    "nodes_third_layer = 32\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(len(X_train.columns,))))\n",
    "\n",
    "# First dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_first_layer, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))  # Dropout layer follows the first dense layer\n",
    "\n",
    "# Second dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_second_layer, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))  # Dropout layer follows the second dense layer\n",
    "\n",
    "# Third dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_third_layer, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(tf.keras.layers.Dense(3))  # Cannot use softmax, it is imconpatible with innvestigate\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training the model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Train on 878476 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 1/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 3.4959 - accuracy: 0.4248 - lr: 0.0010\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 2/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 2.4949 - accuracy: 0.4944 - lr: 0.0010\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 3/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.9170 - accuracy: 0.5460 - lr: 0.0010\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 4/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.5343 - accuracy: 0.5843 - lr: 0.0010\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 5/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.2752 - accuracy: 0.6161 - lr: 0.0010\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 6/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 1.1008 - accuracy: 0.6408 - lr: 0.0010\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 7/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.9991 - accuracy: 0.6599 - lr: 0.0010\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 8/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.9356 - accuracy: 0.6768 - lr: 0.0010\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 9/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8979 - accuracy: 0.6907 - lr: 0.0010\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 10/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8758 - accuracy: 0.7035 - lr: 0.0010\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 11/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8595 - accuracy: 0.7138 - lr: 0.0010\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8482 - accuracy: 0.7248 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8472 - accuracy: 0.7262 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 14/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8450 - accuracy: 0.7267 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 15/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8459 - accuracy: 0.7264 - lr: 1.0000e-07\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 16/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8454 - accuracy: 0.7265 - lr: 1.0000e-08\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 17/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8469 - accuracy: 0.7259 - lr: 1.0000e-09\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 18/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8457 - accuracy: 0.7274 - lr: 1.0000e-10\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 19/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8459 - accuracy: 0.7265 - lr: 1.0000e-11\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 20/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8466 - accuracy: 0.7265 - lr: 1.0000e-12\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.0000001044244145e-13.\n",
      "Epoch 21/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8456 - accuracy: 0.7268 - lr: 1.0000e-13\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.0000001179769417e-14.\n",
      "Epoch 22/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8462 - accuracy: 0.7259 - lr: 1.0000e-14\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.0000001518582595e-15.\n",
      "Epoch 23/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8459 - accuracy: 0.7260 - lr: 1.0000e-15\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.0000001095066122e-16.\n",
      "Epoch 24/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8455 - accuracy: 0.7267 - lr: 1.0000e-16\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.0000000830368326e-17.\n",
      "Epoch 25/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8450 - accuracy: 0.7263 - lr: 1.0000e-17\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.0000000664932204e-18.\n",
      "Epoch 26/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8472 - accuracy: 0.7258 - lr: 1.0000e-18\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.000000045813705e-19.\n",
      "Epoch 27/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8463 - accuracy: 0.7259 - lr: 1.0000e-19\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.000000032889008e-20.\n",
      "Epoch 28/1000\n",
      "878476/878476 [==============================] - 1s 1us/sample - loss: 0.8455 - accuracy: 0.7263 - lr: 1.0000e-20\n",
      "---------------------------------------------------------------------------------\n",
      "ELAPSE TIME TRAINING MODEL:  0.45901650190353394 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training the model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Modify model.fit to include the EarlyStopping callback\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train), callbacks=[early_stopping,lr_scheduler])\n",
    "\n",
    "end = time.time()\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('ELAPSE TIME TRAINING MODEL: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Model Prediction\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSE TIME MODEL PREDICTION:  0.20019287665685018 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "84.82819968135954\n",
      "Counter({0: 192329, 2: 125302, 1: 58969})\n",
      "Counter({0: 201598, 2: 172875, 1: 2127})\n",
      "84.82819968135954\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Model Prediction')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time()\n",
    "print('ELAPSE TIME MODEL PREDICTION: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "#print(y_pred)\n",
    "ynew = np.argmax(y_pred,axis = 1)\n",
    "#print(ynew)\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "#print(score)\n",
    "pred_label = label[ynew]\n",
    "#print(score)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)\n",
    "\n",
    "label_counts = Counter(y_test)\n",
    "print(label_counts)\n",
    "\n",
    "label_counts = Counter(ynew)\n",
    "print(label_counts)\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "          0       1         2\n",
      "0  192034.0     0.0     295.0\n",
      "1    9564.0  2127.0   47278.0\n",
      "2       0.0     0.0  125302.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.8482819968135953\n",
      "Precision total:  0.8924572597077574\n",
      "Recall total:  0.6781786564448292\n",
      "F1 total:  0.6283514903077259\n",
      "BACC total:  0.6781786564448292\n",
      "MCC total:  0.7645696095568848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8482819968135953,\n",
       " 0.8924572597077574,\n",
       " 0.6781786564448292,\n",
       " 0.6283514903077259,\n",
       " 0.6781786564448292,\n",
       " 0.7645696095568848)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_metrics('dnn', ynew, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample_df = X_test.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.064 -0.04  -0.    ...  0.     0.    -0.   ]\n",
      " [ 0.05  -0.035 -0.    ...  0.     0.    -0.   ]\n",
      " [ 0.039 -0.084 -0.    ...  0.     0.    -0.   ]\n",
      " ...\n",
      " [ 0.032  0.096  0.08  ... -0.     0.    -0.   ]\n",
      " [ 0.049  0.091  0.021 ... -0.    -0.    -0.   ]\n",
      " [ 0.039  0.094  0.022 ... -0.    -0.    -0.   ]]\n",
      "376600\n",
      "26\n",
      "(376600, 26)\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 0.064 -0.04  -0.    ...  0.     0.    -0.   ]\n",
      " [ 0.05  -0.035 -0.    ...  0.     0.    -0.   ]\n",
      " [ 0.039 -0.084 -0.    ...  0.     0.    -0.   ]\n",
      " ...\n",
      " [ 0.032  0.096  0.08  ... -0.     0.    -0.   ]\n",
      " [ 0.049  0.091  0.021 ... -0.    -0.    -0.   ]\n",
      " [ 0.039  0.094  0.022 ... -0.    -0.    -0.   ]]\n",
      "('LAST_SWITCHED', 'FIRST_SWITCHED', 'FLOW_ID', 'L4_SRC_PORT', 'TCP_WIN_MIN_IN', 'TCP_WIN_MAX_IN', 'TOTAL_FLOWS_EXP', 'L4_DST_PORT', 'PROTOCOL', 'TCP_WIN_SCALE_IN', 'TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS', 'TCP_WIN_MIN_OUT', 'TCP_WIN_MAX_OUT', 'TCP_WIN_SCALE_OUT', 'TCP_WIN_MSS_IN', 'DST_TOS', 'SRC_TOS', 'OUT_PKTS', 'OUT_BYTES', 'IN_PKTS', 'IN_BYTES', 'MIN_IP_PKT_LEN', 'MAX_IP_PKT_LEN', 'TOTAL_PKTS_EXP', 'TOTAL_BYTES_EXP')\n",
      "(25524.546875, 25458.408203125, 19162.685546875, 17269.87109375, 16063.9130859375, 15959.560546875, 14052.0419921875, 10568.65625, 3195.387451171875, 2119.53369140625, 403.2012939453125, 385.33233642578125, 367.6079406738281, 353.84014892578125, 324.8810119628906, 247.33749389648438, 200.6614990234375, 118.13121032714844, 1.086195707321167, 0.6206826567649841, 0.34416019916534424, 0.19606168568134308, 0.0, 0.0, 0.0, 0.0)\n",
      "CPU times: user 2min 3s, sys: 39.2 s, total: 2min 42s\n",
      "Wall time: 22.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('LAST_SWITCHED',\n",
       " 'FIRST_SWITCHED',\n",
       " 'FLOW_ID',\n",
       " 'L4_SRC_PORT',\n",
       " 'TCP_WIN_MIN_IN',\n",
       " 'TCP_WIN_MAX_IN',\n",
       " 'TOTAL_FLOWS_EXP',\n",
       " 'L4_DST_PORT',\n",
       " 'PROTOCOL',\n",
       " 'TCP_WIN_SCALE_IN',\n",
       " 'TCP_FLAGS',\n",
       " 'FLOW_DURATION_MILLISECONDS',\n",
       " 'TCP_WIN_MIN_OUT',\n",
       " 'TCP_WIN_MAX_OUT',\n",
       " 'TCP_WIN_SCALE_OUT',\n",
       " 'TCP_WIN_MSS_IN',\n",
       " 'DST_TOS',\n",
       " 'SRC_TOS',\n",
       " 'OUT_PKTS',\n",
       " 'OUT_BYTES',\n",
       " 'IN_PKTS',\n",
       " 'IN_BYTES',\n",
       " 'MIN_IP_PKT_LEN',\n",
       " 'MAX_IP_PKT_LEN',\n",
       " 'TOTAL_PKTS_EXP',\n",
       " 'TOTAL_BYTES_EXP')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Create an analyzer for the model\n",
    "analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model)\n",
    "\n",
    "# Perform LRP analysis on the input data\n",
    "analysis = analyzer.analyze(X_test)\n",
    "#uncomment for single sample\n",
    "# analysis = analyzer.analyze(single_sample_df)\n",
    "\n",
    "# Perform LRP analysis on a certain number of samples\n",
    "# analysis = analyzer.analyze(X_test.sample(100))\n",
    "\n",
    "# Print or use the analysis results as needed\n",
    "print(analysis)\n",
    "\n",
    "print(len(X_test))\n",
    "print(len(X_test.columns))\n",
    "names = X_test.columns\n",
    "print(analysis.shape)\n",
    "print(type(analysis))\n",
    "scores = pd.DataFrame(analysis)\n",
    "print(analysis)\n",
    "scores_abs = scores.abs()\n",
    "\n",
    "# Calculate the sum of each column\n",
    "sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "names = list(names)\n",
    "\n",
    "sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "# Zip the two lists together\n",
    "combined = list(zip(names, sum_of_columns))\n",
    "\n",
    "# Sort the combined list in descending order based on the values from sum_of_columns\n",
    "sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "\n",
    "print(sorted_names)\n",
    "print(sorted_sum_of_columns)\n",
    "\n",
    "\n",
    "sorted_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.077 -0.034 -0.    ...  0.     0.    -0.   ]\n",
      " [ 0.052 -0.025 -0.    ...  0.     0.    -0.   ]\n",
      " [ 0.036 -0.085 -0.    ... -0.     0.    -0.   ]\n",
      " ...\n",
      " [ 0.034  0.102  0.088 ...  0.     0.    -0.   ]\n",
      " [ 0.076  0.076  0.024 ...  0.    -0.    -0.   ]\n",
      " [ 0.078  0.08   0.025 ...  0.    -0.    -0.   ]]\n",
      "376600\n",
      "26\n",
      "(376600, 26)\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 0.077 -0.034 -0.    ...  0.     0.    -0.   ]\n",
      " [ 0.052 -0.025 -0.    ...  0.     0.    -0.   ]\n",
      " [ 0.036 -0.085 -0.    ... -0.     0.    -0.   ]\n",
      " ...\n",
      " [ 0.034  0.102  0.088 ...  0.     0.    -0.   ]\n",
      " [ 0.076  0.076  0.024 ...  0.    -0.    -0.   ]\n",
      " [ 0.078  0.08   0.025 ...  0.    -0.    -0.   ]]\n",
      "('FIRST_SWITCHED', 'LAST_SWITCHED', 'FLOW_ID', 'TCP_WIN_MAX_IN', 'TOTAL_FLOWS_EXP', 'L4_SRC_PORT', 'TCP_WIN_MIN_IN', 'L4_DST_PORT', 'PROTOCOL', 'TCP_WIN_SCALE_IN', 'FLOW_DURATION_MILLISECONDS', 'TCP_WIN_MSS_IN', 'TCP_WIN_MAX_OUT', 'TCP_FLAGS', 'TCP_WIN_MIN_OUT', 'TCP_WIN_SCALE_OUT', 'DST_TOS', 'SRC_TOS', 'OUT_PKTS', 'OUT_BYTES', 'IN_PKTS', 'IN_BYTES', 'MIN_IP_PKT_LEN', 'MAX_IP_PKT_LEN', 'TOTAL_PKTS_EXP', 'TOTAL_BYTES_EXP')\n",
      "(27416.2421875, 25519.86328125, 18489.583984375, 17260.0390625, 16172.4482421875, 15188.734375, 13900.595703125, 10431.310546875, 3975.55517578125, 2991.5927734375, 460.5320739746094, 420.30352783203125, 405.4984436035156, 354.1634826660156, 333.5752868652344, 314.2555847167969, 177.81304931640625, 127.20597839355469, 1.0960392951965332, 0.696509063243866, 0.37182021141052246, 0.14881348609924316, 0.0, 0.0, 0.0, 0.0)\n",
      "CPU times: user 5.12 s, sys: 1.23 s, total: 6.36 s\n",
      "Wall time: 1.51 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('FIRST_SWITCHED',\n",
       " 'LAST_SWITCHED',\n",
       " 'FLOW_ID',\n",
       " 'TCP_WIN_MAX_IN',\n",
       " 'TOTAL_FLOWS_EXP',\n",
       " 'L4_SRC_PORT',\n",
       " 'TCP_WIN_MIN_IN',\n",
       " 'L4_DST_PORT',\n",
       " 'PROTOCOL',\n",
       " 'TCP_WIN_SCALE_IN',\n",
       " 'FLOW_DURATION_MILLISECONDS',\n",
       " 'TCP_WIN_MSS_IN',\n",
       " 'TCP_WIN_MAX_OUT',\n",
       " 'TCP_FLAGS',\n",
       " 'TCP_WIN_MIN_OUT',\n",
       " 'TCP_WIN_SCALE_OUT',\n",
       " 'DST_TOS',\n",
       " 'SRC_TOS',\n",
       " 'OUT_PKTS',\n",
       " 'OUT_BYTES',\n",
       " 'IN_PKTS',\n",
       " 'IN_BYTES',\n",
       " 'MIN_IP_PKT_LEN',\n",
       " 'MAX_IP_PKT_LEN',\n",
       " 'TOTAL_PKTS_EXP',\n",
       " 'TOTAL_BYTES_EXP')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create an analyzer for the model\n",
    "analyzer = innvestigate.create_analyzer(\"lrp.z\", model)\n",
    "\n",
    "# Perform LRP analysis on the input data\n",
    "analysis = analyzer.analyze(X_test)\n",
    "\n",
    "#uncomment for single sample\n",
    "# analysis = analyzer.analyze(single_sample_df)\n",
    "\n",
    "# Perform LRP analysis on a certain number of samples\n",
    "# analysis = analyzer.analyze(X_test.sample(2500))\n",
    "\n",
    "\n",
    "# Print or use the analysis results as needed\n",
    "print(analysis)\n",
    "print(len(X_test))\n",
    "print(len(X_test.columns))\n",
    "names = X_test.columns\n",
    "print(analysis.shape)\n",
    "print(type(analysis))\n",
    "scores = pd.DataFrame(analysis)\n",
    "print(analysis)\n",
    "scores_abs = scores.abs()\n",
    "\n",
    "# Calculate the sum of each column\n",
    "sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "names = list(names)\n",
    "\n",
    "sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "# Zip the two lists together\n",
    "combined = list(zip(names, sum_of_columns))\n",
    "\n",
    "# Sort the combined list in descending order based on the values from sum_of_columns\n",
    "sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "\n",
    "print(sorted_names)\n",
    "print(sorted_sum_of_columns)\n",
    "\n",
    "sorted_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Generating Explainer\n",
      "---------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:From /home/oarreche@ads.iu.edu/anaconda3/envs/tf23/lib/python3.8/site-packages/shap/explainers/tf_utils.py:28: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      col_name  feature_importance_vals\n",
      "4   FLOW_DURATION_MILLISECONDS             7.109965e-02\n",
      "10              TCP_WIN_MIN_IN             6.256452e-02\n",
      "9              TCP_WIN_MAX_OUT             4.563568e-02\n",
      "8               TCP_WIN_MAX_IN             2.987764e-02\n",
      "14           TCP_WIN_SCALE_OUT             2.458372e-02\n",
      "13            TCP_WIN_SCALE_IN             2.019242e-02\n",
      "11             TCP_WIN_MIN_OUT             1.477787e-02\n",
      "1                  L4_SRC_PORT             1.316693e-02\n",
      "7                    TCP_FLAGS             9.503863e-03\n",
      "0                      FLOW_ID             2.083951e-03\n",
      "17             TOTAL_FLOWS_EXP             1.337469e-03\n",
      "12              TCP_WIN_MSS_IN             4.277693e-04\n",
      "25                    OUT_PKTS             5.095696e-06\n",
      "5                LAST_SWITCHED             3.419828e-06\n",
      "3               FIRST_SWITCHED             3.293029e-06\n",
      "23                     IN_PKTS             2.812239e-06\n",
      "22                    IN_BYTES             1.931497e-07\n",
      "24                   OUT_BYTES             1.694743e-07\n",
      "6                     PROTOCOL             0.000000e+00\n",
      "2                  L4_DST_PORT             0.000000e+00\n",
      "15                     SRC_TOS             0.000000e+00\n",
      "16                     DST_TOS             0.000000e+00\n",
      "18              MIN_IP_PKT_LEN             0.000000e+00\n",
      "19              MAX_IP_PKT_LEN             0.000000e+00\n",
      "20              TOTAL_PKTS_EXP             0.000000e+00\n",
      "21             TOTAL_BYTES_EXP             0.000000e+00\n",
      "---------------------------------------------------------------------------------\n",
      "'FLOW_DURATION_MILLISECONDS',\n",
      "'TCP_WIN_MIN_IN',\n",
      "'TCP_WIN_MAX_OUT',\n",
      "'TCP_WIN_MAX_IN',\n",
      "'TCP_WIN_SCALE_OUT',\n",
      "'TCP_WIN_SCALE_IN',\n",
      "'TCP_WIN_MIN_OUT',\n",
      "'L4_SRC_PORT',\n",
      "'TCP_FLAGS',\n",
      "'FLOW_ID',\n",
      "'TOTAL_FLOWS_EXP',\n",
      "'TCP_WIN_MSS_IN',\n",
      "'OUT_PKTS',\n",
      "'LAST_SWITCHED',\n",
      "'FIRST_SWITCHED',\n",
      "'IN_PKTS',\n",
      "'IN_BYTES',\n",
      "'OUT_BYTES',\n",
      "'PROTOCOL',\n",
      "'L4_DST_PORT',\n",
      "'SRC_TOS',\n",
      "'DST_TOS',\n",
      "'MIN_IP_PKT_LEN',\n",
      "'MAX_IP_PKT_LEN',\n",
      "'TOTAL_PKTS_EXP',\n",
      "'TOTAL_BYTES_EXP',\n",
      "---------------------------------------------------------------------------------\n",
      "CPU times: user 8min 4s, sys: 24.3 s, total: 8min 28s\n",
      "Wall time: 59.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_file_name = 'SHAPCIC.txt'\n",
    "with open(output_file_name, \"w\") as f:print('',file = f)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Generating Explainer')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "samples = 2500\n",
    "\n",
    "#uncomment for single sample\n",
    "# samples = 1\n",
    "\n",
    "Label = label\n",
    "\n",
    "test = X_test\n",
    "train = X_train\n",
    "\n",
    "start_index = 0\n",
    "end_index = samples\n",
    "\n",
    "explainer = shap.DeepExplainer(model,train[start_index:end_index].values.astype('float'))\n",
    "shap_values = explainer.shap_values(test[start_index:end_index].values.astype('float'))\n",
    "# shap_values = explainer.shap_values(test[start_index:len(test)].values.astype('float'))\n",
    "\n",
    "vals= np.abs(shap_values).mean(1)\n",
    "feature_importance = pd.DataFrame(list(zip(train.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "feature_importance.head()\n",
    "print(feature_importance.to_string())\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "# feature_importance_vals = 'feature_importance_vals'  # Replace with the name of the column you want to extract\n",
    "feature_val = feature_importance['feature_importance_vals'].tolist()\n",
    "\n",
    "# col_name = 'col_name'  # Replace with the name of the column you want to extract\n",
    "feature_name = feature_importance['col_name'].tolist()\n",
    "\n",
    "# Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "zipped_lists = list(zip(feature_name, feature_val))\n",
    "zipped_lists.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Convert the sorted result back into separate lists\n",
    "sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "for k in sorted_list1:\n",
    "  with open(output_file_name, \"a\") as f:print(\"df.pop('\",k,\"')\", sep='',file = f)\n",
    "with open(output_file_name, \"a\") as f:print(\"Trial_ =[\", file = f)\n",
    "for k in sorted_list1:\n",
    "  with open(output_file_name, \"a\") as f:print(\"'\",k,\"',\", sep='', file = f)\n",
    "  print(\"'\",k,\"',\", sep='')\n",
    "with open(output_file_name, \"a\") as f:print(\"]\", file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('tf23': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "41c4aa321b0cd106a9117bcfe6d256e690681c38ac795834eda323315e35bf01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
