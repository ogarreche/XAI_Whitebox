{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 09:49:26.050093: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Initializing DNN program\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Importing Libraries\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oarreche@ads.iu.edu/anaconda3/envs/tf23/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining Metric Equations\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining features of interest\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Loading Databases\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Normalizing database\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Separating features and labels\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'BENIGN': 2273097, 'Dos/Ddos': 380699, 'PortScan': 158930, 'Brute Force': 13835, 'Web Attack': 2180, 'Bot': 1966, 'Infiltration': 36})\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating datasets\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'BENIGN': 2273097, 'Dos/Ddos': 380699, 'PortScan': 158930, 'Brute Force': 13835, 'Web Attack': 2180, 'Bot': 1966, 'Infiltration': 36})\n",
      "---------------------------------------------------------------------------------\n",
      "Separating Training and Testing db\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining the DNN model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "---------------------------------------------------------------------------------\n",
      "Training the model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Train on 1981520 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 09:49:38.641179: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-28 09:49:38.648058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:67:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.54GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-05-28 09:49:38.648233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:68:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.53GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-05-28 09:49:38.648253: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-28 09:49:39.084923: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-28 09:49:39.085112: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-05-28 09:49:39.170864: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-28 09:49:39.211646: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-28 09:49:39.225694: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-05-28 09:49:39.291584: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-05-28 09:49:39.291853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-05-28 09:49:39.291876: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-05-28 09:49:39.293322: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-28 09:49:39.294968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-28 09:49:39.294995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2024-05-28 09:49:39.315532: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3699850000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 6.5306 - accuracy: 0.0048\n",
      "Epoch 2/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.9612 - accuracy: 0.5472\n",
      "Epoch 3/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.6575 - accuracy: 0.7287\n",
      "Epoch 4/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.5584 - accuracy: 0.7653\n",
      "Epoch 5/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.4710 - accuracy: 0.7798\n",
      "Epoch 6/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.4231 - accuracy: 0.7889\n",
      "Epoch 7/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.3836 - accuracy: 0.7958\n",
      "Epoch 8/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.3495 - accuracy: 0.8000\n",
      "Epoch 9/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.3189 - accuracy: 0.8023\n",
      "Epoch 10/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2957 - accuracy: 0.8030\n",
      "Epoch 11/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2816 - accuracy: 0.8031\n",
      "Epoch 12/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2775 - accuracy: 0.8031\n",
      "Epoch 13/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2767 - accuracy: 0.8031\n",
      "Epoch 14/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2725 - accuracy: 0.8031\n",
      "Epoch 15/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2660 - accuracy: 0.8031\n",
      "Epoch 16/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2597 - accuracy: 0.8031\n",
      "Epoch 17/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2554 - accuracy: 0.8031\n",
      "Epoch 18/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2529 - accuracy: 0.8031\n",
      "Epoch 19/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2518 - accuracy: 0.8031\n",
      "Epoch 20/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2512 - accuracy: 0.8031\n",
      "Epoch 21/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2510 - accuracy: 0.8031\n",
      "Epoch 22/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2507 - accuracy: 0.8031\n",
      "Epoch 23/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2504 - accuracy: 0.8031\n",
      "Epoch 24/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2499 - accuracy: 0.8031\n",
      "Epoch 25/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2492 - accuracy: 0.8031\n",
      "Epoch 26/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2482 - accuracy: 0.8031\n",
      "Epoch 27/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2470 - accuracy: 0.8031\n",
      "Epoch 28/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2456 - accuracy: 0.8031\n",
      "Epoch 29/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2440 - accuracy: 0.8031\n",
      "Epoch 30/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2423 - accuracy: 0.8031\n",
      "---------------------------------------------------------------------------------\n",
      "ELAPSE TIME TRAINING MODEL:  0.15275254646937053 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Model Prediction\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSE TIME MODEL PREDICTION:  0.13415991862614948 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "80.27714746303386\n",
      "Counter({0: 681732, 2: 114534, 1: 47609, 3: 4083, 5: 672, 4: 578, 6: 15})\n",
      "Counter({0: 849223})\n",
      "80.27714746303386\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "### DNN Regression\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Initializing DNN program')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "#---------------------------------------------------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Importing Libraries')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import LSTM\n",
    "#from keras.layers import Dropout\n",
    "#from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.preprocessing import sequence\n",
    "#from keras.utils import pad_sequences\n",
    "#from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "import shap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "np.random.seed(0)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining metric equations\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Metric Equations')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "def ACC(TP,TN,FP,FN):\n",
    "    Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return Acc\n",
    "def ACC_2 (TP, FN):\n",
    "    ac = (TP/(TP+FN))\n",
    "    return ac\n",
    "def PRECISION(TP,FP):\n",
    "    Precision = TP/(TP+FP)\n",
    "    return Precision\n",
    "def RECALL(TP,FN):\n",
    "    Recall = TP/(TP+FN)\n",
    "    return Recall\n",
    "def F1(Recall, Precision):\n",
    "    F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "    return F1\n",
    "def BACC(TP,TN,FP,FN):\n",
    "    BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "    return BACC\n",
    "def MCC(TP,TN,FP,FN):\n",
    "    MCC = (TN*TP-FN*FP)/(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**.5)\n",
    "    return MCC\n",
    "def AUC_ROC(y_test_bin,y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    auc_avg = 0\n",
    "    counting = 0\n",
    "    for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "     # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "      #print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "      auc_avg += auc(fpr[i], tpr[i])\n",
    "      counting = i+1\n",
    "    return auc_avg/counting\n",
    "#---------------------------------------------------------------------\n",
    "# Defining features of interest\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining features of interest')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "req_cols = [\n",
    "    \n",
    "    ' Packet Length Std', ' Total Length of Bwd Packets', ' Subflow Bwd Bytes',\n",
    "    ' Destination Port', ' Packet Length Variance', ' Bwd Packet Length Mean',' Avg Bwd Segment Size',\n",
    "    'Bwd Packet Length Max', ' Init_Win_bytes_backward','Total Length of Fwd Packets',\n",
    "    ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' Average Packet Size', ' Packet Length Mean',\n",
    "    ' Max Packet Length',\n",
    "    ' Down/Up Ratio', ' Fwd URG Flags', ' Flow IAT Std', 'Subflow Fwd Packets', ' Flow Packets/s', ' URG Flag Count', 'FIN Flag Count', ' Bwd Packets/s', 'Bwd Avg Bulk Rate'\n",
    "    , ' act_data_pkt_fwd', ' Fwd Packet Length Std', ' Bwd Avg Bytes/Bulk', ' Active Max', ' Flow IAT Max', ' min_seg_size_forward', ' Bwd Packet Length Std', ' Fwd IAT Std', ' Fwd Avg Bulk Rate', ' Fwd Packet Length Mean', ' Fwd Packet Length Max', ' Idle Std', ' CWE Flag Count', 'Fwd IAT Total'\n",
    "    \n",
    "    , ' ACK Flag Count', ' Bwd URG Flags', ' Flow IAT Min', ' Flow IAT Mean', ' Total Backward Packets', ' Fwd Avg Packets/Bulk', 'Fwd Avg Bytes/Bulk', ' SYN Flag Count', ' Min Packet Length', ' Fwd Packet Length Min', 'Idle Mean', 'Fwd PSH Flags', ' Fwd IAT Min'\n",
    "     \n",
    "    ,  ' Fwd Header Length', ' RST Flag Count', ' Idle Max', ' PSH Flag Count', ' Bwd Header Length', ' ECE Flag Count', ' Subflow Bwd Packets', 'Active Mean', 'Flow Bytes/s', ' Bwd IAT Mean', ' Avg Fwd Segment Size', ' Bwd Packet Length Min', ' Active Std', ' Bwd IAT Min', ' Flow Duration', 'Fwd Packets/s', ' Fwd IAT Max', 'Bwd IAT Total', ' Idle Min', ' Bwd PSH Flags', ' Bwd Avg Packets/Bulk', ' Total Fwd Packets', ' Active Min', ' Bwd IAT Std', ' Fwd IAT Mean', ' Bwd IAT Max'\n",
    "            \n",
    "            , ' Label']\n",
    "\n",
    "# Information gain top 10 features\n",
    "top10 = [' Average Packet Size',\n",
    "          ' Packet Length Std', \n",
    "          ' Packet Length Variance', \n",
    "          ' Packet Length Mean',\n",
    "            ' Destination Port', \n",
    "            ' Subflow Bwd Bytes', \n",
    "            ' Total Length of Bwd Packets', \n",
    "            ' Avg Bwd Segment Size', \n",
    "            ' Bwd Packet Length Mean',  \n",
    "            'Bwd Packet Length Max', \n",
    "            ' Label']\n",
    "\n",
    "\n",
    "\n",
    "req_cols = top10\n",
    "\n",
    "# req_cols = [' Destination Port',' Flow Duration',' Total Fwd Packets',' Total Backward Packets','Total Length of Fwd Packets',' Total Length of Bwd Packets',' Label']\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "\n",
    "path_str = '/home/oarreche@ads.iu.edu/HITL/cicids/cicids_db/'\n",
    "fraction = 1\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Loading Databases')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "df0 = pd.read_csv (path_str + 'Wednesday-workingHours.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "df1 = pd.read_csv (path_str + 'Tuesday-WorkingHours.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df2 = pd.read_csv (path_str +'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df3 = pd.read_csv (path_str +'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df4 = pd.read_csv (path_str +'Monday-WorkingHours.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df5 = pd.read_csv (path_str +'Friday-WorkingHours-Morning.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df6 = pd.read_csv (path_str +'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df7 = pd.read_csv (path_str +'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "frames = [df0, df1, df2, df3, df4, df5, df6, df7]\n",
    "\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Normalizing database')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "df_max_scaled = df.copy()\n",
    "\n",
    "y = df_max_scaled[' Label'].replace({'DDoS' :'Dos/Ddos' ,'DoS GoldenEye': 'Dos/Ddos', 'DoS Hulk': 'Dos/Ddos', 'DoS Slowhttptest': 'Dos/Ddos', 'DoS slowloris': 'Dos/Ddos', 'Heartbleed': 'Dos/Ddos','FTP-Patator': 'Brute Force', 'SSH-Patator': 'Brute Force','Web Attack - Brute Force': 'Web Attack', 'Web Attack - Sql Injection': 'Web Attack', 'Web Attack - XSS': 'Web Attack'})\n",
    "\n",
    "df_max_scaled.pop(' Label')\n",
    "\n",
    "\n",
    "df_max_scaled\n",
    "for col in df_max_scaled.columns:\n",
    "    t = abs(df_max_scaled[col].max())\n",
    "    df_max_scaled[col] = df_max_scaled[col]/t\n",
    "df_max_scaled\n",
    "df = df_max_scaled.assign( Label = y)\n",
    "#df\n",
    "df = df.fillna(0)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating features and labels')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# result_list = [counter['None'],counter['Denial of Service'], counter['Port Scanning']]\n",
    "# print('number of Labels  ',result_list)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating datasets')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "test2 = X.assign(ALERT = y)\n",
    "\n",
    "Dos_samples = test2[test2['ALERT'] == 'Dos/Ddos']\n",
    "Normal_samples = test2[test2['ALERT'] == 'BENIGN']\n",
    "PS_samples = test2[test2['ALERT'] == 'PortScan']\n",
    "\n",
    "Infiltration_samples = test2[test2['ALERT'] == 'PortScan']\n",
    "Bot_samples = test2[test2['ALERT'] == 'Bot']\n",
    "Web_samples = test2[test2['ALERT'] == 'Web Attack']\n",
    "Brute_samples = test2[test2['ALERT'] == 'Brute Force']\n",
    "\n",
    "\n",
    "PS_y = PS_samples.pop('ALERT')\n",
    "Dos_y = Dos_samples.pop('ALERT')\n",
    "Normal_y = Normal_samples.pop('ALERT')\n",
    "\n",
    "Infiltration_y = Infiltration_samples.pop('ALERT')\n",
    "Bot_y = Bot_samples.pop('ALERT')\n",
    "Web_y = Web_samples.pop('ALERT')\n",
    "Brute_y = Brute_samples.pop('ALERT')\n",
    "\n",
    "\n",
    "test2.pop('ALERT')\n",
    "\n",
    "# # Create an instance of RandomUnderSampler\n",
    "# rus = RandomUnderSampler()\n",
    "\n",
    "# # Balance the dataset using RandomUnderSampler\n",
    "# X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# # Create an instance of SMOTE\n",
    "# smote = SMOTE()\n",
    "\n",
    "# # Balance the dataset using SMOTE\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# X = X_resampled\n",
    "# y = y_resampled\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "y, label = pd.factorize(y)\n",
    "# y_test, label = pd.factorize(test['Label'])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate Training and Testing db\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating Training and Testing db')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.7,random_state=42)\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Now you can use Keras modules directly from tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Flatten\n",
    "import innvestigate\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining the DNN model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# dropout_rate = 0.01\n",
    "nodes = 7\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(len(X_train.columns,))))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(7))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training the model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Modify model.fit to include the EarlyStopping callback\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('ELAPSE TIME TRAINING MODEL: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Model Prediction')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time()\n",
    "print('ELAPSE TIME MODEL PREDICTION: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "#print(y_pred)\n",
    "ynew = np.argmax(y_pred,axis = 1)\n",
    "#print(ynew)\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "#print(score)\n",
    "pred_label = label[ynew]\n",
    "#print(score)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_test)\n",
    "print(label_counts)\n",
    "\n",
    "label_counts = Counter(ynew)\n",
    "print(label_counts)\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train\n",
    "test = X_test\n",
    "labels_train = y_train\n",
    "labels_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = list(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to test sample with the waterfall plot\n",
    "def waterfall_explanator(sample):\n",
    "\n",
    "    index = np.argmax(model.predict(sample)) # Prediction of the sample\n",
    "    prediction = index\n",
    "\n",
    "    analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model)\n",
    "    analysis = analyzer.analyze(sample)\n",
    "\n",
    "    names = sample.columns\n",
    "\n",
    "    scores = pd.DataFrame(analysis)\n",
    "    scores_abs = scores.abs()\n",
    "    sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "    names = list(names)\n",
    "\n",
    "    sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "    sum_of_columns\n",
    "    combined = list(zip(names, sum_of_columns))\n",
    "    # Sort the combined list in descending order based on the values from sum_of_columns\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "    sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "    shap_val = sorted_sum_of_columns\n",
    "    feature_name = sorted_names\n",
    "    # sorted_names\n",
    "    feature_val = []\n",
    "    for j in sorted_names:\n",
    "            feature_val.append(float(sample[j]))\n",
    "    return (prediction, shap_val,feature_val,feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_all(single_class_samples,number_samples, number_of_features_pertubation):\n",
    "    Bucket = {\n",
    "    '0.0': 0,\n",
    "    '0.1':0,\n",
    "    '0.2':0,\n",
    "    '0.3':0,\n",
    "    '0.4':0,\n",
    "    '0.5':0,\n",
    "    '0.6':0,\n",
    "    '0.7':0,\n",
    "    '0.8':0,\n",
    "    '0.9':0,\n",
    "    '1.0':0,\n",
    "\n",
    "           }\n",
    "    # Counter_chart = 0\n",
    "    Counter_all_samples = 0\n",
    "    counter_samples_changed_class = 0\n",
    "    print('------------------------------------------------')\n",
    "    print('Initiating Completeness Experiment')\n",
    "    print('------------------------------------------------')\n",
    "    for i in range(0,number_samples):\n",
    "        #select sample\n",
    "        try:\n",
    "            sample = single_class_samples[i:i+1]\n",
    "        except:\n",
    "            break # break if there more samples requested than samples in the dataset\n",
    "        # Explanate the original sample\n",
    "        u = waterfall_explanator(sample)\n",
    "        #select top 5 features from the original sample\n",
    "        top_k_features = []\n",
    "        top_k_features.append(u[3][0]) #append first feature\n",
    "        break_condition = False\n",
    "        for k in range(1,number_of_features_pertubation):\n",
    "            for j in range(11):  # 11 steps to include 1.0 (0 to 10)\n",
    "                if break_condition == True: break\n",
    "                perturbation = j / 10.0  # Divide by 10 to get steps of 0.1\n",
    "                temp_var = sample[top_k_features[k-1]]\n",
    "                result = np.where((temp_var - perturbation) < 0, True, False)\n",
    "                if result < 0: \n",
    "                    sample[top_k_features[k-1]] = 1 - perturbation\n",
    "                else: sample[top_k_features[k-1]] = temp_var - perturbation\n",
    "                # sample[top_k_features[k-1]] = perturbation\n",
    "                v = waterfall_explanator(sample)\n",
    "                if v[0] != u[0]: \n",
    "                    # print(str(perturbation))\n",
    "                    Bucket[str(perturbation)] += 1              \n",
    "                    break_condition = True\n",
    "                    counter_samples_changed_class += 1     \n",
    "                    # Bucket[str(perturbation)] = counter_samples_changed_class              \n",
    "                    break\n",
    "                else: sample[top_k_features[k-1]] = abs(temp_var - 1) # set the sample feature value as the symetric opposite\n",
    "            # print(u)\n",
    "            top_k_features.append(u[3][k]) #append second, third feature .. and so on\n",
    "            if break_condition == True: break\n",
    "        Counter_all_samples += 1\n",
    "        progress  = 100*Counter_all_samples/number_samples\n",
    "        if progress%10 == 0: print('Progress', progress ,'%')\n",
    "    # print('Number of Normal samples that changed classification: ',counter_samples_changed_class)\n",
    "    # print('Number of all samples analyzed: ',Counter_all_samples)\n",
    "    # for key in Bucket:\n",
    "    #     Bucket[key]=number_samples - Bucket[key]\n",
    "    # Bucket['0.0'] = number_samples\n",
    "    # for key in Bucket:\n",
    "    #     Bucket[key]=Bucket[key]\n",
    "    dict = Bucket\n",
    "    temp = 0\n",
    "    for k in dict:\n",
    "        dict[k] = dict[k] + temp\n",
    "        temp = dict[k]\n",
    "    total = number_samples\n",
    "    y_axis = []\n",
    "    for k in dict:\n",
    "        dict[k] = abs(dict[k] - total)\n",
    "        y_axis.append(dict[k]/total)    \n",
    "    return(counter_samples_changed_class,Counter_all_samples,y_axis)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_samples = 100\n",
    "K_feat = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = K_samples\n",
    "# num_samples = 3\n",
    "\n",
    "num_feat_pertubation = K_feat\n",
    "# num_feat_pertubation = 3\n",
    "output_file_name = 'DNN_IG_CIC_Completeness.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(83, 100, [1.0, 0.88, 0.75, 0.68, 0.51, 0.48, 0.45, 0.42, 0.42, 0.42, 0.17])\n",
      "Number of DoS samples that changed classification:  83\n",
      "Number of all samples analyzed:  100\n",
      "83.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "p = completeness_all(Dos_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of DoS samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_dos = p[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_name, \"w\") as f:print('',file = f)\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_dos = [',y_axis_dos ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Normal_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Normal samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_normal = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_normal = [',y_axis_normal ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = completeness_all(PS_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of PS samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_ps = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_ps = [',y_axis_ps ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(3, 100, [1.0, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97])\n",
      "Number of Infiltration samples that changed classification:  3\n",
      "Number of all samples analyzed:  100\n",
      "3.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Infiltration_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Infiltration samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_infiltration = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_infiltration = [',y_axis_infiltration ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(0, 100, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "Number of Bot samples that changed classification:  0\n",
      "Number of all samples analyzed:  100\n",
      "0.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = completeness_all(Bot_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Bot samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_bot = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_bot = [',y_axis_bot ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(2, 100, [1.0, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98])\n",
      "Number of Web samples that changed classification:  2\n",
      "Number of all samples analyzed:  100\n",
      "2.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Web_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Web samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_web = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_web = [',y_axis_web ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(31, 100, [1.0, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69])\n",
      "Number of Brute samples that changed classification:  31\n",
      "Number of all samples analyzed:  100\n",
      "31.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Brute_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Brute samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_brute = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_brute = [',y_axis_brute ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_axis_dos = [ [1.0, 0.88, 0.75, 0.68, 0.51, 0.48, 0.45, 0.42, 0.42, 0.42, 0.17] ]\n",
    "y_axis_normal = [ [1.0, 0.69, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.67, 0.67, 0.6] ]\n",
    "y_axis_ps = [ [1.0, 0.97, 0.97, 0.97, 0.97, 0.97, 0.96, 0.96, 0.96, 0.96, 0.69] ]\n",
    "y_axis_infiltration = [ [1.0, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97] ]\n",
    "y_axis_bot = [ [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] ]\n",
    "y_axis_web = [ [1.0, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98] ]\n",
    "y_axis_brute = [ [1.0, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69] ]\n",
    "\n",
    "\n",
    "\n",
    "# Plot the first line\n",
    "plt.plot(x_axis, y_axis_dos, label='DoS', color='blue', linestyle='--', marker='o')\n",
    "\n",
    "# # Plot the second line\n",
    "# plt.plot(x_axis, y_axis_normal, label='Normal', color='red', linestyle='--', marker='x')\n",
    "\n",
    "# # Plot the third line\n",
    "# plt.plot(x_axis, y_axis_ps, label='Port Scan', color='green', linestyle='--', marker='s')\n",
    "\n",
    "# # Plot the fourth line\n",
    "# plt.plot(x_axis, y_axis_infiltration, label='Infiltration', color='purple', linestyle='--', marker='p')\n",
    "\n",
    "# # Plot the fifth line\n",
    "# plt.plot(x_axis, y_axis_bot, label='Bot', color='orange', linestyle='--', marker='h')\n",
    "\n",
    "# # Plot the sixth line\n",
    "# plt.plot(x_axis, y_axis_web, label='Web Attack', color='magenta', linestyle='--', marker='+')\n",
    "\n",
    "# # Plot the seventh line\n",
    "# plt.plot(x_axis, y_axis_brute, label='Brute Force', color='cyan', linestyle='--', marker='_')\n",
    "\n",
    "# Enable grid lines (both major and minor grids)\n",
    "plt.grid()\n",
    "\n",
    "# Customize grid lines (optional)\n",
    "# plt.grid()\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Perturbations')\n",
    "plt.ylabel('Samples remaining')\n",
    "plt.legend()\n",
    "\n",
    "# Set the title of the plot\n",
    "# plt.title('Accuracy x Features - SHAP SML')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.savefig('GRAPH_PERT_SHAP_CIC.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
