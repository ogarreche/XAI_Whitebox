{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We First load the dataset and set the parameters to the model. For this experiment we are using DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 22:17:50.253518: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Initializing DNN program\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Importing Libraries\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oarreche@ads.iu.edu/anaconda3/envs/tf23/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining Metric Equations\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining features of interest\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Loading Database\n",
      "--------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating features and labels\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'Denial of Service': 642515, 'Port Scanning': 417040, 'None': 195521})\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating datasets\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'Denial of Service': 642515, 'Port Scanning': 417040, 'None': 195521})\n",
      "---------------------------------------------------------------------------------\n",
      "Separating Training and Testing db\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining the DNN model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "---------------------------------------------------------------------------------\n",
      "Training the model\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 22:18:38.546495: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-31 22:18:38.553050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:67:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.54GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-05-31 22:18:38.553202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:68:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.53GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-05-31 22:18:38.553226: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-31 22:18:38.568952: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-31 22:18:38.569049: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-05-31 22:18:38.571292: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-31 22:18:38.571595: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-31 22:18:38.572098: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-05-31 22:18:38.572706: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-05-31 22:18:38.572801: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-05-31 22:18:38.572810: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 878553 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 22:18:38.887592: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-31 22:18:38.888786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-31 22:18:38.888802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2024-05-31 22:18:38.905967: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3699850000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878553/878553 [==============================] - 1s 1us/sample - loss: 6.6583 - accuracy: 0.2974 - lr: 0.0010\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 2/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 5.3234 - accuracy: 0.4050 - lr: 0.0010\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 3/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 4.5855 - accuracy: 0.4914 - lr: 0.0010\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 4/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 4.0931 - accuracy: 0.5468 - lr: 0.0010\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 5/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 3.6346 - accuracy: 0.5879 - lr: 0.0010\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 6/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 3.2085 - accuracy: 0.6268 - lr: 0.0010\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 7/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.8969 - accuracy: 0.6587 - lr: 0.0010\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 8/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.6992 - accuracy: 0.6828 - lr: 0.0010\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 9/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.5819 - accuracy: 0.6973 - lr: 0.0010\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 10/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.5281 - accuracy: 0.7041 - lr: 0.0010\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 11/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.5041 - accuracy: 0.7064 - lr: 0.0010\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4893 - accuracy: 0.7084 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4898 - accuracy: 0.7083 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 14/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4866 - accuracy: 0.7085 - lr: 1.0000e-06\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 15/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4909 - accuracy: 0.7085 - lr: 1.0000e-07\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 16/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4879 - accuracy: 0.7085 - lr: 1.0000e-08\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 17/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4846 - accuracy: 0.7093 - lr: 1.0000e-09\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 18/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4866 - accuracy: 0.7084 - lr: 1.0000e-10\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 19/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4919 - accuracy: 0.7086 - lr: 1.0000e-11\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 20/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4891 - accuracy: 0.7084 - lr: 1.0000e-12\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.0000001044244145e-13.\n",
      "Epoch 21/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4891 - accuracy: 0.7088 - lr: 1.0000e-13\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.0000001179769417e-14.\n",
      "Epoch 22/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4872 - accuracy: 0.7090 - lr: 1.0000e-14\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.0000001518582595e-15.\n",
      "Epoch 23/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4883 - accuracy: 0.7087 - lr: 1.0000e-15\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.0000001095066122e-16.\n",
      "Epoch 24/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4862 - accuracy: 0.7085 - lr: 1.0000e-16\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.0000000830368326e-17.\n",
      "Epoch 25/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4871 - accuracy: 0.7089 - lr: 1.0000e-17\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.0000000664932204e-18.\n",
      "Epoch 26/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4891 - accuracy: 0.7087 - lr: 1.0000e-18\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.000000045813705e-19.\n",
      "Epoch 27/1000\n",
      "878553/878553 [==============================] - 1s 1us/sample - loss: 2.4875 - accuracy: 0.7084 - lr: 1.0000e-19\n",
      "---------------------------------------------------------------------------------\n",
      "ELAPSE TIME TRAINING MODEL:  0.4633863687515259 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Model Prediction\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSE TIME MODEL PREDICTION:  0.09670320749282837 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "84.29232742754095\n",
      "Counter({0: 192898, 2: 125009, 1: 58616})\n",
      "Counter({0: 195459, 2: 181061, 1: 3})\n",
      "84.29232742754095\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "### DNN Regression\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Initializing DNN program')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "#---------------------------------------------------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Importing Libraries')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import LSTM\n",
    "#from keras.layers import Dropout\n",
    "#from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.preprocessing import sequence\n",
    "#from keras.utils import pad_sequences\n",
    "#from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "import shap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "np.random.seed(0)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining metric equations\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Metric Equations')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "def ACC(TP,TN,FP,FN):\n",
    "    Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return Acc\n",
    "def ACC_2 (TP, FN):\n",
    "    ac = (TP/(TP+FN))\n",
    "    return ac\n",
    "def PRECISION(TP,FP):\n",
    "    Precision = TP/(TP+FP)\n",
    "    return Precision\n",
    "def RECALL(TP,FN):\n",
    "    Recall = TP/(TP+FN)\n",
    "    return Recall\n",
    "def F1(Recall, Precision):\n",
    "    F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "    return F1\n",
    "def BACC(TP,TN,FP,FN):\n",
    "    BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "    return BACC\n",
    "def MCC(TP,TN,FP,FN):\n",
    "    MCC = (TN*TP-FN*FP)/(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**.5)\n",
    "    return MCC\n",
    "def AUC_ROC(y_test_bin,y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    auc_avg = 0\n",
    "    counting = 0\n",
    "    for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "     # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "      #print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "      auc_avg += auc(fpr[i], tpr[i])\n",
    "      counting = i+1\n",
    "    return auc_avg/counting\n",
    "#---------------------------------------------------------------------\n",
    "# Defining features of interest\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining features of interest')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "req_cols = ['FLOW_DURATION_MILLISECONDS','FIRST_SWITCHED',\n",
    "            'TOTAL_FLOWS_EXP','TCP_WIN_MSS_IN','LAST_SWITCHED',\n",
    "            'TCP_WIN_MAX_IN','TCP_WIN_MIN_IN','TCP_WIN_MIN_OUT',\n",
    "           'PROTOCOL','TCP_WIN_MAX_OUT','TCP_FLAGS',\n",
    "            'TCP_WIN_SCALE_OUT','TCP_WIN_SCALE_IN','SRC_TOS',\n",
    "            'DST_TOS','FLOW_ID','L4_SRC_PORT','L4_DST_PORT',\n",
    "           'MIN_IP_PKT_LEN','MAX_IP_PKT_LEN','TOTAL_PKTS_EXP',\n",
    "           'TOTAL_BYTES_EXP','IN_BYTES','IN_PKTS','OUT_BYTES','OUT_PKTS',\n",
    "            'ALERT']\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "address = '/home/oarreche@ads.iu.edu/HITL/sensor/sensor_db'\n",
    "print('Loading Database')\n",
    "print('--------------------------------------------------')\n",
    "\n",
    "fraction = 0.1\n",
    "fraction2 = 0.01\n",
    "\n",
    "#Denial of Service\n",
    "df0 = pd.read_csv (address + '/dos-03-15-2022-15-44-32.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df1 = pd.read_csv (address + '/dos-03-16-2022-13-45-18.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df2 = pd.read_csv (address + '/dos-03-17-2022-16-22-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df3 = pd.read_csv (address + '/dos-03-18-2022-19-27-05.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df4 = pd.read_csv (address + '/dos-03-19-2022-20-01-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df5 = pd.read_csv (address + '/dos-03-20-2022-14-27-54.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "#Malware\n",
    "#df6 = pd.read_csv ('sensor_db/malware-03-25-2022-17-57-07.csv', usecols=req_cols)\n",
    "\n",
    "#Normal\n",
    "df7 = pd.read_csv  (address + '/normal-03-15-2022-15-43-44.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df8 = pd.read_csv  (address + '/normal-03-16-2022-13-44-27.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df9 = pd.read_csv  (address + '/normal-03-17-2022-16-21-30.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df10 = pd.read_csv (address + '/normal-03-18-2022-19-17-31.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df11 = pd.read_csv (address + '/normal-03-18-2022-19-25-48.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df12 = pd.read_csv (address + '/normal-03-19-2022-20-01-16.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "df13 = pd.read_csv (address + '/normal-03-20-2022-14-27-30.csv', usecols=req_cols).sample(frac = fraction2)\n",
    "\n",
    "\n",
    "#PortScanning\n",
    "df14 = pd.read_csv  (address + '/portscanning-03-15-2022-15-44-06.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df15 = pd.read_csv  (address + '/portscanning-03-16-2022-13-44-50.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df16 = pd.read_csv  (address + '/portscanning-03-17-2022-16-22-53.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df17 = pd.read_csv  (address + '/portscanning-03-18-2022-19-27-05.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df18 = pd.read_csv  (address + '/portscanning-03-19-2022-20-01-45.csv', usecols=req_cols).sample(frac = fraction)\n",
    "df19 = pd.read_csv  (address + '/portscanning-03-20-2022-14-27-49.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "frames = [df0, df1, df2, df3, df4, df5, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19]\n",
    "\n",
    "# fraction = 0.1\n",
    "\n",
    "#concat data frames\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "# shuffle the DataFrame rows\n",
    "# df = df.sample(frac = fraction)\n",
    "\n",
    "y = df.pop('ALERT')\n",
    "X = df\n",
    "\n",
    "df_max_scaled = X\n",
    "for col in df_max_scaled.columns:\n",
    "    t = abs(df_max_scaled[col].max())\n",
    "    df_max_scaled[col] = df_max_scaled[col]/t\n",
    "df_max_scaled\n",
    "df = df_max_scaled.assign( Label = y)\n",
    "#df\n",
    "df = df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating features and labels')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating datasets')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "test2 = X.assign(ALERT = y)\n",
    "\n",
    "Dos_samples = test2[test2['ALERT'] == 'Denial of Service']\n",
    "Normal_samples = test2[test2['ALERT'] == 'None']\n",
    "PS_samples = test2[test2['ALERT'] == 'Port Scanning']\n",
    "Attack_samples = test2[test2['ALERT'] == 'Attack']\n",
    "\n",
    "PS_y = PS_samples.pop('ALERT')\n",
    "Dos_y = Dos_samples.pop('ALERT')\n",
    "Normal_y = Normal_samples.pop('ALERT')\n",
    "Attack_y = Attack_samples.pop('ALERT')\n",
    "\n",
    "test2.pop('ALERT')\n",
    "\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "y, label = pd.factorize(y)\n",
    "# y_test, label = pd.factorize(test['Label'])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating Training and Testing db')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.7,random_state=42)\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Now you can use Keras modules directly from tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Flatten\n",
    "import innvestigate\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining the DNN model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "nodes_first_layer = 128\n",
    "nodes_second_layer = 64\n",
    "nodes_third_layer = 32\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(len(X_train.columns,))))\n",
    "\n",
    "# First dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_first_layer, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))  # Dropout layer follows the first dense layer\n",
    "\n",
    "# Second dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_second_layer, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))  # Dropout layer follows the second dense layer\n",
    "\n",
    "# Third dense layer\n",
    "model.add(tf.keras.layers.Dense(nodes_third_layer, activation='relu'))\n",
    "\n",
    "# Output layer, assuming the task involves classification into 3 classes\n",
    "model.add(tf.keras.layers.Dense(3))  # No activation here, add 'softmax' when using the model for prediction\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training the model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Modify model.fit to include the EarlyStopping callback\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train), callbacks=[early_stopping,lr_scheduler])\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('ELAPSE TIME TRAINING MODEL: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Model Prediction')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time()\n",
    "print('ELAPSE TIME MODEL PREDICTION: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "#print(y_pred)\n",
    "ynew = np.argmax(y_pred,axis = 1)\n",
    "#print(ynew)\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "#print(score)\n",
    "pred_label = label[ynew]\n",
    "#print(score)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_test)\n",
    "print(label_counts)\n",
    "\n",
    "label_counts = Counter(ynew)\n",
    "print(label_counts)\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train\n",
    "test = X_test\n",
    "labels_train = y_train\n",
    "labels_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = shap.DeepExplainer(model,train.values.astype('float'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to test sample with the waterfall plot\n",
    "def waterfall_explanator(sample):\n",
    "\n",
    "    index = np.argmax(model.predict(sample)) # Prediction of the sample\n",
    "    prediction = index\n",
    "\n",
    "    analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model)\n",
    "    analysis = analyzer.analyze(sample)\n",
    "\n",
    "    names = sample.columns\n",
    "\n",
    "    scores = pd.DataFrame(analysis)\n",
    "    scores_abs = scores.abs()\n",
    "    sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "    names = list(names)\n",
    "\n",
    "    sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "    sum_of_columns\n",
    "    combined = list(zip(names, sum_of_columns))\n",
    "    # Sort the combined list in descending order based on the values from sum_of_columns\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "    sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "    shap_val = sorted_sum_of_columns\n",
    "    feature_name = sorted_names\n",
    "    # sorted_names\n",
    "    feature_val = []\n",
    "    for j in sorted_names:\n",
    "            feature_val.append(float(sample[j]))\n",
    "    return (prediction, shap_val,feature_val,feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_all(single_class_samples,number_samples, number_of_features_pertubation):\n",
    "    Bucket = {\n",
    "    '0.0': 0,\n",
    "    '0.1':0,\n",
    "    '0.2':0,\n",
    "    '0.3':0,\n",
    "    '0.4':0,\n",
    "    '0.5':0,\n",
    "    '0.6':0,\n",
    "    '0.7':0,\n",
    "    '0.8':0,\n",
    "    '0.9':0,\n",
    "    '1.0':0,\n",
    "\n",
    "           }\n",
    "    # Counter_chart = 0\n",
    "    Counter_all_samples = 0\n",
    "    counter_samples_changed_class = 0\n",
    "    print('------------------------------------------------')\n",
    "    print('Initiating Completeness Experiment')\n",
    "    print('------------------------------------------------')\n",
    "    for i in range(0,number_samples):\n",
    "        #select sample\n",
    "        try:\n",
    "            sample = single_class_samples[i:i+1]\n",
    "        except:\n",
    "            break # break if there more samples requested than samples in the dataset\n",
    "        # Explanate the original sample\n",
    "        u = waterfall_explanator(sample)\n",
    "        #select top 5 features from the original sample\n",
    "        top_k_features = []\n",
    "        top_k_features.append(u[3][0]) #append first feature\n",
    "        break_condition = False\n",
    "        for k in range(1,number_of_features_pertubation):\n",
    "            for j in range(11):  # 11 steps to include 1.0 (0 to 10)\n",
    "                if break_condition == True: break\n",
    "                perturbation = j / 10.0  # Divide by 10 to get steps of 0.1\n",
    "                temp_var = sample[top_k_features[k-1]]\n",
    "                result = np.where((temp_var - perturbation) < 0, True, False)\n",
    "                if result < 0: \n",
    "                    sample[top_k_features[k-1]] = 1 - perturbation\n",
    "                else: sample[top_k_features[k-1]] = temp_var - perturbation\n",
    "                # sample[top_k_features[k-1]] = perturbation\n",
    "                v = waterfall_explanator(sample)\n",
    "                if v[0] != u[0]: \n",
    "                    # print(str(perturbation))\n",
    "                    Bucket[str(perturbation)] += 1              \n",
    "                    break_condition = True\n",
    "                    counter_samples_changed_class += 1     \n",
    "                    # Bucket[str(perturbation)] = counter_samples_changed_class              \n",
    "                    break\n",
    "                else: sample[top_k_features[k-1]] = abs(temp_var - 1) # set the sample feature value as the symetric opposite\n",
    "            # print(u)\n",
    "            top_k_features.append(u[3][k]) #append second, third feature .. and so on\n",
    "            if break_condition == True: break\n",
    "        Counter_all_samples += 1\n",
    "        progress  = 100*Counter_all_samples/number_samples\n",
    "        if progress%10 == 0: print('Progress', progress ,'%')\n",
    "    # print('Number of Normal samples that changed classification: ',counter_samples_changed_class)\n",
    "    # print('Number of all samples analyzed: ',Counter_all_samples)\n",
    "    # for key in Bucket:\n",
    "    #     Bucket[key]=number_samples - Bucket[key]\n",
    "    # Bucket['0.0'] = number_samples\n",
    "    # for key in Bucket:\n",
    "    #     Bucket[key]=Bucket[key]\n",
    "    dict = Bucket\n",
    "    temp = 0\n",
    "    for k in dict:\n",
    "        dict[k] = dict[k] + temp\n",
    "        temp = dict[k]\n",
    "    total = number_samples\n",
    "    y_axis = []\n",
    "    for k in dict:\n",
    "        dict[k] = abs(dict[k] - total)\n",
    "        y_axis.append(dict[k]/total)    \n",
    "    return(counter_samples_changed_class,Counter_all_samples,y_axis)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_samples = 100\n",
    "K_feat =  2\n",
    "# Dos_samples.shape[1]\n",
    "# Dos_samples\n",
    "output_file_name = 'DNN_IG_SML_Completeness.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = K_samples\n",
    "num_feat_pertubation = K_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(output_file_name, \"w\") as f:print('',file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = completeness_all(Dos_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of DoS samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_dos = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_dos = [',y_axis_dos ,']',file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Normal_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Normal samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_normal = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_normal = [',y_axis_normal ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(95, 100, [1.0, 0.16, 0.16, 0.13, 0.13, 0.06, 0.06, 0.05, 0.05, 0.05, 0.05])\n",
      "Number of PS samples that changed classification:  95\n",
      "Number of all samples analyzed:  100\n",
      "95.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = completeness_all(PS_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of PS samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_ps = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_ps = [',y_axis_ps ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_axis_dos = [ [1.0, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16] ]\n",
    "y_axis_ps = [ [1.0, 0.16, 0.16, 0.13, 0.13, 0.06, 0.06, 0.05, 0.05, 0.05, 0.05] ]\n",
    "\n",
    "\n",
    "# Plot the first line\n",
    "plt.plot(x_axis, y_axis_dos, label='DoS', color='blue', linestyle='--', marker='o')\n",
    "\n",
    "# # Plot the second line\n",
    "# plt.plot(x_axis, y_axis_normal, label='Normal', color='red', linestyle='--', marker='x')\n",
    "\n",
    "# # Plot the third line\n",
    "# plt.plot(x_axis, y_axis_ps, label='Port Scan', color='green', linestyle='--', marker='s')\n",
    "\n",
    "# # Plot the fourth line\n",
    "# plt.plot(x_axis, y_axis_infiltration, label='Infiltration', color='purple', linestyle='--', marker='p')\n",
    "\n",
    "# # Plot the fifth line\n",
    "# plt.plot(x_axis, y_axis_bot, label='Bot', color='orange', linestyle='--', marker='h')\n",
    "\n",
    "# # Plot the sixth line\n",
    "# plt.plot(x_axis, y_axis_web, label='Web Attack', color='magenta', linestyle='--', marker='+')\n",
    "\n",
    "# # Plot the seventh line\n",
    "# plt.plot(x_axis, y_axis_brute, label='Brute Force', color='cyan', linestyle='--', marker='_')\n",
    "\n",
    "# Enable grid lines (both major and minor grids)\n",
    "plt.grid()\n",
    "\n",
    "# Customize grid lines (optional)\n",
    "# plt.grid()\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Perturbations')\n",
    "plt.ylabel('Samples remaining')\n",
    "plt.legend()\n",
    "\n",
    "# Set the title of the plot\n",
    "# plt.title('Accuracy x Features - SHAP SML')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.savefig('GRAPH_PERT_SHAP_CIC.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
