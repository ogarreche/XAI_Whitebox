{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 09:58:14.567027: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Initializing DNN program\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Importing Libraries\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oarreche@ads.iu.edu/anaconda3/envs/tf23/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining Metric Equations\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining features of interest\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Dimensions of the Training set: (125973, 43)\n",
      "Dimensions of the Test set: (22544, 43)\n",
      "Label distribution Training set:\n",
      "normal             67343\n",
      "neptune            41214\n",
      "satan               3633\n",
      "ipsweep             3599\n",
      "portsweep           2931\n",
      "smurf               2646\n",
      "nmap                1493\n",
      "back                 956\n",
      "teardrop             892\n",
      "warezclient          890\n",
      "pod                  201\n",
      "guess_passwd          53\n",
      "buffer_overflow       30\n",
      "warezmaster           20\n",
      "land                  18\n",
      "imap                  11\n",
      "rootkit               10\n",
      "loadmodule             9\n",
      "ftp_write              8\n",
      "multihop               7\n",
      "phf                    4\n",
      "perl                   3\n",
      "spy                    2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution Test set:\n",
      "normal             9711\n",
      "neptune            4657\n",
      "guess_passwd       1231\n",
      "mscan               996\n",
      "warezmaster         944\n",
      "apache2             737\n",
      "satan               735\n",
      "processtable        685\n",
      "smurf               665\n",
      "back                359\n",
      "snmpguess           331\n",
      "saint               319\n",
      "mailbomb            293\n",
      "snmpgetattack       178\n",
      "portsweep           157\n",
      "ipsweep             141\n",
      "httptunnel          133\n",
      "nmap                 73\n",
      "pod                  41\n",
      "buffer_overflow      20\n",
      "multihop             18\n",
      "named                17\n",
      "ps                   15\n",
      "sendmail             14\n",
      "rootkit              13\n",
      "xterm                13\n",
      "teardrop             12\n",
      "xlock                 9\n",
      "land                  7\n",
      "xsnoop                4\n",
      "ftp_write             3\n",
      "worm                  2\n",
      "loadmodule            2\n",
      "perl                  2\n",
      "sqlattack             2\n",
      "udpstorm              2\n",
      "phf                   2\n",
      "imap                  1\n",
      "Name: label, dtype: int64\n",
      "Training set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 70 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 23 categories\n",
      "\n",
      "Distribution of categories in service:\n",
      "http        40338\n",
      "private     21853\n",
      "domain_u     9043\n",
      "smtp         7313\n",
      "ftp_data     6860\n",
      "Name: service, dtype: int64\n",
      "Test set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 64 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 38 categories\n",
      "['Protocol_type_icmp', 'Protocol_type_tcp', 'Protocol_type_udp', 'service_IRC', 'service_X11', 'service_Z39_50', 'service_aol', 'service_auth', 'service_bgp', 'service_courier', 'service_csnet_ns', 'service_ctf', 'service_daytime', 'service_discard', 'service_domain', 'service_domain_u', 'service_echo', 'service_eco_i', 'service_ecr_i', 'service_efs', 'service_exec', 'service_finger', 'service_ftp', 'service_ftp_data', 'service_gopher', 'service_harvest', 'service_hostnames', 'service_http', 'service_http_2784', 'service_http_443', 'service_http_8001', 'service_imap4', 'service_iso_tsap', 'service_klogin', 'service_kshell', 'service_ldap', 'service_link', 'service_login', 'service_mtp', 'service_name', 'service_netbios_dgm', 'service_netbios_ns', 'service_netbios_ssn', 'service_netstat', 'service_nnsp', 'service_nntp', 'service_ntp_u', 'service_other', 'service_pm_dump', 'service_pop_2', 'service_pop_3', 'service_printer', 'service_private', 'service_red_i', 'service_remote_job', 'service_rje', 'service_shell', 'service_smtp', 'service_sql_net', 'service_ssh', 'service_sunrpc', 'service_supdup', 'service_systat', 'service_telnet', 'service_tftp_u', 'service_tim_i', 'service_time', 'service_urh_i', 'service_urp_i', 'service_uucp', 'service_uucp_path', 'service_vmnet', 'service_whois', 'flag_OTH', 'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0', 'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH']\n",
      "   protocol_type  service  flag\n",
      "0              1       20     9\n",
      "1              2       44     9\n",
      "2              1       49     5\n",
      "3              1       24     9\n",
      "4              1       24     9\n",
      "(125973, 123)\n",
      "(22544, 123)\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "X_train has shape: (125973, 122) \n",
      "y_train has shape: (125973, 1)\n",
      "X_test has shape: (22544, 122) \n",
      "y_test has shape: (22544, 1)\n",
      "Counter({0: 67343, 1: 45927, 2: 11656, 3: 995, 4: 52})\n",
      "[[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " ...\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]]\n",
      "---------------------------------------------------------------------------------\n",
      "Separating datasets\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Separating Training and Testing db\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining the DNN model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "5\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 5)                 615       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 645\n",
      "Trainable params: 645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 103961 samples\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 09:58:18.806091: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-28 09:58:18.812741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:67:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.54GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-05-28 09:58:18.812907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:68:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.53GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-05-28 09:58:18.812932: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-28 09:58:18.829001: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-28 09:58:18.829131: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-05-28 09:58:18.831471: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-28 09:58:18.831825: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-28 09:58:18.832351: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-05-28 09:58:18.832964: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-05-28 09:58:18.833069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-05-28 09:58:18.833078: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-05-28 09:58:18.833567: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-28 09:58:18.834753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-28 09:58:18.834766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2024-05-28 09:58:18.855507: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3699850000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103961/103961 [==============================] - 0s 2us/sample - loss: 8.1938 - accuracy: 0.0987\n",
      "Epoch 2/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 2.7262 - accuracy: 0.5148\n",
      "Epoch 3/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 2.4240 - accuracy: 0.7904\n",
      "Epoch 4/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.6642 - accuracy: 0.8018\n",
      "Epoch 5/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.4573 - accuracy: 0.8294\n",
      "Epoch 6/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.2404 - accuracy: 0.8309\n",
      "Epoch 7/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1405 - accuracy: 0.8334\n",
      "Epoch 8/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0951 - accuracy: 0.8344\n",
      "Epoch 9/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0523 - accuracy: 0.8345\n",
      "Epoch 10/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0698 - accuracy: 0.8352\n",
      "Epoch 11/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0808 - accuracy: 0.8375\n",
      "Epoch 12/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1002 - accuracy: 0.8430\n",
      "Epoch 13/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1022 - accuracy: 0.8456\n",
      "Epoch 14/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0924 - accuracy: 0.8500\n",
      "Epoch 15/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0962 - accuracy: 0.8563\n",
      "Epoch 16/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1103 - accuracy: 0.8588\n",
      "Epoch 17/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1120 - accuracy: 0.8607\n",
      "Epoch 18/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1100 - accuracy: 0.8615\n",
      "Epoch 19/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1052 - accuracy: 0.8605\n",
      "Epoch 20/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1095 - accuracy: 0.8601\n",
      "Epoch 21/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1081 - accuracy: 0.8603\n",
      "Epoch 22/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1055 - accuracy: 0.8603\n",
      "Epoch 23/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1037 - accuracy: 0.8612\n",
      "Epoch 24/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1019 - accuracy: 0.8615\n",
      "Epoch 25/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0968 - accuracy: 0.8597\n",
      "Epoch 26/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0858 - accuracy: 0.8594\n",
      "Epoch 27/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0627 - accuracy: 0.8596\n",
      "Epoch 28/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0413 - accuracy: 0.8597\n",
      "Epoch 29/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0282 - accuracy: 0.8599\n",
      "Epoch 30/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0218 - accuracy: 0.8605\n",
      "Epoch 31/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0161 - accuracy: 0.8612\n",
      "Epoch 32/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0046 - accuracy: 0.8616\n",
      "Epoch 33/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9670 - accuracy: 0.8619\n",
      "Epoch 34/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9638 - accuracy: 0.8624\n",
      "Epoch 35/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9619 - accuracy: 0.8634\n",
      "Epoch 36/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9668 - accuracy: 0.8604\n",
      "Epoch 37/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9766 - accuracy: 0.8647\n",
      "Epoch 38/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9936 - accuracy: 0.8664\n",
      "Epoch 39/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0363 - accuracy: 0.8701\n",
      "Epoch 40/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0520 - accuracy: 0.8725\n",
      "Epoch 41/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0445 - accuracy: 0.8735\n",
      "Epoch 42/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0162 - accuracy: 0.8745\n",
      "Epoch 43/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9888 - accuracy: 0.8760\n",
      "Epoch 44/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9842 - accuracy: 0.8763\n",
      "Epoch 45/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9801 - accuracy: 0.8758\n",
      "Epoch 46/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9794 - accuracy: 0.8758\n",
      "Epoch 47/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9837 - accuracy: 0.8766\n",
      "Epoch 48/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9880 - accuracy: 0.8773\n",
      "Epoch 49/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9900 - accuracy: 0.8782\n",
      "Epoch 50/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9935 - accuracy: 0.8786\n",
      "Epoch 51/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9942 - accuracy: 0.8789\n",
      "Epoch 52/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9951 - accuracy: 0.8791\n",
      "Epoch 53/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9997 - accuracy: 0.8780\n",
      "Epoch 54/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.0084 - accuracy: 0.8771\n",
      "Epoch 55/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9762 - accuracy: 0.8664\n",
      "Epoch 56/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 0.9919 - accuracy: 0.8599\n",
      "Epoch 57/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1259 - accuracy: 0.8552\n",
      "Epoch 58/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.1635 - accuracy: 0.8440\n",
      "Epoch 59/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.2531 - accuracy: 0.8256\n",
      "Epoch 60/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.2783 - accuracy: 0.8097\n",
      "Epoch 61/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.2748 - accuracy: 0.7799\n",
      "Epoch 62/1000\n",
      "103961/103961 [==============================] - 0s 1us/sample - loss: 1.2692 - accuracy: 0.7600\n",
      "---------------------------------------------------------------------------------\n",
      "ELAPSE TIME TRAINING MODEL:  0.1122524340947469 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Model Prediction\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSE TIME MODEL PREDICTION:  0.005966623624165853 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({0: 23102, 1: 16081, 2: 4144, 3: 1197, 4: 32})\n",
      "Counter({0: 22015, 1: 15479, 2: 5693, 3: 1369})\n",
      "87.67842714785887\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "### DNN Regression\n",
    "# It is now time to implements single-input and multiple-inputs DNN models\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Initializing DNN program')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "#---------------------------------------------------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Importing Libraries')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import LSTM\n",
    "#from keras.layers import Dropout\n",
    "#from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.preprocessing import sequence\n",
    "#from keras.utils import pad_sequences\n",
    "#from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "import shap\n",
    "np.random.seed(0)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining metric equations\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Metric Equations')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "def ACC(TP,TN,FP,FN):\n",
    "    Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return Acc\n",
    "def ACC_2 (TP, FN):\n",
    "    ac = (TP/(TP+FN))\n",
    "    return ac\n",
    "def PRECISION(TP,FP):\n",
    "    Precision = TP/(TP+FP)\n",
    "    return Precision\n",
    "def RECALL(TP,FN):\n",
    "    Recall = TP/(TP+FN)\n",
    "    return Recall\n",
    "def F1(Recall, Precision):\n",
    "    F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "    return F1\n",
    "def BACC(TP,TN,FP,FN):\n",
    "    BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "    return BACC\n",
    "def MCC(TP,TN,FP,FN):\n",
    "    MCC = (TN*TP-FN*FP)/(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**.5)\n",
    "    return MCC\n",
    "def AUC_ROC(y_test_bin,y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    auc_avg = 0\n",
    "    counting = 0\n",
    "    for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "     # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "      #print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "      auc_avg += auc(fpr[i], tpr[i])\n",
    "      counting = i+1\n",
    "    return auc_avg/counting\n",
    "#---------------------------------------------------------------------\n",
    "# Defining features of interest\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining features of interest')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "\n",
    "# First ensemble with NSL-KDD\n",
    "# Parameters\n",
    "\n",
    "#----------------------------------------------\n",
    "# 0 for not using it as base learner\n",
    "# 1 for using it as base learner\n",
    "\n",
    "use_model_ada = 1 \n",
    "use_model_dnn = 1 \n",
    "use_model_mlp = 1 \n",
    "use_model_lgbm = 1 \n",
    "use_model_rf = 1 \n",
    "use_model_svm = 1\n",
    "use_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "# 0 for training the model\n",
    "# 1 for using the saved version of the model\n",
    "\n",
    "load_model_ada = 0 \n",
    "load_model_dnn = 0 \n",
    "load_model_mlp = 0 \n",
    "load_model_lgbm = 0 \n",
    "load_model_rf = 0 \n",
    "load_model_svm = 0\n",
    "load_model_knn = 0 \n",
    "#----------------------------------------------\n",
    "\n",
    "# load_model_ada = 1\n",
    "# load_model_dnn = 1 \n",
    "# load_model_mlp = 1 \n",
    "# load_model_lgbm = 1 \n",
    "# load_model_rf = 1 \n",
    "# load_model_svm = 1\n",
    "# load_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "feature_selection_bit = 0\n",
    "# feature_selection_bit = 1\n",
    "\n",
    "\n",
    "\n",
    "# Specify the name of the output text file\n",
    "if feature_selection_bit == 0:\n",
    "    output_file_name = \"ensemble_base_models_all_features.txt\"\n",
    "    with open(output_file_name, \"w\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---- ensemble_base_models_all_features', file = f)\n",
    "\n",
    "elif feature_selection_bit == 1:\n",
    "    output_file_name = \"ensemble_base_models_feature_selection.txt\"\n",
    "    with open(output_file_name, \"w\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('----ensemble_base_models_feature_selection--', file = f)\n",
    "\n",
    "#!/usr/bin/env python       \n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import (StandardScaler, OrdinalEncoder,LabelEncoder, MinMaxScaler, OneHotEncoder)\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler , RobustScaler, PowerTransformer\n",
    "\n",
    "# importing library for plotting\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# from keras.layers import Dense # importing dense layer\n",
    "\n",
    "# from keras.layers import Input\n",
    "# from keras.models import Model\n",
    "# representation of model layers\n",
    "#from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shap\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_program = time.time()\n",
    "\n",
    "\n",
    "\n",
    "def confusion_metrics (name_model,predictions,true_labels,time_taken):\n",
    "\n",
    "    name = name_model\n",
    "    pred_label = predictions\n",
    "    y_test_01 = true_labels \n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(name, file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "    FPR = FP / (FP + TN)\n",
    "    FPR = 100*(sum(FPR)/len(FPR))\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    # voting_acc_01 = Acc\n",
    "    # voting_pre_01 = Precision\n",
    "    # weighed_avg_rec_01 = Recall\n",
    "    # weighed_avg_f1_01 = F1\n",
    "    # weighed_avg_bacc_01 = BACC\n",
    "    # weighed_avg_mcc_01 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Time Taken: ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('FPR: ', FPR, '%' ,file = f)\n",
    "\n",
    "    return Acc, Precision, Recall, F1, BACC, MCC, FPR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# attach the column names to the dataset\n",
    "feature=[\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\n",
    "          \"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\n",
    "          \"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\n",
    "          \"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
    "          \"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"]\n",
    "# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\n",
    "# these have already been removed.\n",
    "\n",
    "train='KDDTrain+.txt'\n",
    "test='KDDTest+.txt'\n",
    "\n",
    "df=pd.read_csv(train,names=feature)\n",
    "df_test=pd.read_csv(test,names=feature)\n",
    "\n",
    "\n",
    "\n",
    "# shape, this gives the dimensions of the dataset\n",
    "print('Dimensions of the Training set:',df.shape)\n",
    "print('Dimensions of the Test set:',df_test.shape)\n",
    "\n",
    "\n",
    "df.drop(['difficulty'],axis=1,inplace=True)\n",
    "df_test.drop(['difficulty'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print('Label distribution Training set:')\n",
    "print(df['label'].value_counts())\n",
    "print()\n",
    "print('Label distribution Test set:')\n",
    "print(df_test['label'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n",
    "# explore categorical features\n",
    "print('Training set:')\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\n",
    "print()\n",
    "print('Distribution of categories in service:')\n",
    "print(df['service'].value_counts().sort_values(ascending=False).head())\n",
    "\n",
    "\n",
    "\n",
    "# Test set\n",
    "print('Test set:')\n",
    "for col_name in df_test.columns:\n",
    "    if df_test[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df_test[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "# insert code to get a list of categorical columns into a variable, categorical_columns\n",
    "categorical_columns=['protocol_type', 'service', 'flag'] \n",
    " # Get the categorical values into a 2D numpy array\n",
    "df_categorical_values = df[categorical_columns]\n",
    "testdf_categorical_values = df_test[categorical_columns]\n",
    "df_categorical_values.head()\n",
    "\n",
    "\n",
    "# protocol type\n",
    "unique_protocol=sorted(df.protocol_type.unique())\n",
    "string1 = 'Protocol_type_'\n",
    "unique_protocol2=[string1 + x for x in unique_protocol]\n",
    "# service\n",
    "unique_service=sorted(df.service.unique())\n",
    "string2 = 'service_'\n",
    "unique_service2=[string2 + x for x in unique_service]\n",
    "# flag\n",
    "unique_flag=sorted(df.flag.unique())\n",
    "string3 = 'flag_'\n",
    "unique_flag2=[string3 + x for x in unique_flag]\n",
    "# put together\n",
    "dumcols=unique_protocol2 + unique_service2 + unique_flag2\n",
    "print(dumcols)\n",
    "\n",
    "#do same for test set\n",
    "unique_service_test=sorted(df_test.service.unique())\n",
    "unique_service2_test=[string2 + x for x in unique_service_test]\n",
    "testdumcols=unique_protocol2 + unique_service2_test + unique_flag2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_categorical_values_enc=df_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "print(df_categorical_values_enc.head())\n",
    "# test set\n",
    "testdf_categorical_values_enc=testdf_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "df_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\n",
    "df_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(),columns=dumcols)\n",
    "# test set\n",
    "testdf_categorical_values_encenc = enc.fit_transform(testdf_categorical_values_enc)\n",
    "testdf_cat_data = pd.DataFrame(testdf_categorical_values_encenc.toarray(),columns=testdumcols)\n",
    "\n",
    "df_cat_data.head()\n",
    "\n",
    "\n",
    "trainservice=df['service'].tolist()\n",
    "testservice= df_test['service'].tolist()\n",
    "difference=list(set(trainservice) - set(testservice))\n",
    "string = 'service_'\n",
    "difference=[string + x for x in difference]\n",
    "difference\n",
    "\n",
    "for col in difference:\n",
    "    testdf_cat_data[col] = 0\n",
    "\n",
    "testdf_cat_data.shape\n",
    "\n",
    "newdf=df.join(df_cat_data)\n",
    "newdf.drop('flag', axis=1, inplace=True)\n",
    "newdf.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf.drop('service', axis=1, inplace=True)\n",
    "# test data\n",
    "newdf_test=df_test.join(testdf_cat_data)\n",
    "newdf_test.drop('flag', axis=1, inplace=True)\n",
    "newdf_test.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf_test.drop('service', axis=1, inplace=True)\n",
    "print(newdf.shape)\n",
    "print(newdf_test.shape)\n",
    "\n",
    "\n",
    "# take label column\n",
    "labeldf=newdf['label']\n",
    "labeldf_test=newdf_test['label']\n",
    "# change the label column\n",
    "newlabeldf=labeldf.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "newlabeldf_test=labeldf_test.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "# put the new label column back\n",
    "newdf['label'] = newlabeldf\n",
    "newdf_test['label'] = newlabeldf_test\n",
    "print(newdf['label'].head())\n",
    "\n",
    "\n",
    "# Specify your selected features. Note that you'll need to modify this list according to your final processed dataframe\n",
    "#Uncomment the below lines to use these top 20 features from shap analysis\n",
    "#selected_features = [\"root_shell\",\"service_telnet\",\"num_shells\",\"service_uucp\",\"dst_host_same_src_port_rate\"\n",
    "#                     ,\"dst_host_rerror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_srv_count\",\"service_private\",\"logged_in\",\n",
    "#                    \"dst_host_serror_rate\",\"serror_rate\",\"srv_serror_rate\",\"flag_S0\",\"diff_srv_rate\",\"dst_host_srv_diff_host_rate\",\"num_file_creations\",\"flag_RSTR\"#,\"dst_host_same_srv_rate\",\"service_Idap\",\"label\"]\n",
    "                     \n",
    "\n",
    "# Select those features from your dataframe\n",
    "#newdf = newdf[selected_features]\n",
    "#newdf_test = newdf_test[selected_features]\n",
    "\n",
    "# Now your dataframe only contains your selected features.\n",
    "\n",
    "# creating a dataframe with multi-class labels (Dos,Probe,R2L,U2R,normal)\n",
    "multi_data = newdf.copy()\n",
    "multi_label = pd.DataFrame(multi_data.label)\n",
    "\n",
    "multi_data_test=newdf_test.copy()\n",
    "multi_label_test = pd.DataFrame(multi_data_test.label)\n",
    "\n",
    "\n",
    "# using standard scaler for normalizing\n",
    "std_scaler = StandardScaler()\n",
    "def standardization(df,col):\n",
    "    for i in col:\n",
    "        arr = df[i]\n",
    "        arr = np.array(arr)\n",
    "        df[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
    "    return df\n",
    "\n",
    "numeric_col = multi_data.select_dtypes(include='number').columns\n",
    "data = standardization(multi_data,numeric_col)\n",
    "numeric_col_test = multi_data_test.select_dtypes(include='number').columns\n",
    "data_test = standardization(multi_data_test,numeric_col_test)\n",
    "\n",
    "# label encoding (0,1,2,3,4) multi-class labels (Dos,normal,Probe,R2L,U2R)\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2_test = preprocessing.LabelEncoder()\n",
    "enc_label = multi_label.apply(le2.fit_transform)\n",
    "enc_label_test = multi_label_test.apply(le2_test.fit_transform)\n",
    "multi_data = multi_data.copy()\n",
    "multi_data_test = multi_data_test.copy()\n",
    "\n",
    "multi_data['intrusion'] = enc_label\n",
    "multi_data_test['intrusion'] = enc_label_test\n",
    "\n",
    "#y_mul = multi_data['intrusion']\n",
    "multi_data\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "\n",
    "multi_data.drop(labels= [ 'label'], axis=1, inplace=True)\n",
    "multi_data\n",
    "multi_data_test.drop(labels= [ 'label'], axis=1, inplace=True)\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "y_train_multi= multi_data[['intrusion']]\n",
    "X_train_multi= multi_data.drop(labels=['intrusion'], axis=1)\n",
    "\n",
    "print('X_train has shape:',X_train_multi.shape,'\\ny_train has shape:',y_train_multi.shape)\n",
    "\n",
    "y_test_multi= multi_data_test[['intrusion']]\n",
    "X_test_multi= multi_data_test.drop(labels=['intrusion'], axis=1)\n",
    "\n",
    "print('X_test has shape:',X_test_multi.shape,'\\ny_test has shape:',y_test_multi.shape)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_train_multi['intrusion'])\n",
    "print(label_counts)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "y_train_multi = LabelBinarizer().fit_transform(y_train_multi)\n",
    "\n",
    "y_test_multi = LabelBinarizer().fit_transform(y_test_multi)\n",
    "\n",
    "\n",
    "Y_train=y_train_multi.copy()\n",
    "X_train=X_train_multi.copy()\n",
    "\n",
    "Y_test=y_test_multi.copy()\n",
    "X_test=X_test_multi.copy()\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Assuming you have features X and labels Y\n",
    "# X, Y = make_classification()\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy='minority', random_state=100)\n",
    "\n",
    "X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "print(Y_test)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "X_train.values\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "single_class_train = np.argmax(y_train_multi, axis=1)\n",
    "single_class_test = np.argmax(y_test_multi, axis=1)\n",
    "\n",
    "\n",
    "df1 = X_train_multi.assign(Label = single_class_train)\n",
    "df2 =  X_test_multi.assign(Label = single_class_test)\n",
    "\n",
    "frames = [df1,  df2]\n",
    "\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "\n",
    "df_fs = df\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating datasets')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "test2 = X.assign(ALERT = y)\n",
    "\n",
    "\n",
    "# label encoding (0,1,2,3,4) multi-class labels (Dos,normal,Probe,R2L,U2R)\n",
    "\n",
    "Dos_samples = test2[test2['ALERT'] == 0]\n",
    "Normal_samples = test2[test2['ALERT'] == 1]\n",
    "Probe_samples = test2[test2['ALERT'] == 2]\n",
    "R2L_samples = test2[test2['ALERT'] == 3]\n",
    "U2R_samples = test2[test2['ALERT'] == 4]\n",
    "\n",
    "Probe_y = Probe_samples.pop('ALERT')\n",
    "Dos_y = Dos_samples.pop('ALERT')\n",
    "Normal_y = Normal_samples.pop('ALERT')\n",
    "U2R_y = U2R_samples.pop('ALERT')\n",
    "R2L_y = R2L_samples.pop('ALERT')\n",
    "\n",
    "test2.pop('ALERT')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = X.assign(Label = y)\n",
    "\n",
    "\n",
    "# y = df.pop('Label')\n",
    "# X = df\n",
    "\n",
    "y1, y2 = pd.factorize(y)\n",
    "\n",
    "y_0 = pd.DataFrame(y1)\n",
    "y_1 = pd.DataFrame(y1)\n",
    "y_2 = pd.DataFrame(y1)\n",
    "y_3 = pd.DataFrame(y1)\n",
    "y_4 = pd.DataFrame(y1)\n",
    "\n",
    "\n",
    "# y_0 = y_0.replace(0, 0)\n",
    "# y_0 = y_0.replace(1, 1)\n",
    "y_0 = y_0.replace(2, 1)\n",
    "y_0 = y_0.replace(3, 1)\n",
    "y_0 = y_0.replace(4, 1)\n",
    "\n",
    "\n",
    "y_1 = y_1.replace(1, 999)\n",
    "y_1 = y_1.replace(0, 1)\n",
    "# y_1 = y_1.replace(1, 0)\n",
    "y_1 = y_1.replace(2, 1)\n",
    "y_1 = y_1.replace(3, 1)\n",
    "y_1 = y_1.replace(4, 1)\n",
    "y_1 = y_1.replace(999, 1)\n",
    "\n",
    "\n",
    "y_2 = y_2.replace(0, 1)\n",
    "y_2 = y_2.replace(1, 1)\n",
    "y_2 = y_2.replace(2, 0)\n",
    "y_2 = y_2.replace(3, 1)\n",
    "y_2 = y_2.replace(4, 1)\n",
    "\n",
    "\n",
    "y_3 = y_3.replace(0, 1)\n",
    "# y_3 = y_3.replace(1, 1)\n",
    "y_3 = y_3.replace(2, 1)\n",
    "y_3 = y_3.replace(3, 0)\n",
    "y_3 = y_3.replace(4, 1)\n",
    "\n",
    "\n",
    "y_4 = y_4.replace(0, 1)\n",
    "# y_4 = y_4.replace(1, 1)\n",
    "y_4 = y_4.replace(2, 1)\n",
    "y_4 = y_4.replace(3, 1)\n",
    "y_4 = y_4.replace(4, 0)\n",
    "\n",
    "\n",
    "\n",
    "df = df.assign(Label = y)\n",
    "\n",
    "#Divide the dataset between level 00 and level 01\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split = 0.7 # 0.5\n",
    "\n",
    "# # X_00,X_01, y_00, y_01 = sklearn.model_selection.train_test_split(X, y, train_size=split)\n",
    "# X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=split)\n",
    "\n",
    "# Separate Training and Testing db\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating Training and Testing db')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.7,random_state=42)\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "y_train\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Now you can use Keras modules directly from tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Flatten\n",
    "import innvestigate\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining the DNN model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# dropout_rate = 0.01\n",
    "# nodes = (len(X_train.columns,) + 5 //2) \n",
    "nodes = 5\n",
    "print(nodes)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(len(X_train.columns,))))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# # Define your DNN model\n",
    "# model = Sequential([\n",
    "#     Flatten(input_shape=(28, 28)),  # Input layer\n",
    "#     Dense(128, activation='relu'),  # Hidden layer\n",
    "#     Dense(10, activation='relu')  # Output layer\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save('your_model.h5')\n",
    "\n",
    "# # Load your trained model\n",
    "# model = tf.keras.models.load_model('your_model.h5')\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "start = time.time()\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Modify model.fit to include the EarlyStopping callback\n",
    "# model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train), callbacks=[early_stopping,lr_scheduler])\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train), callbacks=[early_stopping])\n",
    "\n",
    "# Pass the lr_scheduler callback to your model.fit() function\n",
    "# model.fit(X_train, y_train, callbacks=[lr_scheduler], ...)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('ELAPSE TIME TRAINING MODEL: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Model Prediction')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time()\n",
    "print('ELAPSE TIME MODEL PREDICTION: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "#print(y_pred)\n",
    "ynew = np.argmax(y_pred,axis = 1)\n",
    "#print(ynew)\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "#print(score)\n",
    "label = y2\n",
    "# pred_label = label[ynew]\n",
    "#print(score)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# pd.crosstab(test['ALERT'], preds, rownames=['Actual ALERT'], colnames = ['Predicted ALERT'])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_test)\n",
    "print(label_counts)\n",
    "\n",
    "label_counts = Counter(ynew)\n",
    "print(label_counts)\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train\n",
    "test = X_test\n",
    "labels_train = y_train\n",
    "labels_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Dos','normal','Probe','R2L','U2R']\n",
    "# label encoding (0,1,2,3,4) multi-class labels (Dos,normal,Probe,R2L,U2R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = shap.DeepExplainer(model,train.values.astype('float'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to test sample with the waterfall plot\n",
    "def waterfall_explanator(sample):\n",
    "\n",
    "    index = np.argmax(model.predict(sample)) # Prediction of the sample\n",
    "    prediction = index\n",
    "\n",
    "    analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model)\n",
    "    analysis = analyzer.analyze(sample)\n",
    "\n",
    "    names = sample.columns\n",
    "\n",
    "    scores = pd.DataFrame(analysis)\n",
    "    scores_abs = scores.abs()\n",
    "    sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "    names = list(names)\n",
    "\n",
    "    sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "    sum_of_columns\n",
    "    combined = list(zip(names, sum_of_columns))\n",
    "    # Sort the combined list in descending order based on the values from sum_of_columns\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "    sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "    shap_val = sorted_sum_of_columns\n",
    "    feature_name = sorted_names\n",
    "    # sorted_names\n",
    "    feature_val = []\n",
    "    for j in sorted_names:\n",
    "            feature_val.append(float(sample[j]))\n",
    "    return (prediction, shap_val,feature_val,feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_all(single_class_samples,number_samples, number_of_features_pertubation):\n",
    "    Bucket = {\n",
    "    '0.0': 0,\n",
    "    '0.1':0,\n",
    "    '0.2':0,\n",
    "    '0.3':0,\n",
    "    '0.4':0,\n",
    "    '0.5':0,\n",
    "    '0.6':0,\n",
    "    '0.7':0,\n",
    "    '0.8':0,\n",
    "    '0.9':0,\n",
    "    '1.0':0,\n",
    "\n",
    "           }\n",
    "    # Counter_chart = 0\n",
    "    Counter_all_samples = 0\n",
    "    counter_samples_changed_class = 0\n",
    "    print('------------------------------------------------')\n",
    "    print('Initiating Completeness Experiment')\n",
    "    print('------------------------------------------------')\n",
    "    for i in range(0,number_samples):\n",
    "        #select sample\n",
    "        try:\n",
    "            sample = single_class_samples[i:i+1]\n",
    "        except:\n",
    "            break # break if there more samples requested than samples in the dataset\n",
    "        # Explanate the original sample\n",
    "        u = waterfall_explanator(sample)\n",
    "        #select top 5 features from the original sample\n",
    "        top_k_features = []\n",
    "        top_k_features.append(u[3][0]) #append first feature\n",
    "        break_condition = False\n",
    "        for k in range(1,number_of_features_pertubation):\n",
    "            for j in range(11):  # 11 steps to include 1.0 (0 to 10)\n",
    "                if break_condition == True: break\n",
    "                perturbation = j / 10.0  # Divide by 10 to get steps of 0.1\n",
    "                temp_var = sample[top_k_features[k-1]]\n",
    "                result = np.where((temp_var - perturbation) < 0, True, False)\n",
    "                if result < 0: \n",
    "                    sample[top_k_features[k-1]] = 1 - perturbation\n",
    "                else: sample[top_k_features[k-1]] = temp_var - perturbation\n",
    "                # sample[top_k_features[k-1]] = perturbation\n",
    "                v = waterfall_explanator(sample)\n",
    "                if v[0] != u[0]: \n",
    "                    # print(str(perturbation))\n",
    "                    Bucket[str(perturbation)] += 1              \n",
    "                    break_condition = True\n",
    "                    counter_samples_changed_class += 1     \n",
    "                    # Bucket[str(perturbation)] = counter_samples_changed_class              \n",
    "                    break\n",
    "                else: sample[top_k_features[k-1]] = abs(temp_var - 1) # set the sample feature value as the symetric opposite\n",
    "            # print(u)\n",
    "            top_k_features.append(u[3][k]) #append second, third feature .. and so on\n",
    "            if break_condition == True: break\n",
    "        Counter_all_samples += 1\n",
    "        progress  = 100*Counter_all_samples/number_samples\n",
    "        if progress%10 == 0: print('Progress', progress ,'%')\n",
    "    # print('Number of Normal samples that changed classification: ',counter_samples_changed_class)\n",
    "    # print('Number of all samples analyzed: ',Counter_all_samples)\n",
    "    # for key in Bucket:\n",
    "    #     Bucket[key]=number_samples - Bucket[key]\n",
    "    # Bucket['0.0'] = number_samples\n",
    "    # for key in Bucket:\n",
    "    #     Bucket[key]=Bucket[key]\n",
    "    dict = Bucket\n",
    "    temp = 0\n",
    "    for k in dict:\n",
    "        dict[k] = dict[k] + temp\n",
    "        temp = dict[k]\n",
    "    total = number_samples\n",
    "    y_axis = []\n",
    "    for k in dict:\n",
    "        dict[k] = abs(dict[k] - total)\n",
    "        y_axis.append(dict[k]/total)    \n",
    "    return(counter_samples_changed_class,Counter_all_samples,y_axis)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_samples = 100\n",
    "# K_feat =  Dos_samples.shape[1]\n",
    "K_feat =  2\n",
    "output_file_name = 'DNN_IG_NSL_Completeness.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = K_samples\n",
    "num_feat_pertubation = K_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(3, 100, [1.0, 0.98, 0.98, 0.98, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97])\n",
      "Number of DoS samples that changed classification:  3\n",
      "Number of all samples analyzed:  100\n",
      "3.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = completeness_all(Dos_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of DoS samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_dos = p[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(output_file_name, \"w\") as f:print('',file = f)\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_dos = [',y_axis_dos ,']',file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mcompleteness_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNormal_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_feat_pertubation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(p)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Normal samples that changed classification: \u001b[39m\u001b[38;5;124m'\u001b[39m,p[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mcompleteness_all\u001b[0;34m(single_class_samples, number_samples, number_of_features_pertubation)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: sample[top_k_features[k\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m temp_var \u001b[38;5;241m-\u001b[39m perturbation\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# sample[top_k_features[k-1]] = perturbation\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[43mwaterfall_explanator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m u[\u001b[38;5;241m0\u001b[39m]: \n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# print(str(perturbation))\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     Bucket[\u001b[38;5;28mstr\u001b[39m(perturbation)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m              \n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mwaterfall_explanator\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      5\u001b[0m prediction \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m      7\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m innvestigate\u001b[38;5;241m.\u001b[39mcreate_analyzer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintegrated_gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[0;32m----> 8\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m names \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     12\u001b[0m scores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(analysis)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/innvestigate/analyzer/wrapper.py:155\u001b[0m, in \u001b[0;36mAugmentReduceBase.analyze\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# As described in the AugmentReduceBase init,\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# both ns_mode \"max_activation\" and \"index\" make use\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# of a subanalyzer using neuron_selection_mode=\"index\".\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ns_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_activation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# obtain max neuron activations over batch\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subanalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ns_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# TODO: make neuron_selection arg or kwarg, not both\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:988\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    987\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 988\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py:703\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[1;32m    701\u001b[0m x, _, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_standardize_user_data(\n\u001b[1;32m    702\u001b[0m     x, check_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, steps_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m'\u001b[39m, steps\u001b[38;5;241m=\u001b[39msteps)\n\u001b[0;32m--> 703\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py:380\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(mode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_index, batch_logs)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    382\u001b[0m   batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4020\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m   4018\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(inputs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 4020\u001b[0m   session \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4021\u001b[0m   feed_arrays \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   4022\u001b[0m   array_vals \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:745\u001b[0m, in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _MANUAL_VAR_INIT:\n\u001b[1;32m    744\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 745\u001b[0m     \u001b[43m_initialize_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:1193\u001b[0m, in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     candidate_vars\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m candidate_vars:\n\u001b[1;32m   1191\u001b[0m   \u001b[38;5;66;03m# This step is expensive, so we only run it on variables not already\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m   \u001b[38;5;66;03m# marked as initialized.\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m   is_initialized \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m      \u001b[49m\u001b[43m[\u001b[49m\u001b[43mvariables_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_variable_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcandidate_vars\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m   \u001b[38;5;66;03m# TODO(kathywu): Some metric variables loaded from SavedModel are never\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m   \u001b[38;5;66;03m# actually used, and do not have an initializer.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m   should_be_initialized \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1198\u001b[0m       (\u001b[38;5;129;01mnot\u001b[39;00m is_initialized[n]) \u001b[38;5;129;01mand\u001b[39;00m v\u001b[38;5;241m.\u001b[39minitializer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m n, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(candidate_vars)]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/client/session.py:967\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    964\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 967\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    970\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/client/session.py:1190\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1190\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1193\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/client/session.py:1368\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/client/session.py:1375\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1374\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1377\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/client/session.py:1358\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1357\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m-> 1358\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extend_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1360\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf23/lib/python3.8/site-packages/tensorflow/python/client/session.py:1398\u001b[0m, in \u001b[0;36mBaseSession._extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extend_graph\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1397\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39m_session_run_lock():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1398\u001b[0m     \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExtendSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Normal_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Normal samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_normal = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_normal = [',y_axis_normal ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(16, 100, [1.0, 0.98, 0.98, 0.95, 0.94, 0.9, 0.86, 0.84, 0.84, 0.84, 0.84])\n",
      "Number of Probe samples that changed classification:  16\n",
      "Number of all samples analyzed:  100\n",
      "16.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = completeness_all(Probe_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Probe samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_probe = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_probe = [',y_axis_probe ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(34, 100, [1.0, 0.99, 0.98, 0.97, 0.97, 0.96, 0.93, 0.9, 0.66, 0.66, 0.66])\n",
      "Number of R2L_samples samples that changed classification:  34\n",
      "Number of all samples analyzed:  100\n",
      "34.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = completeness_all(R2L_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of R2L_samples samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_R2L = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_R2L = [',y_axis_R2L ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(46, 100, [1.0, 0.93, 0.88, 0.83, 0.81, 0.78, 0.73, 0.72, 0.64, 0.56, 0.54])\n",
      "Number of U2R samples that changed classification:  46\n",
      "Number of all samples analyzed:  100\n",
      "46.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = completeness_all(U2R_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of U2R samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_U2R = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_U2R = [',y_axis_U2R ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "y_axis_dos = [ [1.0, 0.98, 0.98, 0.98, 0.98, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97] ]\n",
    "y_axis_normal = [ [1.0, 1.0, 1.0, 1.0, 0.98, 0.97, 0.95, 0.95, 0.95, 0.95, 0.95] ]\n",
    "y_axis_probe = [1.0, 0.98, 0.98, 0.95, 0.94, 0.9, 0.86, 0.84, 0.84, 0.84, 0.84]\n",
    "y_axis_R2L = [ [1.0, 0.99, 0.98, 0.97, 0.97, 0.96, 0.93, 0.9, 0.66, 0.66, 0.66] ]\n",
    "y_axis_U2R = [ [1.0, 0.93, 0.88, 0.83, 0.81, 0.78, 0.73, 0.72, 0.64, 0.56, 0.54] ]\n",
    "\n",
    "\n",
    "# Plot the first line\n",
    "plt.plot(x_axis, y_axis_dos, label='DoS', color='blue', linestyle='--', marker='o')\n",
    "\n",
    "# # Plot the second line\n",
    "# plt.plot(x_axis, y_axis_normal, label='Normal', color='red', linestyle='--', marker='x')\n",
    "\n",
    "# # Plot the third line\n",
    "# plt.plot(x_axis, y_axis_ps, label='Port Scan', color='green', linestyle='--', marker='s')\n",
    "\n",
    "# # Plot the fourth line\n",
    "# plt.plot(x_axis, y_axis_infiltration, label='Infiltration', color='purple', linestyle='--', marker='p')\n",
    "\n",
    "# # Plot the fifth line\n",
    "# plt.plot(x_axis, y_axis_bot, label='Bot', color='orange', linestyle='--', marker='h')\n",
    "\n",
    "# # Plot the sixth line\n",
    "# plt.plot(x_axis, y_axis_web, label='Web Attack', color='magenta', linestyle='--', marker='+')\n",
    "\n",
    "# # Plot the seventh line\n",
    "# plt.plot(x_axis, y_axis_brute, label='Brute Force', color='cyan', linestyle='--', marker='_')\n",
    "\n",
    "# Enable grid lines (both major and minor grids)\n",
    "plt.grid()\n",
    "\n",
    "# Customize grid lines (optional)\n",
    "# plt.grid()\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Perturbations')\n",
    "plt.ylabel('Samples remaining')\n",
    "plt.legend()\n",
    "\n",
    "# Set the title of the plot\n",
    "# plt.title('Accuracy x Features - SHAP SML')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.savefig('GRAPH_PERT_SHAP_CIC.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
