{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 09:54:44.869950: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Initializing DNN program\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Importing Libraries\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oarreche@ads.iu.edu/anaconda3/envs/tf23/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining Metric Equations\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining features of interest\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Loading Databases\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Normalizing database\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Separating features and labels\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'BENIGN': 2273097, 'Dos/Ddos': 380699, 'PortScan': 158930, 'Brute Force': 13835, 'Web Attack': 2180, 'Bot': 1966, 'Infiltration': 36})\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Separating datasets\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'BENIGN': 2273097, 'Dos/Ddos': 380699, 'PortScan': 158930, 'Brute Force': 13835, 'Web Attack': 2180, 'Bot': 1966, 'Infiltration': 36})\n",
      "---------------------------------------------------------------------------------\n",
      "Separating Training and Testing db\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining the DNN model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "---------------------------------------------------------------------------------\n",
      "Training the model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Train on 1981520 samples\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 09:54:56.004525: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-30 09:54:56.010942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:67:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.54GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-05-30 09:54:56.011091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:68:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.53GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2024-05-30 09:54:56.011112: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-30 09:54:56.026737: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-30 09:54:56.026841: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-05-30 09:54:56.029083: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-30 09:54:56.029420: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-30 09:54:56.029906: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-05-30 09:54:56.030624: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-05-30 09:54:56.030723: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-05-30 09:54:56.030732: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-05-30 09:54:56.031138: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-30 09:54:56.032560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-30 09:54:56.032572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2024-05-30 09:54:56.041624: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3699850000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 2.2713 - accuracy: 0.7342\n",
      "Epoch 2/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 2.6248 - accuracy: 0.7968\n",
      "Epoch 3/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 2.8016 - accuracy: 0.7998\n",
      "Epoch 4/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 2.7485 - accuracy: 0.8009\n",
      "Epoch 5/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 2.6404 - accuracy: 0.8016\n",
      "Epoch 6/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 2.5482 - accuracy: 0.8018\n",
      "Epoch 7/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 2.2420 - accuracy: 0.8020\n",
      "Epoch 8/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.9486 - accuracy: 0.8023\n",
      "Epoch 9/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.3447 - accuracy: 0.8025\n",
      "Epoch 10/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2356 - accuracy: 0.8026\n",
      "Epoch 11/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2288 - accuracy: 0.8027\n",
      "Epoch 12/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2324 - accuracy: 0.8028\n",
      "Epoch 13/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2329 - accuracy: 0.8029\n",
      "Epoch 14/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2313 - accuracy: 0.8030\n",
      "Epoch 15/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2281 - accuracy: 0.8030\n",
      "Epoch 16/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2185 - accuracy: 0.8030\n",
      "Epoch 17/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.2038 - accuracy: 0.8030\n",
      "Epoch 18/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1971 - accuracy: 0.8030\n",
      "Epoch 19/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1908 - accuracy: 0.8030\n",
      "Epoch 20/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1852 - accuracy: 0.8030\n",
      "Epoch 21/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1815 - accuracy: 0.8030\n",
      "Epoch 22/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1864 - accuracy: 0.8031\n",
      "Epoch 23/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1955 - accuracy: 0.8031\n",
      "Epoch 24/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1916 - accuracy: 0.8031\n",
      "Epoch 25/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1846 - accuracy: 0.8031\n",
      "Epoch 26/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1835 - accuracy: 0.8031\n",
      "Epoch 27/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1853 - accuracy: 0.8031\n",
      "Epoch 28/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1877 - accuracy: 0.8031\n",
      "Epoch 29/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1895 - accuracy: 0.8031\n",
      "Epoch 30/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1904 - accuracy: 0.8031\n",
      "Epoch 31/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1900 - accuracy: 0.8031\n",
      "Epoch 32/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1887 - accuracy: 0.8031\n",
      "Epoch 33/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1866 - accuracy: 0.8031\n",
      "Epoch 34/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1842 - accuracy: 0.8031\n",
      "Epoch 35/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1822 - accuracy: 0.8031\n",
      "Epoch 36/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1811 - accuracy: 0.8031\n",
      "Epoch 37/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1812 - accuracy: 0.8031\n",
      "Epoch 38/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1821 - accuracy: 0.8031\n",
      "Epoch 39/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1819 - accuracy: 0.8031\n",
      "Epoch 40/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1802 - accuracy: 0.8031\n",
      "Epoch 41/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1780 - accuracy: 0.8031\n",
      "Epoch 42/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1770 - accuracy: 0.8031\n",
      "Epoch 43/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1763 - accuracy: 0.8031\n",
      "Epoch 44/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1775 - accuracy: 0.8031\n",
      "Epoch 45/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1775 - accuracy: 0.8031\n",
      "Epoch 46/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1770 - accuracy: 0.8031\n",
      "Epoch 47/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1759 - accuracy: 0.8031\n",
      "Epoch 48/1000\n",
      "1981520/1981520 [==============================] - 0s 0us/sample - loss: 1.1745 - accuracy: 0.8031\n",
      "---------------------------------------------------------------------------------\n",
      "ELAPSE TIME TRAINING MODEL:  0.22546061277389526 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Model Prediction\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSE TIME MODEL PREDICTION:  0.1398331840833028 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "80.27620542543006\n",
      "Counter({0: 681732, 2: 114534, 1: 47609, 3: 4083, 5: 672, 4: 578, 6: 15})\n",
      "Counter({0: 849209, 2: 14})\n",
      "80.27620542543006\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "### DNN Regression\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Initializing DNN program')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "#---------------------------------------------------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Importing Libraries')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import LSTM\n",
    "#from keras.layers import Dropout\n",
    "#from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.preprocessing import sequence\n",
    "#from keras.utils import pad_sequences\n",
    "#from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "import shap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "np.random.seed(0)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining metric equations\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Metric Equations')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "def ACC(TP,TN,FP,FN):\n",
    "    Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return Acc\n",
    "def ACC_2 (TP, FN):\n",
    "    ac = (TP/(TP+FN))\n",
    "    return ac\n",
    "def PRECISION(TP,FP):\n",
    "    Precision = TP/(TP+FP)\n",
    "    return Precision\n",
    "def RECALL(TP,FN):\n",
    "    Recall = TP/(TP+FN)\n",
    "    return Recall\n",
    "def F1(Recall, Precision):\n",
    "    F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "    return F1\n",
    "def BACC(TP,TN,FP,FN):\n",
    "    BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "    return BACC\n",
    "def MCC(TP,TN,FP,FN):\n",
    "    MCC = (TN*TP-FN*FP)/(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**.5)\n",
    "    return MCC\n",
    "def AUC_ROC(y_test_bin,y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    auc_avg = 0\n",
    "    counting = 0\n",
    "    for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "     # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "      #print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "      auc_avg += auc(fpr[i], tpr[i])\n",
    "      counting = i+1\n",
    "    return auc_avg/counting\n",
    "#---------------------------------------------------------------------\n",
    "# Defining features of interest\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining features of interest')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# req_cols = [ ' Packet Length Std', ' Total Length of Bwd Packets', ' Subflow Bwd Bytes',\n",
    "# ' Destination Port', ' Packet Length Variance', ' Bwd Packet Length Mean',' Avg Bwd Segment Size',\n",
    "# 'Bwd Packet Length Max', ' Init_Win_bytes_backward','Total Length of Fwd Packets',\n",
    "# ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' Average Packet Size', ' Packet Length Mean',\n",
    "# ' Max Packet Length',' Label']\n",
    "\n",
    "# req_cols = [' Destination Port',' Flow Duration',' Total Fwd Packets',' Total Backward Packets','Total Length of Fwd Packets',' Total Length of Bwd Packets',' Fwd Packet Length Max',' Fwd Packet Length Min',' Fwd Packet Length Mean',' Fwd Packet Length Std','Bwd Packet Length Max',' Bwd Packet Length Min',' Bwd Packet Length Mean',' Bwd Packet Length Std','Flow Bytes/s',' Flow Packets/s',' Flow IAT Mean',' Flow IAT Std',' Flow IAT Max',' Flow IAT Min','Fwd IAT Total',' Fwd IAT Mean',' Fwd IAT Std',' Fwd IAT Max',' Fwd IAT Min','Bwd IAT Total',' Bwd IAT Mean',' Bwd IAT Std',' Bwd IAT Max',' Bwd IAT Min','Fwd PSH Flags',' Bwd PSH Flags',' Fwd URG Flags',' Bwd URG Flags',' Fwd Header Length',' Bwd Header Length','Fwd Packets/s',' Bwd Packets/s',' Min Packet Length',' Max Packet Length',' Packet Length Mean',' Packet Length Std',' Packet Length Variance','FIN Flag Count',' SYN Flag Count',' RST Flag Count',' PSH Flag Count',' ACK Flag Count',' URG Flag Count',' CWE Flag Count',' ECE Flag Count',' Down/Up Ratio',' Average Packet Size',' Avg Fwd Segment Size',' Avg Bwd Segment Size',' Fwd Header Length','Fwd Avg Bytes/Bulk',' Fwd Avg Packets/Bulk',' Fwd Avg Bulk Rate',' Bwd Avg Bytes/Bulk',' Bwd Avg Packets/Bulk','Bwd Avg Bulk Rate','Subflow Fwd Packets',' Subflow Fwd Bytes',' Subflow Bwd Packets',' Subflow Bwd Bytes','Init_Win_bytes_forward',' Init_Win_bytes_backward',' act_data_pkt_fwd',' min_seg_size_forward','Active Mean',' Active Std',' Active Max',' Active Min','Idle Mean',' Idle Std',' Idle Max',' Idle Min',' Label']\n",
    "\n",
    "# req_cols = [' Down/Up Ratio', ' Fwd URG Flags', ' Flow IAT Std', 'Subflow Fwd Packets', ' Flow Packets/s', ' URG Flag Count', 'FIN Flag Count', ' Bwd Packets/s', 'Bwd Avg Bulk Rate', ' act_data_pkt_fwd', ' Fwd Packet Length Std', ' Bwd Avg Bytes/Bulk', ' Active Max', ' Flow IAT Max', ' min_seg_size_forward', ' Bwd Packet Length Std', ' Fwd IAT Std', ' Fwd Avg Bulk Rate', ' Fwd Packet Length Mean', ' Fwd Packet Length Max', ' Idle Std', ' CWE Flag Count', 'Fwd IAT Total', ' ACK Flag Count', ' Bwd URG Flags', ' Flow IAT Min', ' Flow IAT Mean', ' Total Backward Packets', ' Fwd Avg Packets/Bulk', 'Fwd Avg Bytes/Bulk', ' SYN Flag Count', ' Min Packet Length', ' Fwd Packet Length Min', 'Idle Mean', 'Fwd PSH Flags', ' Fwd IAT Min', ' Fwd Header Length', ' RST Flag Count', ' Idle Max', ' PSH Flag Count', ' Bwd Header Length', ' ECE Flag Count', ' Subflow Bwd Packets', 'Active Mean', 'Flow Bytes/s', ' Bwd IAT Mean', ' Avg Fwd Segment Size', ' Bwd Packet Length Min', ' Active Std', ' Bwd IAT Min', ' Flow Duration', 'Fwd Packets/s', ' Fwd IAT Max', 'Bwd IAT Total', ' Idle Min', ' Bwd PSH Flags', ' Bwd Avg Packets/Bulk', ' Total Fwd Packets', ' Active Min', ' Bwd IAT Std', ' Fwd IAT Mean', ' Bwd IAT Max', ' Label']\n",
    "\n",
    "req_cols = [\n",
    "    \n",
    "    ' Packet Length Std', ' Total Length of Bwd Packets', ' Subflow Bwd Bytes',\n",
    "    ' Destination Port', ' Packet Length Variance', ' Bwd Packet Length Mean',' Avg Bwd Segment Size',\n",
    "    'Bwd Packet Length Max', ' Init_Win_bytes_backward','Total Length of Fwd Packets',\n",
    "    ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' Average Packet Size', ' Packet Length Mean',\n",
    "    ' Max Packet Length',\n",
    "    ' Down/Up Ratio', ' Fwd URG Flags', ' Flow IAT Std', 'Subflow Fwd Packets', ' Flow Packets/s', ' URG Flag Count', 'FIN Flag Count', ' Bwd Packets/s', 'Bwd Avg Bulk Rate'\n",
    "    , ' act_data_pkt_fwd', ' Fwd Packet Length Std', ' Bwd Avg Bytes/Bulk', ' Active Max', ' Flow IAT Max', ' min_seg_size_forward', ' Bwd Packet Length Std', ' Fwd IAT Std', ' Fwd Avg Bulk Rate', ' Fwd Packet Length Mean', ' Fwd Packet Length Max', ' Idle Std', ' CWE Flag Count', 'Fwd IAT Total'\n",
    "    \n",
    "    , ' ACK Flag Count', ' Bwd URG Flags', ' Flow IAT Min', ' Flow IAT Mean', ' Total Backward Packets', ' Fwd Avg Packets/Bulk', 'Fwd Avg Bytes/Bulk', ' SYN Flag Count', ' Min Packet Length', ' Fwd Packet Length Min', 'Idle Mean', 'Fwd PSH Flags', ' Fwd IAT Min'\n",
    "     \n",
    "    ,  ' Fwd Header Length', ' RST Flag Count', ' Idle Max', ' PSH Flag Count', ' Bwd Header Length', ' ECE Flag Count', ' Subflow Bwd Packets', 'Active Mean', 'Flow Bytes/s', ' Bwd IAT Mean', ' Avg Fwd Segment Size', ' Bwd Packet Length Min', ' Active Std', ' Bwd IAT Min', ' Flow Duration', 'Fwd Packets/s', ' Fwd IAT Max', 'Bwd IAT Total', ' Idle Min', ' Bwd PSH Flags', ' Bwd Avg Packets/Bulk', ' Total Fwd Packets', ' Active Min', ' Bwd IAT Std', ' Fwd IAT Mean', ' Bwd IAT Max'\n",
    "            \n",
    "            , ' Label']\n",
    "\n",
    "# Information gain top 10 features\n",
    "top10 = [' Average Packet Size',\n",
    "          ' Packet Length Std', \n",
    "          ' Packet Length Variance', \n",
    "          ' Packet Length Mean',\n",
    "            ' Destination Port', \n",
    "            ' Subflow Bwd Bytes', \n",
    "            ' Total Length of Bwd Packets', \n",
    "            ' Avg Bwd Segment Size', \n",
    "            ' Bwd Packet Length Mean',  \n",
    "            'Bwd Packet Length Max', \n",
    "            ' Label']\n",
    "\n",
    "\n",
    "\n",
    "req_cols = top10\n",
    "\n",
    "# req_cols = [' Destination Port',' Flow Duration',' Total Fwd Packets',' Total Backward Packets','Total Length of Fwd Packets',' Total Length of Bwd Packets',' Label']\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "\n",
    "path_str = '/home/oarreche@ads.iu.edu/HITL/cicids/cicids_db/'\n",
    "fraction = 1\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Loading Databases')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "df0 = pd.read_csv (path_str + 'Wednesday-workingHours.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "df1 = pd.read_csv (path_str + 'Tuesday-WorkingHours.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df2 = pd.read_csv (path_str +'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df3 = pd.read_csv (path_str +'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df4 = pd.read_csv (path_str +'Monday-WorkingHours.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df5 = pd.read_csv (path_str +'Friday-WorkingHours-Morning.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df6 = pd.read_csv (path_str +'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df7 = pd.read_csv (path_str +'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "frames = [df0, df1, df2, df3, df4, df5, df6, df7]\n",
    "\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Normalizing database')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "df_max_scaled = df.copy()\n",
    "\n",
    "y = df_max_scaled[' Label'].replace({'DDoS' :'Dos/Ddos' ,'DoS GoldenEye': 'Dos/Ddos', 'DoS Hulk': 'Dos/Ddos', 'DoS Slowhttptest': 'Dos/Ddos', 'DoS slowloris': 'Dos/Ddos', 'Heartbleed': 'Dos/Ddos','FTP-Patator': 'Brute Force', 'SSH-Patator': 'Brute Force','Web Attack - Brute Force': 'Web Attack', 'Web Attack - Sql Injection': 'Web Attack', 'Web Attack - XSS': 'Web Attack'})\n",
    "\n",
    "df_max_scaled.pop(' Label')\n",
    "\n",
    "\n",
    "df_max_scaled\n",
    "for col in df_max_scaled.columns:\n",
    "    t = abs(df_max_scaled[col].max())\n",
    "    df_max_scaled[col] = df_max_scaled[col]/t\n",
    "df_max_scaled\n",
    "df = df_max_scaled.assign( Label = y)\n",
    "#df\n",
    "df = df.fillna(0)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating features and labels')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# result_list = [counter['None'],counter['Denial of Service'], counter['Port Scanning']]\n",
    "# print('number of Labels  ',result_list)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating datasets')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "test2 = X.assign(ALERT = y)\n",
    "\n",
    "Dos_samples = test2[test2['ALERT'] == 'Dos/Ddos']\n",
    "Normal_samples = test2[test2['ALERT'] == 'BENIGN']\n",
    "PS_samples = test2[test2['ALERT'] == 'PortScan']\n",
    "\n",
    "Infiltration_samples = test2[test2['ALERT'] == 'PortScan']\n",
    "Bot_samples = test2[test2['ALERT'] == 'Bot']\n",
    "Web_samples = test2[test2['ALERT'] == 'Web Attack']\n",
    "Brute_samples = test2[test2['ALERT'] == 'Brute Force']\n",
    "\n",
    "\n",
    "PS_y = PS_samples.pop('ALERT')\n",
    "Dos_y = Dos_samples.pop('ALERT')\n",
    "Normal_y = Normal_samples.pop('ALERT')\n",
    "\n",
    "Infiltration_y = Infiltration_samples.pop('ALERT')\n",
    "Bot_y = Bot_samples.pop('ALERT')\n",
    "Web_y = Web_samples.pop('ALERT')\n",
    "Brute_y = Brute_samples.pop('ALERT')\n",
    "\n",
    "\n",
    "test2.pop('ALERT')\n",
    "\n",
    "# # Create an instance of RandomUnderSampler\n",
    "# rus = RandomUnderSampler()\n",
    "\n",
    "# # Balance the dataset using RandomUnderSampler\n",
    "# X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# # Create an instance of SMOTE\n",
    "# smote = SMOTE()\n",
    "\n",
    "# # Balance the dataset using SMOTE\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# X = X_resampled\n",
    "# y = y_resampled\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "y, label = pd.factorize(y)\n",
    "# y_test, label = pd.factorize(test['Label'])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate Training and Testing db\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating Training and Testing db')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.7,random_state=42)\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Now you can use Keras modules directly from tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Flatten\n",
    "import innvestigate\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining the DNN model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# dropout_rate = 0.01\n",
    "nodes = 7\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(len(X_train.columns,))))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(7))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training the model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Modify model.fit to include the EarlyStopping callback\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('ELAPSE TIME TRAINING MODEL: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Model Prediction')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time()\n",
    "print('ELAPSE TIME MODEL PREDICTION: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "#print(y_pred)\n",
    "ynew = np.argmax(y_pred,axis = 1)\n",
    "#print(ynew)\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "#print(score)\n",
    "pred_label = label[ynew]\n",
    "#print(score)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_test)\n",
    "print(label_counts)\n",
    "\n",
    "label_counts = Counter(ynew)\n",
    "print(label_counts)\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train\n",
    "test = X_test\n",
    "labels_train = y_train\n",
    "labels_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = list(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to test sample with the waterfall plot\n",
    "def waterfall_explanator(sample):\n",
    "\n",
    "    index = np.argmax(model.predict(sample)) # Prediction of the sample\n",
    "    prediction = index\n",
    "\n",
    "    analyzer = innvestigate.create_analyzer(\"lrp.z\", model)\n",
    "    analysis = analyzer.analyze(sample)\n",
    "    names = sample.columns\n",
    "    scores = pd.DataFrame(analysis)\n",
    "    scores_abs = scores.abs()\n",
    "\n",
    "    sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "    names = list(names)\n",
    "    sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "    sum_of_columns\n",
    "    # Zip the two lists together\n",
    "    combined = list(zip(names, sum_of_columns))\n",
    "    # Sort the combined list in descending order based on the values from sum_of_columns\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "    # Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "    sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "\n",
    "    shap_val = sorted_sum_of_columns\n",
    "    feature_name = sorted_names\n",
    "    # sorted_names\n",
    "    feature_val = []\n",
    "    for j in sorted_names:\n",
    "            feature_val.append(float(sample[j]))\n",
    "    return (prediction, shap_val,feature_val,feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_all(single_class_samples,number_samples, number_of_features_pertubation):\n",
    "    Bucket = {\n",
    "    '0.0': 0,\n",
    "    '0.1':0,\n",
    "    '0.2':0,\n",
    "    '0.3':0,\n",
    "    '0.4':0,\n",
    "    '0.5':0,\n",
    "    '0.6':0,\n",
    "    '0.7':0,\n",
    "    '0.8':0,\n",
    "    '0.9':0,\n",
    "    '1.0':0,\n",
    "\n",
    "           }\n",
    "    # Counter_chart = 0\n",
    "    Counter_all_samples = 0\n",
    "    counter_samples_changed_class = 0\n",
    "    print('------------------------------------------------')\n",
    "    print('Initiating Completeness Experiment')\n",
    "    print('------------------------------------------------')\n",
    "    for i in range(0,number_samples):\n",
    "        #select sample\n",
    "        try:\n",
    "            sample = single_class_samples[i:i+1]\n",
    "        except:\n",
    "            break # break if there more samples requested than samples in the dataset\n",
    "        # Explanate the original sample\n",
    "        u = waterfall_explanator(sample)\n",
    "        #select top 5 features from the original sample\n",
    "        top_k_features = []\n",
    "        top_k_features.append(u[3][0]) #append first feature\n",
    "        break_condition = False\n",
    "        for k in range(1,number_of_features_pertubation):\n",
    "            for j in range(11):  # 11 steps to include 1.0 (0 to 10)\n",
    "                if break_condition == True: break\n",
    "                perturbation = j / 10.0  # Divide by 10 to get steps of 0.1\n",
    "                temp_var = sample[top_k_features[k-1]]\n",
    "                result = np.where((temp_var - perturbation) < 0, True, False)\n",
    "                if result < 0: \n",
    "                    sample[top_k_features[k-1]] = 1 - perturbation\n",
    "                else: sample[top_k_features[k-1]] = temp_var - perturbation\n",
    "                # sample[top_k_features[k-1]] = perturbation\n",
    "                v = waterfall_explanator(sample)\n",
    "                if v[0] != u[0]: \n",
    "                    # print(str(perturbation))\n",
    "                    Bucket[str(perturbation)] += 1              \n",
    "                    break_condition = True\n",
    "                    counter_samples_changed_class += 1     \n",
    "                    # Bucket[str(perturbation)] = counter_samples_changed_class              \n",
    "                    break\n",
    "                else: sample[top_k_features[k-1]] = abs(temp_var - 1) # set the sample feature value as the symetric opposite\n",
    "            # print(u)\n",
    "            top_k_features.append(u[3][k]) #append second, third feature .. and so on\n",
    "            if break_condition == True: break\n",
    "        Counter_all_samples += 1\n",
    "        progress  = 100*Counter_all_samples/number_samples\n",
    "        if progress%10 == 0: print('Progress', progress ,'%')\n",
    "    # print('Number of Normal samples that changed classification: ',counter_samples_changed_class)\n",
    "    # print('Number of all samples analyzed: ',Counter_all_samples)\n",
    "    # for key in Bucket:\n",
    "    #     Bucket[key]=number_samples - Bucket[key]\n",
    "    # Bucket['0.0'] = number_samples\n",
    "    # for key in Bucket:\n",
    "    #     Bucket[key]=Bucket[key]\n",
    "    dict = Bucket\n",
    "    temp = 0\n",
    "    for k in dict:\n",
    "        dict[k] = dict[k] + temp\n",
    "        temp = dict[k]\n",
    "    total = number_samples\n",
    "    y_axis = []\n",
    "    for k in dict:\n",
    "        dict[k] = abs(dict[k] - total)\n",
    "        y_axis.append(dict[k]/total)    \n",
    "    return(counter_samples_changed_class,Counter_all_samples,y_axis)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_samples = 100\n",
    "K_feat = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = K_samples\n",
    "# num_samples = 3\n",
    "\n",
    "num_feat_pertubation = K_feat\n",
    "# num_feat_pertubation = 3\n",
    "\n",
    "output_file_name = 'DNN_LRP_CIC_Completeness.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Dos_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of DoS samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_dos = p[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(output_file_name, \"w\") as f:print('',file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_dos = [',y_axis_dos ,']',file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Normal_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Normal samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_normal = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_normal = [',y_axis_normal ,']',file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(0, 100, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
      "Number of PS samples that changed classification:  0\n",
      "Number of all samples analyzed:  100\n",
      "0.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = completeness_all(PS_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of PS samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_ps = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_ps = [',y_axis_ps ,']',file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Infiltration_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Infiltration samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_infiltration = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_infiltration = [',y_axis_infiltration ,']',file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(47, 100, [1.0, 0.54, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53])\n",
      "Number of Bot samples that changed classification:  47\n",
      "Number of all samples analyzed:  100\n",
      "47.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = completeness_all(Bot_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Bot samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_bot = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_bot = [',y_axis_bot ,']',file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n",
      "Progress 100.0 %\n",
      "(2, 100, [1.0, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.98, 0.98])\n",
      "Number of Web samples that changed classification:  2\n",
      "Number of all samples analyzed:  100\n",
      "2.0 % - samples are complete \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Web_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Web samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_web = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_web = [',y_axis_web ,']',file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Initiating Completeness Experiment\n",
      "------------------------------------------------\n",
      "Progress 10.0 %\n",
      "Progress 20.0 %\n",
      "Progress 30.0 %\n",
      "Progress 40.0 %\n",
      "Progress 50.0 %\n",
      "Progress 60.0 %\n",
      "Progress 70.0 %\n",
      "Progress 80.0 %\n",
      "Progress 90.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "p = completeness_all(Brute_samples,num_samples,num_feat_pertubation)\n",
    "print(p)\n",
    "print('Number of Brute samples that changed classification: ',p[0])\n",
    "print('Number of all samples analyzed: ',p[1])\n",
    "percentage = 100*p[0]/p[1]\n",
    "print(percentage,'%','- samples are complete ')\n",
    "y_axis_brute = p[2]\n",
    "with open(output_file_name, \"a\") as f:print('y_axis_brute = [',y_axis_brute ,']',file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "y_axis_dos = [ [1.0, 0.78, 0.67, 0.65, 0.58, 0.54, 0.46, 0.46, 0.14, 0.11, 0.03] ]\n",
    "y_axis_normal = [ [1.0, 0.61, 0.6, 0.6, 0.58, 0.58, 0.5, 0.48, 0.24, 0.2, 0.19] ]\n",
    "y_axis_ps = [ [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] ]\n",
    "y_axis_infiltration = [ [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] ]\n",
    "\n",
    "y_axis_web = [ [1.0, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.98, 0.98] ]\n",
    "y_axis_brute = [ [1.0, 0.7, 0.7, 0.7, 0.7, 0.7, 0.37, 0.37, 0.37, 0.37, 0.19] ]\n",
    "y_axis_bot = [ [1.0, 0.54, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53] ]\n",
    "\n",
    "\n",
    "# Plot the first line\n",
    "plt.plot(x_axis, y_axis_dos, label='DoS', color='blue', linestyle='--', marker='o')\n",
    "\n",
    "# # Plot the second line\n",
    "# plt.plot(x_axis, y_axis_normal, label='Normal', color='red', linestyle='--', marker='x')\n",
    "\n",
    "# # Plot the third line\n",
    "# plt.plot(x_axis, y_axis_ps, label='Port Scan', color='green', linestyle='--', marker='s')\n",
    "\n",
    "# # Plot the fourth line\n",
    "# plt.plot(x_axis, y_axis_infiltration, label='Infiltration', color='purple', linestyle='--', marker='p')\n",
    "\n",
    "# # Plot the fifth line\n",
    "# plt.plot(x_axis, y_axis_bot, label='Bot', color='orange', linestyle='--', marker='h')\n",
    "\n",
    "# # Plot the sixth line\n",
    "# plt.plot(x_axis, y_axis_web, label='Web Attack', color='magenta', linestyle='--', marker='+')\n",
    "\n",
    "# # Plot the seventh line\n",
    "# plt.plot(x_axis, y_axis_brute, label='Brute Force', color='cyan', linestyle='--', marker='_')\n",
    "\n",
    "# Enable grid lines (both major and minor grids)\n",
    "plt.grid()\n",
    "\n",
    "# Customize grid lines (optional)\n",
    "# plt.grid()\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Perturbations')\n",
    "plt.ylabel('Samples remaining')\n",
    "plt.legend()\n",
    "\n",
    "# Set the title of the plot\n",
    "# plt.title('Accuracy x Features - SHAP SML')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.savefig('GRAPH_PERT_SHAP_CIC.png')\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
