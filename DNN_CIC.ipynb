{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Regression\n",
    "It is now time to implements single-input and multiple-inputs DNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Initializing DNN program\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Importing Libraries\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining Metric Equations\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Defining features of interest\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Loading Databases\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Normalizing database\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Separating features and labels\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Counter({'BENIGN': 756652, 'Dos/Ddos': 127047, 'PortScan': 52987, 'Brute Force': 4594, 'Web Attack': 716, 'Bot': 628, 'Infiltration': 13})\n",
      "---------------------------------------------------------------------------------\n",
      "Counter({'BENIGN': 756652, 'Dos/Ddos': 127047, 'PortScan': 52987, 'Brute Force': 4594, 'Web Attack': 716, 'Bot': 628, 'Infiltration': 13})\n",
      "---------------------------------------------------------------------------------\n",
      "Separating Training and Testing db\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Initializing DNN program')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "#---------------------------------------------------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Importing Libraries')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import LSTM\n",
    "#from keras.layers import Dropout\n",
    "#from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.preprocessing import sequence\n",
    "#from keras.utils import pad_sequences\n",
    "#from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "import shap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "np.random.seed(0)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Defining metric equations\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Metric Equations')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "def ACC(TP,TN,FP,FN):\n",
    "    Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return Acc\n",
    "def ACC_2 (TP, FN):\n",
    "    ac = (TP/(TP+FN))\n",
    "    return ac\n",
    "def PRECISION(TP,FP):\n",
    "    Precision = TP/(TP+FP)\n",
    "    return Precision\n",
    "def RECALL(TP,FN):\n",
    "    Recall = TP/(TP+FN)\n",
    "    return Recall\n",
    "def F1(Recall, Precision):\n",
    "    F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "    return F1\n",
    "def BACC(TP,TN,FP,FN):\n",
    "    BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "    return BACC\n",
    "def MCC(TP,TN,FP,FN):\n",
    "    MCC = (TN*TP-FN*FP)/(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**.5)\n",
    "    return MCC\n",
    "def AUC_ROC(y_test_bin,y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    auc_avg = 0\n",
    "    counting = 0\n",
    "    for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "     # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "      #print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "      auc_avg += auc(fpr[i], tpr[i])\n",
    "      counting = i+1\n",
    "    return auc_avg/counting\n",
    "#---------------------------------------------------------------------\n",
    "# Defining features of interest\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining features of interest')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# req_cols = [ ' Packet Length Std', ' Total Length of Bwd Packets', ' Subflow Bwd Bytes',\n",
    "# ' Destination Port', ' Packet Length Variance', ' Bwd Packet Length Mean',' Avg Bwd Segment Size',\n",
    "# 'Bwd Packet Length Max', ' Init_Win_bytes_backward','Total Length of Fwd Packets',\n",
    "# ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' Average Packet Size', ' Packet Length Mean',\n",
    "# ' Max Packet Length',' Label']\n",
    "\n",
    "# req_cols = [' Destination Port',' Flow Duration',' Total Fwd Packets',' Total Backward Packets','Total Length of Fwd Packets',' Total Length of Bwd Packets',' Fwd Packet Length Max',' Fwd Packet Length Min',' Fwd Packet Length Mean',' Fwd Packet Length Std','Bwd Packet Length Max',' Bwd Packet Length Min',' Bwd Packet Length Mean',' Bwd Packet Length Std','Flow Bytes/s',' Flow Packets/s',' Flow IAT Mean',' Flow IAT Std',' Flow IAT Max',' Flow IAT Min','Fwd IAT Total',' Fwd IAT Mean',' Fwd IAT Std',' Fwd IAT Max',' Fwd IAT Min','Bwd IAT Total',' Bwd IAT Mean',' Bwd IAT Std',' Bwd IAT Max',' Bwd IAT Min','Fwd PSH Flags',' Bwd PSH Flags',' Fwd URG Flags',' Bwd URG Flags',' Fwd Header Length',' Bwd Header Length','Fwd Packets/s',' Bwd Packets/s',' Min Packet Length',' Max Packet Length',' Packet Length Mean',' Packet Length Std',' Packet Length Variance','FIN Flag Count',' SYN Flag Count',' RST Flag Count',' PSH Flag Count',' ACK Flag Count',' URG Flag Count',' CWE Flag Count',' ECE Flag Count',' Down/Up Ratio',' Average Packet Size',' Avg Fwd Segment Size',' Avg Bwd Segment Size',' Fwd Header Length','Fwd Avg Bytes/Bulk',' Fwd Avg Packets/Bulk',' Fwd Avg Bulk Rate',' Bwd Avg Bytes/Bulk',' Bwd Avg Packets/Bulk','Bwd Avg Bulk Rate','Subflow Fwd Packets',' Subflow Fwd Bytes',' Subflow Bwd Packets',' Subflow Bwd Bytes','Init_Win_bytes_forward',' Init_Win_bytes_backward',' act_data_pkt_fwd',' min_seg_size_forward','Active Mean',' Active Std',' Active Max',' Active Min','Idle Mean',' Idle Std',' Idle Max',' Idle Min',' Label']\n",
    "\n",
    "# req_cols = [' Down/Up Ratio', ' Fwd URG Flags', ' Flow IAT Std', 'Subflow Fwd Packets', ' Flow Packets/s', ' URG Flag Count', 'FIN Flag Count', ' Bwd Packets/s', 'Bwd Avg Bulk Rate', ' act_data_pkt_fwd', ' Fwd Packet Length Std', ' Bwd Avg Bytes/Bulk', ' Active Max', ' Flow IAT Max', ' min_seg_size_forward', ' Bwd Packet Length Std', ' Fwd IAT Std', ' Fwd Avg Bulk Rate', ' Fwd Packet Length Mean', ' Fwd Packet Length Max', ' Idle Std', ' CWE Flag Count', 'Fwd IAT Total', ' ACK Flag Count', ' Bwd URG Flags', ' Flow IAT Min', ' Flow IAT Mean', ' Total Backward Packets', ' Fwd Avg Packets/Bulk', 'Fwd Avg Bytes/Bulk', ' SYN Flag Count', ' Min Packet Length', ' Fwd Packet Length Min', 'Idle Mean', 'Fwd PSH Flags', ' Fwd IAT Min', ' Fwd Header Length', ' RST Flag Count', ' Idle Max', ' PSH Flag Count', ' Bwd Header Length', ' ECE Flag Count', ' Subflow Bwd Packets', 'Active Mean', 'Flow Bytes/s', ' Bwd IAT Mean', ' Avg Fwd Segment Size', ' Bwd Packet Length Min', ' Active Std', ' Bwd IAT Min', ' Flow Duration', 'Fwd Packets/s', ' Fwd IAT Max', 'Bwd IAT Total', ' Idle Min', ' Bwd PSH Flags', ' Bwd Avg Packets/Bulk', ' Total Fwd Packets', ' Active Min', ' Bwd IAT Std', ' Fwd IAT Mean', ' Bwd IAT Max', ' Label']\n",
    "\n",
    "req_cols = [\n",
    "    \n",
    "    ' Packet Length Std', ' Total Length of Bwd Packets', ' Subflow Bwd Bytes',\n",
    "    ' Destination Port', ' Packet Length Variance', ' Bwd Packet Length Mean',' Avg Bwd Segment Size',\n",
    "    'Bwd Packet Length Max', ' Init_Win_bytes_backward','Total Length of Fwd Packets',\n",
    "    ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' Average Packet Size', ' Packet Length Mean',\n",
    "    ' Max Packet Length',\n",
    "    ' Down/Up Ratio', ' Fwd URG Flags', ' Flow IAT Std', 'Subflow Fwd Packets', ' Flow Packets/s', ' URG Flag Count', 'FIN Flag Count', ' Bwd Packets/s', 'Bwd Avg Bulk Rate'\n",
    "    , ' act_data_pkt_fwd', ' Fwd Packet Length Std', ' Bwd Avg Bytes/Bulk', ' Active Max', ' Flow IAT Max', ' min_seg_size_forward', ' Bwd Packet Length Std', ' Fwd IAT Std', ' Fwd Avg Bulk Rate', ' Fwd Packet Length Mean', ' Fwd Packet Length Max', ' Idle Std', ' CWE Flag Count', 'Fwd IAT Total'\n",
    "    \n",
    "    , ' ACK Flag Count', ' Bwd URG Flags', ' Flow IAT Min', ' Flow IAT Mean', ' Total Backward Packets', ' Fwd Avg Packets/Bulk', 'Fwd Avg Bytes/Bulk', ' SYN Flag Count', ' Min Packet Length', ' Fwd Packet Length Min', 'Idle Mean', 'Fwd PSH Flags', ' Fwd IAT Min'\n",
    "     \n",
    "    ,  ' Fwd Header Length', ' RST Flag Count', ' Idle Max', ' PSH Flag Count', ' Bwd Header Length', ' ECE Flag Count', ' Subflow Bwd Packets', 'Active Mean', 'Flow Bytes/s', ' Bwd IAT Mean', ' Avg Fwd Segment Size', ' Bwd Packet Length Min', ' Active Std', ' Bwd IAT Min', ' Flow Duration', 'Fwd Packets/s', ' Fwd IAT Max', 'Bwd IAT Total', ' Idle Min', ' Bwd PSH Flags', ' Bwd Avg Packets/Bulk', ' Total Fwd Packets', ' Active Min', ' Bwd IAT Std', ' Fwd IAT Mean', ' Bwd IAT Max'\n",
    "            \n",
    "            , ' Label']\n",
    "\n",
    "# Information gain top 10 features\n",
    "top10 = [' Average Packet Size',\n",
    "          ' Packet Length Std', \n",
    "          ' Packet Length Variance', \n",
    "          ' Packet Length Mean',\n",
    "            ' Destination Port', \n",
    "            ' Subflow Bwd Bytes', \n",
    "            ' Total Length of Bwd Packets', \n",
    "            ' Avg Bwd Segment Size', \n",
    "            ' Bwd Packet Length Mean',  \n",
    "            'Bwd Packet Length Max', \n",
    "            ' Label']\n",
    "\n",
    "\n",
    "\n",
    "# req_cols = top10\n",
    "\n",
    "# req_cols = [' Destination Port',' Flow Duration',' Total Fwd Packets',' Total Backward Packets','Total Length of Fwd Packets',' Total Length of Bwd Packets',' Label']\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "\n",
    "path_str = '/home/oarreche@ads.iu.edu/HITL/cicids/cicids_db/'\n",
    "fraction = 1\n",
    "#---------------------------------------------------------------------\n",
    "#Load Databases from csv file\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Loading Databases')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "\n",
    "df0 = pd.read_csv (path_str + 'Wednesday-workingHours.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "df1 = pd.read_csv (path_str + 'Tuesday-WorkingHours.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df2 = pd.read_csv (path_str +'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df3 = pd.read_csv (path_str +'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df4 = pd.read_csv (path_str +'Monday-WorkingHours.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df5 = pd.read_csv (path_str +'Friday-WorkingHours-Morning.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df6 = pd.read_csv (path_str +'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "\n",
    "df7 = pd.read_csv (path_str +'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', usecols=req_cols).sample(frac = fraction)\n",
    "\n",
    "frames = [df0, df1, df2, df3, df4, df5, df6, df7]\n",
    "\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "df = df.sample(frac = 0.333)\n",
    "\n",
    "\n",
    "\n",
    "# IG popping unwandted columns descriptive accuracy\n",
    "\n",
    "# df.pop(' min_seg_size_forward')\n",
    "# df.pop(' PSH Flag Count')\n",
    "# df.pop(' ACK Flag Count')\n",
    "# df.pop('Init_Win_bytes_forward')\n",
    "# df.pop(' Destination Port')\n",
    "\n",
    "# df.pop(' Bwd Packet Length Mean')\n",
    "# df.pop('Fwd Packets/s')\n",
    "# df.pop(' Init_Win_bytes_backward')\n",
    "# df.pop(' Packet Length Mean')\n",
    "# df.pop(' Packet Length Std')\n",
    "\n",
    "# df.pop(' Bwd Packet Length Std')\n",
    "# df.pop(' Avg Bwd Segment Size')\n",
    "# df.pop(' Packet Length Variance')\n",
    "# df.pop(' Flow Duration')\n",
    "# df.pop(' Max Packet Length')\n",
    "# df.pop(' Flow IAT Max')\n",
    "# df.pop(' Idle Max')\n",
    "# df.pop('Bwd Packet Length Max')\n",
    "# df.pop(' Fwd IAT Max')\n",
    "# df.pop(' URG Flag Count')\n",
    "# df.pop('Fwd PSH Flags')\n",
    "\n",
    "# df.pop(' Min Packet Length')\n",
    "# df.pop('Idle Mean')\n",
    "# df.pop(' Fwd Packet Length Min')\n",
    "# df.pop(' Average Packet Size')\n",
    "# df.pop(' Idle Min')\n",
    "# df.pop('Bwd IAT Total')\n",
    "# df.pop('Fwd IAT Total')\n",
    "# df.pop(' Fwd Packet Length Max')\n",
    "# df.pop(' Avg Fwd Segment Size')\n",
    "# df.pop(' SYN Flag Count')\n",
    "# df.pop(' Fwd Packet Length Mean')\n",
    "# df.pop(' Flow IAT Std')\n",
    "# df.pop(' Down/Up Ratio')\n",
    "# df.pop(' Fwd IAT Mean')\n",
    "# df.pop('FIN Flag Count')\n",
    "# df.pop(' Bwd Packets/s')\n",
    "# df.pop(' Fwd Packet Length Std')\n",
    "# df.pop(' Flow IAT Mean')\n",
    "# df.pop(' Bwd Packet Length Min')\n",
    "# df.pop(' Fwd IAT Std')\n",
    "# df.pop(' Bwd IAT Mean')\n",
    "\n",
    "# df.pop(' Bwd IAT Std')\n",
    "# df.pop(' Bwd IAT Max')\n",
    "# df.pop(' Bwd IAT Min')\n",
    "# df.pop(' Fwd IAT Min')\n",
    "# df.pop(' Idle Std')\n",
    "# df.pop(' Active Max')\n",
    "# df.pop(' RST Flag Count')\n",
    "# df.pop(' Subflow Fwd Bytes')\n",
    "# df.pop(' ECE Flag Count')\n",
    "# df.pop(' Flow IAT Min')\n",
    "# df.pop('Total Length of Fwd Packets')\n",
    "# df.pop(' Active Std')\n",
    "# df.pop(' Active Min')\n",
    "# df.pop(' Bwd Header Length')\n",
    "# df.pop(' Fwd Header Length')\n",
    "# df.pop('Active Mean')\n",
    "# df.pop(' Total Fwd Packets')\n",
    "# df.pop(' Total Backward Packets')\n",
    "# df.pop('Subflow Fwd Packets')\n",
    "# df.pop(' act_data_pkt_fwd')\n",
    "# df.pop(' Subflow Bwd Bytes')\n",
    "# df.pop(' Fwd URG Flags')\n",
    "# df.pop(' CWE Flag Count')\n",
    "# df.pop(' Subflow Bwd Packets')\n",
    "# df.pop(' Total Length of Bwd Packets')\n",
    "# df.pop('Flow Bytes/s')\n",
    "# df.pop(' Flow Packets/s')\n",
    "# df.pop(' Bwd PSH Flags')\n",
    "# df.pop(' Bwd URG Flags')\n",
    "# df.pop('Fwd Avg Bytes/Bulk')\n",
    "# df.pop(' Fwd Avg Packets/Bulk')\n",
    "# df.pop(' Fwd Avg Bulk Rate')\n",
    "# df.pop(' Bwd Avg Bytes/Bulk')\n",
    "# df.pop(' Bwd Avg Packets/Bulk')\n",
    "# df.pop('Bwd Avg Bulk Rate')\n",
    "\n",
    "# LRP popping unwandted columns descriptive accuracy\n",
    "\n",
    "\n",
    "# df.pop(' min_seg_size_forward')\n",
    "# df.pop(' PSH Flag Count')\n",
    "# df.pop(' ACK Flag Count')\n",
    "# df.pop('Init_Win_bytes_forward')\n",
    "# df.pop(' Destination Port')\n",
    "\n",
    "# df.pop(' Bwd Packet Length Mean')\n",
    "# df.pop('Fwd Packets/s')\n",
    "# df.pop(' Init_Win_bytes_backward')\n",
    "# df.pop(' Packet Length Mean')\n",
    "# df.pop(' Packet Length Std')\n",
    "\n",
    "# df.pop(' Bwd Packet Length Std')\n",
    "# df.pop(' Avg Bwd Segment Size')\n",
    "# df.pop(' Packet Length Variance')\n",
    "# df.pop(' Flow Duration')\n",
    "# df.pop(' Max Packet Length')\n",
    "# df.pop(' Flow IAT Max')\n",
    "# df.pop(' Idle Max')\n",
    "# df.pop('Bwd Packet Length Max')\n",
    "# df.pop(' Fwd IAT Max')\n",
    "# df.pop(' URG Flag Count')\n",
    "# df.pop('Fwd PSH Flags')\n",
    "\n",
    "# df.pop(' Min Packet Length')\n",
    "# df.pop('Idle Mean')\n",
    "# df.pop(' Fwd Packet Length Min')\n",
    "# df.pop(' Average Packet Size')\n",
    "# df.pop(' Idle Min')\n",
    "# df.pop('Bwd IAT Total')\n",
    "# df.pop('Fwd IAT Total')\n",
    "# df.pop(' Fwd Packet Length Max')\n",
    "# df.pop(' Avg Fwd Segment Size')\n",
    "# df.pop(' SYN Flag Count')\n",
    "# df.pop(' Fwd Packet Length Mean')\n",
    "# df.pop(' Flow IAT Std')\n",
    "# df.pop(' Down/Up Ratio')\n",
    "# df.pop(' Fwd IAT Mean')\n",
    "# df.pop('FIN Flag Count')\n",
    "# df.pop(' Bwd Packets/s')\n",
    "# df.pop(' Fwd Packet Length Std')\n",
    "# df.pop(' Flow IAT Mean')\n",
    "# df.pop(' Bwd Packet Length Min')\n",
    "# df.pop(' Fwd IAT Std')\n",
    "# df.pop(' Bwd IAT Mean')\n",
    "# df.pop(' Bwd IAT Std')\n",
    "\n",
    "# df.pop(' Bwd IAT Max')\n",
    "# df.pop(' Bwd IAT Min')\n",
    "# df.pop(' Fwd IAT Min')\n",
    "# df.pop(' Idle Std')\n",
    "# df.pop(' Active Max')\n",
    "# df.pop(' RST Flag Count')\n",
    "# df.pop(' Subflow Fwd Bytes')\n",
    "# df.pop(' ECE Flag Count')\n",
    "# df.pop(' Flow IAT Min')\n",
    "# df.pop('Total Length of Fwd Packets')\n",
    "# df.pop(' Active Std')\n",
    "# df.pop(' Active Min')\n",
    "# df.pop(' Bwd Header Length')\n",
    "# df.pop(' Fwd Header Length')\n",
    "# df.pop('Active Mean')\n",
    "# df.pop(' Total Fwd Packets')\n",
    "# df.pop(' Total Backward Packets')\n",
    "# df.pop('Subflow Fwd Packets')\n",
    "# df.pop(' act_data_pkt_fwd')\n",
    "# df.pop(' Subflow Bwd Bytes')\n",
    "# df.pop(' Fwd URG Flags')\n",
    "# df.pop(' CWE Flag Count')\n",
    "# df.pop(' Subflow Bwd Packets')\n",
    "# df.pop(' Total Length of Bwd Packets')\n",
    "# df.pop('Flow Bytes/s')\n",
    "# df.pop(' Flow Packets/s')\n",
    "# df.pop(' Bwd PSH Flags')\n",
    "# df.pop(' Bwd URG Flags')\n",
    "# df.pop('Fwd Avg Bytes/Bulk')\n",
    "# df.pop(' Fwd Avg Packets/Bulk')\n",
    "# df.pop(' Fwd Avg Bulk Rate')\n",
    "# df.pop(' Bwd Avg Bytes/Bulk')\n",
    "# df.pop(' Bwd Avg Packets/Bulk')\n",
    "# df.pop('Bwd Avg Bulk Rate')\n",
    "\n",
    "\n",
    "# DeepLift popping unwandted columns descriptive accuracy\n",
    "\n",
    "# df.pop(' PSH Flag Count')\n",
    "# df.pop(' ACK Flag Count')\n",
    "# df.pop(' Bwd Packet Length Mean')\n",
    "# df.pop('Init_Win_bytes_forward')\n",
    "# df.pop(' Bwd Packet Length Std')\n",
    "\n",
    "# df.pop('Fwd Packets/s')\n",
    "# df.pop(' Packet Length Std')\n",
    "# df.pop(' Packet Length Variance')\n",
    "# df.pop(' Destination Port')\n",
    "# df.pop(' Avg Bwd Segment Size')\n",
    "\n",
    "# df.pop(' Packet Length Mean')\n",
    "# df.pop(' URG Flag Count')\n",
    "# df.pop(' Idle Max')\n",
    "# df.pop(' Flow IAT Max')\n",
    "# df.pop('Fwd PSH Flags')\n",
    "# df.pop(' Flow Duration')\n",
    "# df.pop(' Max Packet Length')\n",
    "# df.pop('Bwd Packet Length Max')\n",
    "# df.pop(' Fwd IAT Max')\n",
    "# df.pop('FIN Flag Count')\n",
    "# df.pop(' min_seg_size_forward')\n",
    "\n",
    "# df.pop('Idle Mean')\n",
    "# df.pop(' Idle Min')\n",
    "# df.pop(' Init_Win_bytes_backward')\n",
    "# df.pop('Bwd IAT Total')\n",
    "# df.pop('Fwd IAT Total')\n",
    "# df.pop(' Average Packet Size')\n",
    "# df.pop(' Flow IAT Std')\n",
    "# df.pop(' Min Packet Length')\n",
    "# df.pop(' SYN Flag Count')\n",
    "# df.pop(' Fwd Packet Length Min')\n",
    "# df.pop(' Fwd IAT Mean')\n",
    "# df.pop(' Bwd Packet Length Min')\n",
    "# df.pop(' Fwd IAT Std')\n",
    "# df.pop(' Fwd Packet Length Max')\n",
    "# df.pop(' Fwd Packet Length Mean')\n",
    "# df.pop(' Fwd Packet Length Std')\n",
    "# df.pop(' Flow IAT Mean')\n",
    "# df.pop(' Avg Fwd Segment Size')\n",
    "# df.pop(' Bwd IAT Max')\n",
    "# df.pop(' Bwd IAT Mean')\n",
    "# df.pop(' Down/Up Ratio')\n",
    "\n",
    "# df.pop(' Bwd IAT Min')\n",
    "# df.pop(' Fwd IAT Min')\n",
    "# df.pop(' Bwd IAT Std')\n",
    "# df.pop(' Bwd Packets/s')\n",
    "# df.pop(' Idle Std')\n",
    "# df.pop(' Active Max')\n",
    "# df.pop(' Flow IAT Min')\n",
    "# df.pop(' Subflow Fwd Bytes')\n",
    "# df.pop('Total Length of Fwd Packets')\n",
    "# df.pop(' Active Std')\n",
    "# df.pop(' RST Flag Count')\n",
    "# df.pop(' Active Min')\n",
    "# df.pop('Active Mean')\n",
    "# df.pop(' ECE Flag Count')\n",
    "# df.pop(' CWE Flag Count')\n",
    "# df.pop(' Fwd URG Flags')\n",
    "# df.pop(' Total Backward Packets')\n",
    "# df.pop(' act_data_pkt_fwd')\n",
    "# df.pop(' Total Fwd Packets')\n",
    "# df.pop(' Subflow Bwd Bytes')\n",
    "# df.pop('Subflow Fwd Packets')\n",
    "# df.pop(' Fwd Header Length')\n",
    "# df.pop(' Bwd Header Length')\n",
    "# df.pop(' Subflow Bwd Packets')\n",
    "# df.pop(' Total Length of Bwd Packets')\n",
    "# df.pop('Bwd Avg Bulk Rate')\n",
    "# df.pop(' Bwd Avg Packets/Bulk')\n",
    "# df.pop(' Bwd Avg Bytes/Bulk')\n",
    "# df.pop(' Fwd Avg Bulk Rate')\n",
    "# df.pop(' Fwd Avg Packets/Bulk')\n",
    "# df.pop(' Bwd PSH Flags')\n",
    "# df.pop('Flow Bytes/s')\n",
    "# df.pop(' Flow Packets/s')\n",
    "# df.pop(' Bwd URG Flags')\n",
    "# df.pop('Fwd Avg Bytes/Bulk')\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Normalize database\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Normalizing database')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "df_max_scaled = df.copy()\n",
    "\n",
    "y = df_max_scaled[' Label'].replace({'DDoS' :'Dos/Ddos' ,'DoS GoldenEye': 'Dos/Ddos', 'DoS Hulk': 'Dos/Ddos', 'DoS Slowhttptest': 'Dos/Ddos', 'DoS slowloris': 'Dos/Ddos', 'Heartbleed': 'Dos/Ddos','FTP-Patator': 'Brute Force', 'SSH-Patator': 'Brute Force','Web Attack - Brute Force': 'Web Attack', 'Web Attack - Sql Injection': 'Web Attack', 'Web Attack - XSS': 'Web Attack'})\n",
    "\n",
    "df_max_scaled.pop(' Label')\n",
    "\n",
    "\n",
    "df_max_scaled\n",
    "for col in df_max_scaled.columns:\n",
    "    t = abs(df_max_scaled[col].max())\n",
    "    df_max_scaled[col] = df_max_scaled[col]/t\n",
    "df_max_scaled\n",
    "df = df_max_scaled.assign( Label = y)\n",
    "#df\n",
    "df = df.fillna(0)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate features and labels \n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating features and labels')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# result_list = [counter['None'],counter['Denial of Service'], counter['Port Scanning']]\n",
    "# print('number of Labels  ',result_list)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# # Create an instance of RandomUnderSampler\n",
    "# rus = RandomUnderSampler()\n",
    "\n",
    "# # Balance the dataset using RandomUnderSampler\n",
    "# X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# # Create an instance of SMOTE\n",
    "# smote = SMOTE()\n",
    "\n",
    "# # Balance the dataset using SMOTE\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# X = X_resampled\n",
    "# y = y_resampled\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "y, label = pd.factorize(y)\n",
    "# y_test, label = pd.factorize(test['Label'])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Separate Training and Testing db\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Separating Training and Testing db')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.7,random_state=42)\n",
    "df = X.assign( Label = y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Now you can use Keras modules directly from tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Flatten\n",
    "import innvestigate\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining the DNN model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining the DNN model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "# dropout_rate = 0.01\n",
    "nodes = 7\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(len(X_train.columns,))))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(nodes, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(7))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# # Define your DNN model\n",
    "# model = Sequential([\n",
    "#     Flatten(input_shape=(28, 28)),  # Input layer\n",
    "#     Dense(128, activation='relu'),  # Hidden layer\n",
    "#     Dense(10, activation='relu')  # Output layer\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save('your_model.h5')\n",
    "\n",
    "# # Load your trained model\n",
    "# model = tf.keras.models.load_model('your_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training the model\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Train on 659845 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "659845/659845 [==============================] - 0s 1us/sample - loss: 5.5837 - accuracy: 0.0451\n",
      "Epoch 2/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.5090 - accuracy: 0.7833\n",
      "Epoch 3/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.4712 - accuracy: 0.7861\n",
      "Epoch 4/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.4868 - accuracy: 0.7889\n",
      "Epoch 5/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.4805 - accuracy: 0.7943\n",
      "Epoch 6/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.4553 - accuracy: 0.7970\n",
      "Epoch 7/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.4287 - accuracy: 0.7991\n",
      "Epoch 8/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.4029 - accuracy: 0.8011\n",
      "Epoch 9/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.3878 - accuracy: 0.8020\n",
      "Epoch 10/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.3625 - accuracy: 0.8020\n",
      "Epoch 11/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.3256 - accuracy: 0.8020\n",
      "Epoch 12/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.2753 - accuracy: 0.8020\n",
      "Epoch 13/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 2.1836 - accuracy: 0.8020\n",
      "Epoch 14/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 1.9547 - accuracy: 0.8019\n",
      "Epoch 15/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 1.7913 - accuracy: 0.8018\n",
      "Epoch 16/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 1.8916 - accuracy: 0.8011\n",
      "Epoch 17/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 1.8084 - accuracy: 0.8018\n",
      "Epoch 18/1000\n",
      "659845/659845 [==============================] - 0s 0us/sample - loss: 1.8221 - accuracy: 0.8018\n",
      "Epoch 19/1000\n",
      "659845/659845 [==============================] - 0s 1us/sample - loss: 1.7962 - accuracy: 0.8017\n",
      "---------------------------------------------------------------------------------\n",
      "ELAPSE TIME TRAINING MODEL:  0.08219966093699137 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Define your learning rate schedule function\n",
    "# def lr_schedule(epoch):\n",
    "#     # Your learning rate schedule logic here\n",
    "#     learning_rate = 0.1\n",
    "#     if epoch > 10:\n",
    "#         learning_rate = 0.01\n",
    "#     if epoch > 20:\n",
    "#         learning_rate = 0.001\n",
    "#     return learning_rate\n",
    "# lr_sched = LearningRateScheduler(lambda epoch: 1e-3 * (0.75 ** np.floor(epoch / 2)))\n",
    "\n",
    "# # Create a LearningRateScheduler callback\n",
    "# lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training the model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Modify model.fit to include the EarlyStopping callback\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train), callbacks=[early_stopping])\n",
    "\n",
    "# Pass the lr_scheduler callback to your model.fit() function\n",
    "# model.fit(X_train, y_train, callbacks=[lr_scheduler], ...)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('ELAPSE TIME TRAINING MODEL: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Model Prediction\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSE TIME MODEL PREDICTION:  0.1520874857902527 min\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "80.1649268720473\n",
      "Counter({0: 226902, 2: 38054, 1: 16073, 3: 1351, 5: 205, 4: 202, 6: 5})\n",
      "Counter({0: 281845, 2: 570, 3: 377})\n",
      "80.1649268720473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Model Prediction')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time()\n",
    "print('ELAPSE TIME MODEL PREDICTION: ',(end - start)/60, 'min')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('')\n",
    "\n",
    "#print(y_pred)\n",
    "ynew = np.argmax(y_pred,axis = 1)\n",
    "#print(ynew)\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "#print(score)\n",
    "pred_label = label[ynew]\n",
    "#print(score)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# pd.crosstab(test['ALERT'], preds, rownames=['Actual ALERT'], colnames = ['Predicted ALERT'])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_test)\n",
    "print(label_counts)\n",
    "\n",
    "label_counts = Counter(ynew)\n",
    "print(label_counts)\n",
    "\n",
    "accuracy =accuracy_score(y_test, ynew)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [-0.     0.003  0.    ... -0.     0.004  0.003]\n",
      " ...\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [ 0.027 -0.     0.    ... -0.    -0.     0.   ]\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an analyzer for the model\n",
    "analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model)\n",
    "\n",
    "# Replace X_test with your input data\n",
    "\n",
    "# X_test = np.random.rand(100, 28, 28)  # Example input data\n",
    "\n",
    "# X_test2 = X_test.frac(0.001, random_state=42)\n",
    "\n",
    "# Perform LRP analysis on the input data\n",
    "analysis = analyzer.analyze(X_test)\n",
    "\n",
    "# Print or use the analysis results as needed\n",
    "print(analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282792\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "print(len(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(282792, 77)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(analysis.shape)\n",
    "print(type(analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [-0.     0.003  0.    ... -0.     0.004  0.003]\n",
      " ...\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [ 0.027 -0.     0.    ... -0.    -0.     0.   ]\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_abs = scores.abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of each column\n",
    "sum_of_columns = scores_abs.sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Destination Port',\n",
       " ' Flow Duration',\n",
       " ' Total Fwd Packets',\n",
       " ' Total Backward Packets',\n",
       " 'Total Length of Fwd Packets',\n",
       " ' Total Length of Bwd Packets',\n",
       " ' Fwd Packet Length Max',\n",
       " ' Fwd Packet Length Min',\n",
       " ' Fwd Packet Length Mean',\n",
       " ' Fwd Packet Length Std',\n",
       " 'Bwd Packet Length Max',\n",
       " ' Bwd Packet Length Min',\n",
       " ' Bwd Packet Length Mean',\n",
       " ' Bwd Packet Length Std',\n",
       " 'Flow Bytes/s',\n",
       " ' Flow Packets/s',\n",
       " ' Flow IAT Mean',\n",
       " ' Flow IAT Std',\n",
       " ' Flow IAT Max',\n",
       " ' Flow IAT Min',\n",
       " 'Fwd IAT Total',\n",
       " ' Fwd IAT Mean',\n",
       " ' Fwd IAT Std',\n",
       " ' Fwd IAT Max',\n",
       " ' Fwd IAT Min',\n",
       " 'Bwd IAT Total',\n",
       " ' Bwd IAT Mean',\n",
       " ' Bwd IAT Std',\n",
       " ' Bwd IAT Max',\n",
       " ' Bwd IAT Min',\n",
       " 'Fwd PSH Flags',\n",
       " ' Bwd PSH Flags',\n",
       " ' Fwd URG Flags',\n",
       " ' Bwd URG Flags',\n",
       " ' Fwd Header Length',\n",
       " ' Bwd Header Length',\n",
       " 'Fwd Packets/s',\n",
       " ' Bwd Packets/s',\n",
       " ' Min Packet Length',\n",
       " ' Max Packet Length',\n",
       " ' Packet Length Mean',\n",
       " ' Packet Length Std',\n",
       " ' Packet Length Variance',\n",
       " 'FIN Flag Count',\n",
       " ' SYN Flag Count',\n",
       " ' RST Flag Count',\n",
       " ' PSH Flag Count',\n",
       " ' ACK Flag Count',\n",
       " ' URG Flag Count',\n",
       " ' CWE Flag Count',\n",
       " ' ECE Flag Count',\n",
       " ' Down/Up Ratio',\n",
       " ' Average Packet Size',\n",
       " ' Avg Fwd Segment Size',\n",
       " ' Avg Bwd Segment Size',\n",
       " 'Fwd Avg Bytes/Bulk',\n",
       " ' Fwd Avg Packets/Bulk',\n",
       " ' Fwd Avg Bulk Rate',\n",
       " ' Bwd Avg Bytes/Bulk',\n",
       " ' Bwd Avg Packets/Bulk',\n",
       " 'Bwd Avg Bulk Rate',\n",
       " 'Subflow Fwd Packets',\n",
       " ' Subflow Fwd Bytes',\n",
       " ' Subflow Bwd Packets',\n",
       " ' Subflow Bwd Bytes',\n",
       " 'Init_Win_bytes_forward',\n",
       " ' Init_Win_bytes_backward',\n",
       " ' act_data_pkt_fwd',\n",
       " ' min_seg_size_forward',\n",
       " 'Active Mean',\n",
       " ' Active Std',\n",
       " ' Active Max',\n",
       " ' Active Min',\n",
       " 'Idle Mean',\n",
       " ' Idle Std',\n",
       " ' Idle Max',\n",
       " ' Idle Min']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_columns = list(sum_of_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[997.1104736328125,\n",
       " 278.5461730957031,\n",
       " 0.35363054275512695,\n",
       " 0.336607426404953,\n",
       " 2.384303092956543,\n",
       " 0.04885248467326164,\n",
       " 120.88999938964844,\n",
       " 179.53636169433594,\n",
       " 107.06529998779297,\n",
       " 37.317081451416016,\n",
       " 234.478759765625,\n",
       " 30.204286575317383,\n",
       " 911.014404296875,\n",
       " 511.9794921875,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 31.822341918945312,\n",
       " 101.34968566894531,\n",
       " 269.5933837890625,\n",
       " 2.6698520183563232,\n",
       " 121.73059844970703,\n",
       " 62.77967071533203,\n",
       " 29.186058044433594,\n",
       " 233.39959716796875,\n",
       " 10.567804336547852,\n",
       " 134.08322143554688,\n",
       " 21.617116928100586,\n",
       " 16.573711395263672,\n",
       " 15.781695365905762,\n",
       " 13.715787887573242,\n",
       " 219.88604736328125,\n",
       " 0.0,\n",
       " 0.22144289314746857,\n",
       " 0.0,\n",
       " 0.686095118522644,\n",
       " 0.9539599418640137,\n",
       " 851.4979858398438,\n",
       " 48.443389892578125,\n",
       " 214.05775451660156,\n",
       " 276.38165283203125,\n",
       " 594.8876953125,\n",
       " 564.4378662109375,\n",
       " 295.7874450683594,\n",
       " 58.561607360839844,\n",
       " 120.28419494628906,\n",
       " 4.574575424194336,\n",
       " 4674.0068359375,\n",
       " 2323.418701171875,\n",
       " 225.0923309326172,\n",
       " 0.150033101439476,\n",
       " 2.7905843257904053,\n",
       " 87.8862533569336,\n",
       " 172.4701385498047,\n",
       " 120.74518585205078,\n",
       " 509.47271728515625,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.28953540325164795,\n",
       " 4.243412017822266,\n",
       " 0.06663486361503601,\n",
       " 0.2635596692562103,\n",
       " 1093.4700927734375,\n",
       " 646.5818481445312,\n",
       " 0.2874357998371124,\n",
       " 715242.3125,\n",
       " 0.5722739100456238,\n",
       " 2.227146625518799,\n",
       " 5.441637992858887,\n",
       " 1.7369837760925293,\n",
       " 186.34080505371094,\n",
       " 5.451188564300537,\n",
       " 242.47708129882812,\n",
       " 156.1265411376953]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_of_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(sum_of_columns)\n",
    "# # sorted_series = sum_of_columns.sort_values(ascending=False)\n",
    "\n",
    "# print(\"Sorted Series in descending order:\")\n",
    "# print(sorted_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' min_seg_size_forward', ' PSH Flag Count', ' ACK Flag Count', 'Init_Win_bytes_forward', ' Destination Port', ' Bwd Packet Length Mean', 'Fwd Packets/s', ' Init_Win_bytes_backward', ' Packet Length Mean', ' Packet Length Std', ' Bwd Packet Length Std', ' Avg Bwd Segment Size', ' Packet Length Variance', ' Flow Duration', ' Max Packet Length', ' Flow IAT Max', ' Idle Max', 'Bwd Packet Length Max', ' Fwd IAT Max', ' URG Flag Count', 'Fwd PSH Flags', ' Min Packet Length', 'Idle Mean', ' Fwd Packet Length Min', ' Average Packet Size', ' Idle Min', 'Bwd IAT Total', 'Fwd IAT Total', ' Fwd Packet Length Max', ' Avg Fwd Segment Size', ' SYN Flag Count', ' Fwd Packet Length Mean', ' Flow IAT Std', ' Down/Up Ratio', ' Fwd IAT Mean', 'FIN Flag Count', ' Bwd Packets/s', ' Fwd Packet Length Std', ' Flow IAT Mean', ' Bwd Packet Length Min', ' Fwd IAT Std', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', ' Fwd IAT Min', ' Idle Std', ' Active Max', ' RST Flag Count', ' Subflow Fwd Bytes', ' ECE Flag Count', ' Flow IAT Min', 'Total Length of Fwd Packets', ' Active Std', ' Active Min', ' Bwd Header Length', ' Fwd Header Length', 'Active Mean', ' Total Fwd Packets', ' Total Backward Packets', 'Subflow Fwd Packets', ' act_data_pkt_fwd', ' Subflow Bwd Bytes', ' Fwd URG Flags', ' CWE Flag Count', ' Subflow Bwd Packets', ' Total Length of Bwd Packets', 'Flow Bytes/s', ' Flow Packets/s', ' Bwd PSH Flags', ' Bwd URG Flags', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate')\n",
      "(715242.3125, 4674.0068359375, 2323.418701171875, 1093.4700927734375, 997.1104736328125, 911.014404296875, 851.4979858398438, 646.5818481445312, 594.8876953125, 564.4378662109375, 511.9794921875, 509.47271728515625, 295.7874450683594, 278.5461730957031, 276.38165283203125, 269.5933837890625, 242.47708129882812, 234.478759765625, 233.39959716796875, 225.0923309326172, 219.88604736328125, 214.05775451660156, 186.34080505371094, 179.53636169433594, 172.4701385498047, 156.1265411376953, 134.08322143554688, 121.73059844970703, 120.88999938964844, 120.74518585205078, 120.28419494628906, 107.06529998779297, 101.34968566894531, 87.8862533569336, 62.77967071533203, 58.561607360839844, 48.443389892578125, 37.317081451416016, 31.822341918945312, 30.204286575317383, 29.186058044433594, 21.617116928100586, 16.573711395263672, 15.781695365905762, 13.715787887573242, 10.567804336547852, 5.451188564300537, 5.441637992858887, 4.574575424194336, 4.243412017822266, 2.7905843257904053, 2.6698520183563232, 2.384303092956543, 2.227146625518799, 1.7369837760925293, 0.9539599418640137, 0.686095118522644, 0.5722739100456238, 0.35363054275512695, 0.336607426404953, 0.28953540325164795, 0.2874357998371124, 0.2635596692562103, 0.22144289314746857, 0.150033101439476, 0.06663486361503601, 0.04885248467326164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# names = ['John', 'Alice', 'Bob', 'Emily']\n",
    "# sum_of_columns = [10, 5, 15, 8]\n",
    "\n",
    "# Zip the two lists together\n",
    "combined = list(zip(names, sum_of_columns))\n",
    "\n",
    "# Sort the combined list in descending order based on the values from sum_of_columns\n",
    "sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "\n",
    "print(sorted_names)\n",
    "print(sorted_sum_of_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' min_seg_size_forward',\n",
       " ' PSH Flag Count',\n",
       " ' ACK Flag Count',\n",
       " 'Init_Win_bytes_forward',\n",
       " ' Destination Port',\n",
       " ' Bwd Packet Length Mean',\n",
       " 'Fwd Packets/s',\n",
       " ' Init_Win_bytes_backward',\n",
       " ' Packet Length Mean',\n",
       " ' Packet Length Std',\n",
       " ' Bwd Packet Length Std',\n",
       " ' Avg Bwd Segment Size',\n",
       " ' Packet Length Variance',\n",
       " ' Flow Duration',\n",
       " ' Max Packet Length',\n",
       " ' Flow IAT Max',\n",
       " ' Idle Max',\n",
       " 'Bwd Packet Length Max',\n",
       " ' Fwd IAT Max',\n",
       " ' URG Flag Count',\n",
       " 'Fwd PSH Flags',\n",
       " ' Min Packet Length',\n",
       " 'Idle Mean',\n",
       " ' Fwd Packet Length Min',\n",
       " ' Average Packet Size',\n",
       " ' Idle Min',\n",
       " 'Bwd IAT Total',\n",
       " 'Fwd IAT Total',\n",
       " ' Fwd Packet Length Max',\n",
       " ' Avg Fwd Segment Size',\n",
       " ' SYN Flag Count',\n",
       " ' Fwd Packet Length Mean',\n",
       " ' Flow IAT Std',\n",
       " ' Down/Up Ratio',\n",
       " ' Fwd IAT Mean',\n",
       " 'FIN Flag Count',\n",
       " ' Bwd Packets/s',\n",
       " ' Fwd Packet Length Std',\n",
       " ' Flow IAT Mean',\n",
       " ' Bwd Packet Length Min',\n",
       " ' Fwd IAT Std',\n",
       " ' Bwd IAT Mean',\n",
       " ' Bwd IAT Std',\n",
       " ' Bwd IAT Max',\n",
       " ' Bwd IAT Min',\n",
       " ' Fwd IAT Min',\n",
       " ' Idle Std',\n",
       " ' Active Max',\n",
       " ' RST Flag Count',\n",
       " ' Subflow Fwd Bytes',\n",
       " ' ECE Flag Count',\n",
       " ' Flow IAT Min',\n",
       " 'Total Length of Fwd Packets',\n",
       " ' Active Std',\n",
       " ' Active Min',\n",
       " ' Bwd Header Length',\n",
       " ' Fwd Header Length',\n",
       " 'Active Mean',\n",
       " ' Total Fwd Packets',\n",
       " ' Total Backward Packets',\n",
       " 'Subflow Fwd Packets',\n",
       " ' act_data_pkt_fwd',\n",
       " ' Subflow Bwd Bytes',\n",
       " ' Fwd URG Flags',\n",
       " ' CWE Flag Count',\n",
       " ' Subflow Bwd Packets',\n",
       " ' Total Length of Bwd Packets',\n",
       " 'Flow Bytes/s',\n",
       " ' Flow Packets/s',\n",
       " ' Bwd PSH Flags',\n",
       " ' Bwd URG Flags',\n",
       " 'Fwd Avg Bytes/Bulk',\n",
       " ' Fwd Avg Packets/Bulk',\n",
       " ' Fwd Avg Bulk Rate',\n",
       " ' Bwd Avg Bytes/Bulk',\n",
       " ' Bwd Avg Packets/Bulk',\n",
       " 'Bwd Avg Bulk Rate')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [ 0.     0.     0.    ...  0.     0.     0.   ]\n",
      " ...\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [ 0.04  -0.001  0.    ... -0.    -0.     0.   ]\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]]\n",
      "282792\n",
      "77\n",
      "(282792, 77)\n",
      "<class 'numpy.ndarray'>\n",
      "[[-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [ 0.     0.     0.    ...  0.     0.     0.   ]\n",
      " ...\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]\n",
      " [ 0.04  -0.001  0.    ... -0.    -0.     0.   ]\n",
      " [-0.     0.     0.    ... -0.     0.     0.   ]]\n",
      "(' min_seg_size_forward', ' PSH Flag Count', ' ACK Flag Count', ' Destination Port', 'Init_Win_bytes_forward', 'Fwd Packets/s', ' Init_Win_bytes_backward', ' Bwd Packet Length Mean', ' Packet Length Mean', ' Avg Bwd Segment Size', ' Packet Length Std', ' URG Flag Count', ' Bwd Packet Length Std', 'Fwd PSH Flags', ' Packet Length Variance', 'Bwd Packet Length Max', ' Max Packet Length', ' Min Packet Length', ' Average Packet Size', ' Fwd Packet Length Min', ' Avg Fwd Segment Size', ' SYN Flag Count', ' Fwd Packet Length Max', ' Flow Duration', ' Flow IAT Max', ' Down/Up Ratio', ' Fwd Packet Length Mean', ' Fwd IAT Max', 'FIN Flag Count', ' Bwd Packets/s', ' Idle Max', 'Bwd IAT Total', ' Fwd Packet Length Std', 'Idle Mean', 'Fwd IAT Total', ' Flow IAT Std', ' Bwd Packet Length Min', ' Idle Min', ' Fwd IAT Std', ' Fwd IAT Mean', ' Flow IAT Mean', ' Bwd IAT Max', ' Bwd IAT Std', ' Bwd IAT Mean', ' Fwd IAT Min', ' Subflow Fwd Bytes', ' RST Flag Count', ' Flow IAT Min', ' Active Max', ' Idle Std', ' ECE Flag Count', ' Bwd IAT Min', 'Total Length of Fwd Packets', ' Active Min', ' Active Std', 'Active Mean', ' Bwd Header Length', ' Total Fwd Packets', ' Total Backward Packets', 'Subflow Fwd Packets', ' Fwd Header Length', ' Subflow Bwd Bytes', ' act_data_pkt_fwd', ' Fwd URG Flags', ' CWE Flag Count', ' Subflow Bwd Packets', ' Total Length of Bwd Packets', 'Flow Bytes/s', ' Flow Packets/s', ' Bwd PSH Flags', ' Bwd URG Flags', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate')\n",
      "(717071.0625, 4214.896484375, 3553.189697265625, 1669.5565185546875, 1405.407470703125, 908.52392578125, 806.2886962890625, 800.2879638671875, 559.65771484375, 465.6141357421875, 444.1180114746094, 442.4877624511719, 421.1787414550781, 330.86248779296875, 265.75201416015625, 235.08697509765625, 228.94924926757812, 214.4729461669922, 178.26861572265625, 173.3682403564453, 151.6686248779297, 151.05796813964844, 136.9697265625, 109.34600830078125, 98.46509552001953, 97.51301574707031, 95.95667266845703, 78.94825744628906, 66.97727966308594, 63.655052185058594, 57.30451583862305, 51.96643829345703, 46.294857025146484, 45.119529724121094, 38.659027099609375, 37.07481384277344, 31.54294204711914, 28.598787307739258, 26.433034896850586, 14.842650413513184, 11.837433815002441, 9.434671401977539, 7.1268391609191895, 6.279068946838379, 5.191609859466553, 4.348451137542725, 4.240823745727539, 3.0193166732788086, 2.850217580795288, 2.8327536582946777, 2.6904191970825195, 2.532608985900879, 1.5163664817810059, 1.2972618341445923, 1.0122008323669434, 0.4831623435020447, 0.34027308225631714, 0.3130434453487396, 0.2785521447658539, 0.23975889384746552, 0.22063277661800385, 0.21099643409252167, 0.2085515409708023, 0.12934452295303345, 0.10820452868938446, 0.06288012117147446, 0.03353806585073471, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(' min_seg_size_forward',\n",
       " ' PSH Flag Count',\n",
       " ' ACK Flag Count',\n",
       " ' Destination Port',\n",
       " 'Init_Win_bytes_forward',\n",
       " 'Fwd Packets/s',\n",
       " ' Init_Win_bytes_backward',\n",
       " ' Bwd Packet Length Mean',\n",
       " ' Packet Length Mean',\n",
       " ' Avg Bwd Segment Size',\n",
       " ' Packet Length Std',\n",
       " ' URG Flag Count',\n",
       " ' Bwd Packet Length Std',\n",
       " 'Fwd PSH Flags',\n",
       " ' Packet Length Variance',\n",
       " 'Bwd Packet Length Max',\n",
       " ' Max Packet Length',\n",
       " ' Min Packet Length',\n",
       " ' Average Packet Size',\n",
       " ' Fwd Packet Length Min',\n",
       " ' Avg Fwd Segment Size',\n",
       " ' SYN Flag Count',\n",
       " ' Fwd Packet Length Max',\n",
       " ' Flow Duration',\n",
       " ' Flow IAT Max',\n",
       " ' Down/Up Ratio',\n",
       " ' Fwd Packet Length Mean',\n",
       " ' Fwd IAT Max',\n",
       " 'FIN Flag Count',\n",
       " ' Bwd Packets/s',\n",
       " ' Idle Max',\n",
       " 'Bwd IAT Total',\n",
       " ' Fwd Packet Length Std',\n",
       " 'Idle Mean',\n",
       " 'Fwd IAT Total',\n",
       " ' Flow IAT Std',\n",
       " ' Bwd Packet Length Min',\n",
       " ' Idle Min',\n",
       " ' Fwd IAT Std',\n",
       " ' Fwd IAT Mean',\n",
       " ' Flow IAT Mean',\n",
       " ' Bwd IAT Max',\n",
       " ' Bwd IAT Std',\n",
       " ' Bwd IAT Mean',\n",
       " ' Fwd IAT Min',\n",
       " ' Subflow Fwd Bytes',\n",
       " ' RST Flag Count',\n",
       " ' Flow IAT Min',\n",
       " ' Active Max',\n",
       " ' Idle Std',\n",
       " ' ECE Flag Count',\n",
       " ' Bwd IAT Min',\n",
       " 'Total Length of Fwd Packets',\n",
       " ' Active Min',\n",
       " ' Active Std',\n",
       " 'Active Mean',\n",
       " ' Bwd Header Length',\n",
       " ' Total Fwd Packets',\n",
       " ' Total Backward Packets',\n",
       " 'Subflow Fwd Packets',\n",
       " ' Fwd Header Length',\n",
       " ' Subflow Bwd Bytes',\n",
       " ' act_data_pkt_fwd',\n",
       " ' Fwd URG Flags',\n",
       " ' CWE Flag Count',\n",
       " ' Subflow Bwd Packets',\n",
       " ' Total Length of Bwd Packets',\n",
       " 'Flow Bytes/s',\n",
       " ' Flow Packets/s',\n",
       " ' Bwd PSH Flags',\n",
       " ' Bwd URG Flags',\n",
       " 'Fwd Avg Bytes/Bulk',\n",
       " ' Fwd Avg Packets/Bulk',\n",
       " ' Fwd Avg Bulk Rate',\n",
       " ' Bwd Avg Bytes/Bulk',\n",
       " ' Bwd Avg Packets/Bulk',\n",
       " 'Bwd Avg Bulk Rate')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create an analyzer for the model\n",
    "analyzer = innvestigate.create_analyzer(\"lrp.z\", model)\n",
    "\n",
    "# Replace X_test with your input data\n",
    "\n",
    "# X_test = np.random.rand(100, 28, 28)  # Example input data\n",
    "\n",
    "# X_test2 = X_test.frac(0.001, random_state=42)\n",
    "\n",
    "# Perform LRP analysis on the input data\n",
    "analysis = analyzer.analyze(X_test)\n",
    "\n",
    "# Print or use the analysis results as needed\n",
    "print(analysis)\n",
    "print(len(X_test))\n",
    "print(len(X_test.columns))\n",
    "names = X_test.columns\n",
    "print(analysis.shape)\n",
    "print(type(analysis))\n",
    "scores = pd.DataFrame(analysis)\n",
    "print(analysis)\n",
    "scores_abs = scores.abs()\n",
    "\n",
    "# Calculate the sum of each column\n",
    "sum_of_columns = scores_abs.sum(axis=0)\n",
    "\n",
    "names = list(names)\n",
    "\n",
    "names\n",
    "\n",
    "sum_of_columns = list(sum_of_columns)\n",
    "\n",
    "sum_of_columns\n",
    "# type(sum_of_columns)\n",
    "# # sorted_series = sum_of_columns.sort_values(ascending=False)\n",
    "\n",
    "# print(\"Sorted Series in descending order:\")\n",
    "# print(sorted_series)\n",
    "# names = ['John', 'Alice', 'Bob', 'Emily']\n",
    "# sum_of_columns = [10, 5, 15, 8]\n",
    "\n",
    "# Zip the two lists together\n",
    "combined = list(zip(names, sum_of_columns))\n",
    "\n",
    "# Sort the combined list in descending order based on the values from sum_of_columns\n",
    "sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted_combined list to separate names and sum_of_columns\n",
    "sorted_names, sorted_sum_of_columns = zip(*sorted_combined)\n",
    "\n",
    "print(sorted_names)\n",
    "print(sorted_sum_of_columns)\n",
    "\n",
    "sorted_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Generating Explainer\n",
      "---------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:From /home/oarreche@ads.iu.edu/anaconda3/envs/tf23/lib/python3.8/site-packages/shap/explainers/tf_utils.py:28: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        col_name  feature_importance_vals\n",
      "46                PSH Flag Count                 0.092632\n",
      "47                ACK Flag Count                 0.051487\n",
      "12        Bwd Packet Length Mean                 0.032615\n",
      "65        Init_Win_bytes_forward                 0.031980\n",
      "13         Bwd Packet Length Std                 0.028122\n",
      "36                 Fwd Packets/s                 0.026861\n",
      "41             Packet Length Std                 0.026828\n",
      "42        Packet Length Variance                 0.023077\n",
      "0               Destination Port                 0.022605\n",
      "54          Avg Bwd Segment Size                 0.017627\n",
      "40            Packet Length Mean                 0.017510\n",
      "48                URG Flag Count                 0.013118\n",
      "75                      Idle Max                 0.013011\n",
      "18                  Flow IAT Max                 0.012890\n",
      "30                 Fwd PSH Flags                 0.012833\n",
      "1                  Flow Duration                 0.011719\n",
      "39             Max Packet Length                 0.011521\n",
      "10         Bwd Packet Length Max                 0.011189\n",
      "23                   Fwd IAT Max                 0.011045\n",
      "43                FIN Flag Count                 0.009489\n",
      "68          min_seg_size_forward                 0.009116\n",
      "73                     Idle Mean                 0.009070\n",
      "76                      Idle Min                 0.008958\n",
      "66       Init_Win_bytes_backward                 0.008044\n",
      "25                 Bwd IAT Total                 0.006968\n",
      "20                 Fwd IAT Total                 0.005989\n",
      "52           Average Packet Size                 0.005588\n",
      "17                  Flow IAT Std                 0.004885\n",
      "38             Min Packet Length                 0.004644\n",
      "44                SYN Flag Count                 0.004379\n",
      "7          Fwd Packet Length Min                 0.003948\n",
      "21                  Fwd IAT Mean                 0.003134\n",
      "11         Bwd Packet Length Min                 0.002927\n",
      "22                   Fwd IAT Std                 0.002457\n",
      "6          Fwd Packet Length Max                 0.002290\n",
      "8         Fwd Packet Length Mean                 0.002107\n",
      "9          Fwd Packet Length Std                 0.001602\n",
      "16                 Flow IAT Mean                 0.001489\n",
      "53          Avg Fwd Segment Size                 0.001459\n",
      "28                   Bwd IAT Max                 0.001122\n",
      "26                  Bwd IAT Mean                 0.001076\n",
      "51                 Down/Up Ratio                 0.000889\n",
      "29                   Bwd IAT Min                 0.000840\n",
      "24                   Fwd IAT Min                 0.000831\n",
      "27                   Bwd IAT Std                 0.000780\n",
      "37                 Bwd Packets/s                 0.000722\n",
      "74                      Idle Std                 0.000387\n",
      "71                    Active Max                 0.000233\n",
      "19                  Flow IAT Min                 0.000157\n",
      "62             Subflow Fwd Bytes                 0.000113\n",
      "4    Total Length of Fwd Packets                 0.000108\n",
      "70                    Active Std                 0.000104\n",
      "45                RST Flag Count                 0.000073\n",
      "72                    Active Min                 0.000063\n",
      "69                   Active Mean                 0.000059\n",
      "50                ECE Flag Count                 0.000047\n",
      "49                CWE Flag Count                 0.000011\n",
      "32                 Fwd URG Flags                 0.000010\n",
      "3         Total Backward Packets                 0.000008\n",
      "67              act_data_pkt_fwd                 0.000006\n",
      "2              Total Fwd Packets                 0.000006\n",
      "64             Subflow Bwd Bytes                 0.000006\n",
      "61           Subflow Fwd Packets                 0.000006\n",
      "34             Fwd Header Length                 0.000003\n",
      "35             Bwd Header Length                 0.000002\n",
      "63           Subflow Bwd Packets                 0.000002\n",
      "5    Total Length of Bwd Packets                 0.000001\n",
      "60             Bwd Avg Bulk Rate                 0.000000\n",
      "59          Bwd Avg Packets/Bulk                 0.000000\n",
      "58            Bwd Avg Bytes/Bulk                 0.000000\n",
      "57             Fwd Avg Bulk Rate                 0.000000\n",
      "56          Fwd Avg Packets/Bulk                 0.000000\n",
      "31                 Bwd PSH Flags                 0.000000\n",
      "14                  Flow Bytes/s                 0.000000\n",
      "15                Flow Packets/s                 0.000000\n",
      "33                 Bwd URG Flags                 0.000000\n",
      "55            Fwd Avg Bytes/Bulk                 0.000000\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "output_file_name = 'SHAPCIC.txt'\n",
    "with open(output_file_name, \"w\") as f:print('',file = f)\n",
    "\n",
    "###here\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Generating Explainer')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "samples = 5000\n",
    "Label = label\n",
    "# df.pop('Label')\n",
    "#train.pop('Label')\n",
    "test = X_test\n",
    "train = X_train\n",
    "#df.pop('is_train')\n",
    "start_index = 0\n",
    "\n",
    "#end_index = len(test)\n",
    "end_index = samples\n",
    "#test = test[features]\n",
    "explainer = shap.DeepExplainer(model,train[start_index:end_index].values.astype('float'))\n",
    "shap_values = explainer.shap_values(test[start_index:end_index].values.astype('float'))\n",
    "# shap_values = explainer.shap_values(test[start_index:len(test)].values.astype('float'))\n",
    "\n",
    "# shap.summary_plot(shap_values = shap_values,\n",
    "#                  features = test[start_index:end_index],\n",
    "#                   class_names=[label[0],label[1],label[2],label[3],label[4],label[5],label[6]],show=False)\n",
    "\n",
    "# plt.savefig('DNN_Shap_Summary_Cicids.png')\n",
    "# plt.clf()\n",
    "\n",
    "vals= np.abs(shap_values).mean(1)\n",
    "feature_importance = pd.DataFrame(list(zip(train.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "feature_importance.head()\n",
    "print(feature_importance.to_string())\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "# feature_importance_vals = 'feature_importance_vals'  # Replace with the name of the column you want to extract\n",
    "feature_val = feature_importance['feature_importance_vals'].tolist()\n",
    "\n",
    "# col_name = 'col_name'  # Replace with the name of the column you want to extract\n",
    "feature_name = feature_importance['col_name'].tolist()\n",
    "\n",
    "\n",
    "# for item1, item2 in zip(feature_name, feature_val):\n",
    "#     print(item1, item2)\n",
    "\n",
    "\n",
    "# Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "zipped_lists = list(zip(feature_name, feature_val))\n",
    "zipped_lists.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Convert the sorted result back into separate lists\n",
    "sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "for k in sorted_list1:\n",
    "  with open(output_file_name, \"a\") as f:print(\"df.pop('\",k,\"')\", sep='',file = f)\n",
    "with open(output_file_name, \"a\") as f:print(\"Trial_ =[\", file = f)\n",
    "for k in sorted_list1:\n",
    "  with open(output_file_name, \"a\") as f:print(\"'\",k,\"',\", sep='', file = f)\n",
    "with open(output_file_name, \"a\") as f:print(\"]\", file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # shap.summary_plot(shap_values = shap_values[0],\n",
    "# #                  features = test[start_index:end_index],\n",
    "# #                   class_names=[label[0],label[1],label[2],label[3],label[4],label[5],label[6]],show=False)\n",
    "\n",
    "\n",
    "# # plt.savefig('DNN_Shap_Summary_Beeswarms.png')\n",
    "# # plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('Generating Sparsity Graph')\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('')\n",
    "\n",
    "# # Find the minimum and maximum values in the list\n",
    "# min_value = min(feature_val)\n",
    "# max_value = max(feature_val)\n",
    "\n",
    "# # Normalize the list to the range [0, 1]\n",
    "# normalized_list = []\n",
    "# for x in feature_val:\n",
    "#     if max_value - min_value == 0:\n",
    "#         normalized_list.append(0)\n",
    "#     else:\n",
    "#         normalized_list.append((x - min_value) / (max_value - min_value))\n",
    "   \n",
    "# # print(feature_name,normalized_list,'\\n')\n",
    "# # for item1, item2 in zip(feature_name, normalized_list):\n",
    "# #     print(item1, item2)\n",
    "\n",
    "# #calculating Sparsity\n",
    "\n",
    "# # Define the threshold\n",
    "# threshold = 1e-10\n",
    "\n",
    "# # Initialize a count variable to keep track of values below the threshold\n",
    "# count_below_threshold = 0\n",
    "\n",
    "# # Iterate through the list and count values below the threshold\n",
    "# for value in normalized_list:\n",
    "#     if value < threshold:\n",
    "#         count_below_threshold += 1\n",
    "\n",
    "# Sparsity = count_below_threshold/len(normalized_list)\n",
    "# Spar = []\n",
    "# print('Sparsity = ',Sparsity)\n",
    "# X_axis = []\n",
    "# #----------------------------------------------------------------------------\n",
    "# for i in range(0, 11):\n",
    "#     i/10\n",
    "#     threshold = i/10\n",
    "#     for value in normalized_list:\n",
    "#         if value < threshold:\n",
    "#             count_below_threshold += 1\n",
    "\n",
    "#     Sparsity = count_below_threshold/len(normalized_list)\n",
    "#     Spar.append(Sparsity)\n",
    "#     X_axis.append(i/10)\n",
    "#     count_below_threshold = 0\n",
    "\n",
    "\n",
    "# #---------------------------------------------------------------------------\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('y_axis_RF = ', Spar ,'', file = f)\n",
    "# with open(output_file_name, \"a\") as f:print('x_axis_RF = ', X_axis ,'', file = f)\n",
    "\n",
    "# plt.clf()\n",
    "\n",
    "# # Create a plot\n",
    "# plt.plot(X_axis, Spar, marker='o', linestyle='-')\n",
    "\n",
    "# # Set labels for the axes\n",
    "# plt.xlabel('X-Axis')\n",
    "# plt.ylabel('Y-Axis')\n",
    "\n",
    "# # Set the title of the plot\n",
    "# plt.title('Values vs. X-Axis')\n",
    "\n",
    "# # Show the plot\n",
    "# # plt.show()\n",
    "# plt.savefig('sparsity.png')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HITL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
